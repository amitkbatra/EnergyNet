{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeraFlow: Energy Efficiency Optimization of a Cryptomining Detector (POC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e833f5e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e557ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:22.360183Z",
     "start_time": "2023-03-08T23:02:22.221616Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import itertools\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import string\n",
    "import subprocess\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed, load\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score,\n",
    "                             confusion_matrix, f1_score, matthews_corrcoef,\n",
    "                             mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error, precision_score, recall_score,\n",
    "                             roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import copy\n",
    "import gzip\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import zipfile\n",
    "from contextlib import contextmanager\n",
    "from multiprocessing import Process\n",
    "from pathlib import Path\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as rt\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tf2onnx\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a13f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idle_gpu_memory = 61 # in MB\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "    \n",
    "    # check that the GPU utilization is 0\n",
    "    gpu_utilization = os.system(\"nvidia-smi --query-gpu=utilization.gpu --format=csv\")\n",
    "    \n",
    "    if gpu_utilization != 0:\n",
    "        print(\"GPU utilization is not 0\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # check that GPU memory is completely free\n",
    "    gpu_memory = os.system(\"nvidia-smi --query-gpu=memory.free --format=csv\")\n",
    "    \n",
    "    if gpu_memory != 0:\n",
    "        print(\"GPU memory is not completely free\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef38299a",
   "metadata": {},
   "source": [
    "### Set Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edadea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:22.584589Z",
     "start_time": "2023-03-08T23:02:22.483913Z"
    }
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "for physical_device in physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_device, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abbef594",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f961823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:22.725235Z",
     "start_time": "2023-03-08T23:02:22.587047Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def get_metrics(Y, predictions):\n",
    "    if len(Y.unique()) > 2:\n",
    "        print('F1:',f1_score(Y, predictions, average='macro'))\n",
    "    else:\n",
    "        print('F1:',f1_score(Y, predictions))\n",
    "    print('Accuracy:',accuracy_score(Y, predictions))\n",
    "    print(confusion_matrix(Y, predictions))\n",
    "\n",
    "def baseline_model(input_dim, n_output):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim*4, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(input_dim*6, activation='relu'))\n",
    "    model.add(Dense(input_dim*3, activation='relu'))\n",
    "    model.add(Dense(n_output, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def small_model(input_dim, n_output):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(input_dim/2), input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(int(input_dim/3), activation='relu'))\n",
    "    model.add(Dense(n_output, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def save_model_keras(model, name):\n",
    "    model.save(name)\n",
    "\n",
    "def save_scaler(scaler, name):\n",
    "    pickle.dump(scaler, open(name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-empire",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:22.848766Z",
     "start_time": "2023-03-08T23:02:22.727490Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "features = \"#15#c_ip:1 c_port:2 c_pkts_all:3 c_rst_cnt:4 c_ack_cnt:5 c_ack_cnt_p:6 c_bytes_uniq:7 c_pkts_data:8 c_bytes_all:9 c_pkts_retx:10 c_bytes_retx:11 c_pkts_ooo:12 c_syn_cnt:13 c_fin_cnt:14 s_ip:15 s_port:16 s_pkts_all:17 s_rst_cnt:18 s_ack_cnt:19 s_ack_cnt_p:20 s_bytes_uniq:21 s_pkts_data:22 s_bytes_all:23 s_pkts_retx:24 s_bytes_retx:25 s_pkts_ooo:26 s_syn_cnt:27 s_fin_cnt:28 first:29 last:30 durat:31 c_first:32 s_first:33 c_last:34 s_last:35 c_first_ack:36 s_first_ack:37 c_isint:38 s_isint:39 c_iscrypto:40 s_iscrypto:41 con_t:42 p2p_t:43 http_t:44 c_rtt_avg:45 c_rtt_min:46 c_rtt_max:47 c_rtt_std:48 c_rtt_cnt:49 c_ttl_min:50 c_ttl_max:51 s_rtt_avg:52 s_rtt_min:53 s_rtt_max:54 s_rtt_std:55 s_rtt_cnt:56 s_ttl_min:57 s_ttl_max:58 p2p_st:59 ed2k_data:60 ed2k_sig:61 ed2k_c2s:62 ed2k_c2c:63 ed2k_chat:64 c_f1323_opt:65 c_tm_opt:66 c_win_scl:67 c_sack_opt:68 c_sack_cnt:69 c_mss:70 c_mss_max:71 c_mss_min:72 c_win_max:73 c_win_min:74 c_win_0:75 c_cwin_max:76 c_cwin_min:77 c_cwin_ini:78 c_pkts_rto:79 c_pkts_fs:80 c_pkts_reor:81 c_pkts_dup:82 c_pkts_unk:83 c_pkts_fc:84 c_pkts_unrto:85 c_pkts_unfs:86 c_syn_retx:87 s_f1323_opt:88 s_tm_opt:89 s_win_scl:90 s_sack_opt:91 s_sack_cnt:92 s_mss:93 s_mss_max:94 s_mss_min:95 s_win_max:96 s_win_min:97 s_win_0:98 s_cwin_max:99 s_cwin_min:100 s_cwin_ini:101 s_pkts_rto:102 s_pkts_fs:103 s_pkts_reor:104 s_pkts_dup:105 s_pkts_unk:106 s_pkts_fc:107 s_pkts_unrto:108 s_pkts_unfs:109 s_syn_retx:110 http_req_cnt:111 http_res_cnt:112 http_res:113 c_pkts_push:114 s_pkts_push:115 c_tls_SNI:116 s_tls_SCN:117 c_npnalpn:118 s_npnalpn:119 c_tls_sesid:120 c_last_handshakeT:121 s_last_handshakeT:122 c_appdataT:123 s_appdataT:124 c_appdataB:125 s_appdataB:126 fqdn:127 dns_rslv:128 req_tm:129 res_tm:130 http_hostname:131\".split(\" \")\n",
    "ids_str = \"3,5,7,8,9,17,19,21,22,23\"\n",
    "\n",
    "ids = [int(x) for x in ids_str.split(\",\")]\n",
    "gf = [f for x,f in enumerate(features) if x+1 in ids]\n",
    "\n",
    "def tagger(c_ip, s_ip, c_pool, s_pool):\n",
    "    return 1 if (c_ip in c_pool) and (s_ip in s_pool) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0b321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:22.985946Z",
     "start_time": "2023-03-08T23:02:22.851735Z"
    }
   },
   "outputs": [],
   "source": [
    "tls_c_pool = [('10.100.200.4'),\n",
    "          ('10.100.200.3'),\n",
    "          ('10.100.200.2'),\n",
    "          ('10.100.200.2'),\n",
    "          ('10.100.200.2'),\n",
    "          ('10.100.200.2'),\n",
    "          ('10.100.200.3'),\n",
    "          ('10.100.200.4'),\n",
    "          ('10.100.200.2','10.100.200.4'),\n",
    "          ('10.100.200.4','10.100.200.2')\n",
    "]\n",
    "\n",
    "tls_s_pool = [('149.202.83.171'),\n",
    "          ('149.202.83.171'),\n",
    "          ('37.187.95.110'),\n",
    "          ('94.23.23.52'),\n",
    "          ('94.23.23.52','149.202.83.171'),\n",
    "          ('91.121.140.167','149.202.83.171'),\n",
    "          ('37.187.95.110','91.121.140.167'),\n",
    "          ('37.187.95.110','91.121.140.167'),\n",
    "          ('149.202.83.171','37.187.95.110','94.23.23.52','94.23.247.226','91.121.140.167'),\n",
    "          ('149.202.83.171','37.187.95.110','94.23.23.52','94.23.247.226','91.121.140.167')\n",
    "]\n",
    "\n",
    "data_path_5g = 'Criptominado/crypto5g'\n",
    "all_files = [f'{data_path_5g}/{files_dir}/log_tcp_temp_complete' for files_dir in os.listdir(data_path_5g)]\n",
    "all_files = sorted(all_files)\n",
    "all_files.remove(all_files[0])\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fd3b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:26.106359Z",
     "start_time": "2023-03-08T23:02:22.987858Z"
    }
   },
   "outputs": [],
   "source": [
    "df_list = [pd.read_csv(file, sep=' ') for file in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35236cc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:26.184506Z",
     "start_time": "2023-03-08T23:02:26.108805Z"
    }
   },
   "outputs": [],
   "source": [
    "df_list[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ace9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:26.250736Z",
     "start_time": "2023-03-08T23:02:26.189613Z"
    }
   },
   "outputs": [],
   "source": [
    "def tagRows(row, pool_index):\n",
    "    return tagger(row['#15#c_ip:1'], row['s_ip:15'], tls_c_pool[pool_index], tls_s_pool[pool_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70887d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:31.580803Z",
     "start_time": "2023-03-08T23:02:26.253308Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, df in enumerate(df_list):\n",
    "    df['tag'] = df.apply(lambda row: tagRows(row, i), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04492f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:31.680654Z",
     "start_time": "2023-03-08T23:02:31.582789Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_list[0],df_list[2],df_list[4],df_list[6]])\n",
    "df_test = pd.concat([df_list[1],df_list[3],df_list[5],df_list[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508dbf04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:31.740460Z",
     "start_time": "2023-03-08T23:02:31.682705Z"
    }
   },
   "outputs": [],
   "source": [
    "df_list[4][(df_list[4]['tag'] == 1)].groupby(['#15#c_ip:1','c_port:2','s_ip:15','s_port:16']).size().reset_index().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede0538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:31.955612Z",
     "start_time": "2023-03-08T23:02:31.742310Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mydf in df_list:\n",
    "    print(mydf[(mydf['tag'] == 1)].groupby(['#15#c_ip:1','c_port:2','s_ip:15','s_port:16']).size().reset_index().rename(columns={0:'count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0f98d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.035235Z",
     "start_time": "2023-03-08T23:02:31.959171Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tls_s_pool[-2]:\n",
    "    print(df_list[-2].loc[df_list[-2]['s_ip:15'] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43385a64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.085274Z",
     "start_time": "2023-03-08T23:02:32.037311Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df = df_list[-2].loc[df_list[-2]['s_ip:15'] == '94.23.247.226']\n",
    "new_df[['#15#c_ip:1','c_port:2','c_pkts_all:3','s_ip:15','s_port:16','s_pkts_all:17']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d530f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.216800Z",
     "start_time": "2023-03-08T23:02:32.087571Z"
    }
   },
   "outputs": [],
   "source": [
    "df_list[-2].loc[df_list[-2]['#15#c_ip:1'] == '94.23.247.226']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649eebab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.317567Z",
     "start_time": "2023-03-08T23:02:32.218972Z"
    }
   },
   "outputs": [],
   "source": [
    "for mydf in df_list:\n",
    "    print(len(mydf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c4623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.427482Z",
     "start_time": "2023-03-08T23:02:32.321588Z"
    }
   },
   "outputs": [],
   "source": [
    "gf = ['c_syn_cnt:13',\n",
    "     'c_mss:70',\n",
    "     'c_mss_max:71',\n",
    "     'c_mss_min:72',\n",
    "     'c_win_max:73',\n",
    "     'c_win_min:74',\n",
    "     'c_cwin_max:76',\n",
    "     'c_cwin_min:77',\n",
    "     'c_cwin_ini:78',\n",
    "     's_win_scl:90',\n",
    "     's_mss_max:94',\n",
    "     's_mss_min:95',\n",
    "     's_win_max:96',\n",
    "     's_win_min:97',\n",
    "     's_cwin_max:99',\n",
    "     's_cwin_min:100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b1948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.528871Z",
     "start_time": "2023-03-08T23:02:32.429810Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in gf:\n",
    "    print(i.split(':')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32a429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.695739Z",
     "start_time": "2023-03-08T23:02:32.532708Z"
    }
   },
   "outputs": [],
   "source": [
    "standard = StandardScaler().fit(df_train[gf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a7501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.747673Z",
     "start_time": "2023-03-08T23:02:32.698490Z"
    }
   },
   "outputs": [],
   "source": [
    "data_transformed = standard.transform(df_train[gf])\n",
    "data_transformed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55d3d51b",
   "metadata": {},
   "source": [
    "## Model Optimization Benchmarking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d60ca05c",
   "metadata": {},
   "source": [
    "Set logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a44a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.827655Z",
     "start_time": "2023-03-08T23:02:32.750049Z"
    }
   },
   "outputs": [],
   "source": [
    "# delete previous log file\n",
    "if os.path.exists('poc_energy_efficiency_crypto/logs/crypto_spider_5g_fcnn_optimized_benchmark.log'):\n",
    "    os.remove('poc_energy_efficiency_crypto/logs/crypto_spider_5g_fcnn_optimized_benchmark.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731a472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:32.930576Z",
     "start_time": "2023-03-08T23:02:32.830016Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "Path('poc_energy_efficiency_crypto/logs/').mkdir(parents=True, exist_ok=True)\n",
    "filehandler = logging.FileHandler('poc_energy_efficiency_crypto/logs/crypto_spider_5g_fcnn_optimized_benchmark.log')\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "filehandler.setFormatter(formatter)\n",
    "logger.addHandler(filehandler)\n",
    "\n",
    "# add stream handler\n",
    "streamhandler = logging.StreamHandler()\n",
    "streamhandler.setFormatter(formatter)\n",
    "logger.addHandler(streamhandler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "459ab3bf",
   "metadata": {},
   "source": [
    "### Set Benchmarking Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c04ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:33.051548Z",
     "start_time": "2023-03-08T23:02:32.933362Z"
    }
   },
   "outputs": [],
   "source": [
    "num_trials = 5\n",
    "stats_sampling_rate = 1\n",
    "\n",
    "es_patience = 20\n",
    "es_restore_best_weights = True\n",
    "\n",
    "batch_size = 1024\n",
    "validation_split = 0.2\n",
    "epochs = 10000\n",
    "\n",
    "# devices = [\"CPU\", \"GPU\"]\n",
    "devices = [\"CPU\"]\n",
    "\n",
    "ml_task = \"binary_classification\" # \"binary_classification\" or \"regression\"\n",
    "\n",
    "# general_optimizations = [\"runtime\", \"graph\"] NOT USED\n",
    "\n",
    "# Check if the parameters are valid\n",
    "assert devices in [[\"CPU\"], [\"GPU\"], [\"CPU\", \"GPU\"]], \"devices must be either ['CPU'], ['GPU'] or ['CPU', 'GPU']\"\n",
    "assert ml_task in [\"binary_classification\", \"regression\"], \"ml_task must be either 'binary_classification' or 'regression'\"\n",
    "\n",
    "assert num_trials > 0, \"num_trials must be greater than 0\"\n",
    "assert stats_sampling_rate > 0, \"stats_sampling_rate must be greater than 0\"\n",
    "assert batch_size > 0, \"batch_size must be greater than 0\"\n",
    "assert validation_split > 0 and validation_split < 1, \"validation_split must be greater than 0 and less than 1\"\n",
    "assert epochs > 0, \"epochs must be greater than 0\"\n",
    "assert es_patience > 0, \"es_patience must be greater than 0\"\n",
    "assert es_restore_best_weights in [True, False], \"es_restore_best_weights must be either True or False\"\n",
    "assert len(df_train) > 0, \"df_train must contain at least one row\"\n",
    "assert len(df_test) > 0, \"df_test must contain at least one row\"\n",
    "assert len(df_train.columns) > 0, \"df_train must contain at least one column\"\n",
    "assert len(df_test.columns) > 0, \"df_test must contain at least one column\"\n",
    "assert len(df_train.columns) == len(df_test.columns), \"df_train and df_test must have the same number of columns\"\n",
    "\n",
    "logger.info(f'Number of trials set to: {num_trials}')\n",
    "logger.info(f'Epochs set to: {epochs}')\n",
    "logger.info(f'Batch size set to: {batch_size}')\n",
    "logger.info(f'Validation split set to: {validation_split}')\n",
    "logger.info(f'Patience set to: {es_patience}')\n",
    "logger.info(f'Restore best weights set to: {es_restore_best_weights}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44f61c8c",
   "metadata": {},
   "source": [
    "### Define Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888cbc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:33.202845Z",
     "start_time": "2023-03-08T23:02:33.052949Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"EXP0\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": None\n",
    "    },\n",
    "    \"EXP1\": {\n",
    "        \"post_training_optimizations\": [\"full_integer_quantization\"],\n",
    "        \"training_aware_optimizations\": None\n",
    "    },\n",
    "    \"EXP2\": {\n",
    "        \"post_training_optimizations\": [\"float16_quantization\"],\n",
    "        \"training_aware_optimizations\": None\n",
    "    },\n",
    "    \"EXP3\": {\n",
    "        \"post_training_optimizations\": [\"float16_int8_quantization\"],\n",
    "        \"training_aware_optimizations\": None\n",
    "    },\n",
    "    \"EXP4\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"pruning\"]\n",
    "    },\n",
    "    \"EXP5\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"quantization_aware_training\"]\n",
    "    },\n",
    "    \"EXP6\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"knowledge_distillation\"]\n",
    "    },\n",
    "    \"EXP7\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"pruning\", \"quantization_aware_training\"],\n",
    "    },\n",
    "    \"EXP8\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"knowledge_distillation\", \"pruning\"]\n",
    "    },\n",
    "    \"EXP9\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"knowledge_distillation\", \"quantization_aware_training\"]\n",
    "    },\n",
    "    \"EXP10\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"knowledge_distillation\", \"pruning\", \"quantization_aware_training\"]\n",
    "    },\n",
    "    \"EXP11\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"pruning\", \"float16_quantization\"],\n",
    "    },\n",
    "    \"EXP12\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"knowledge_distillation\", \"float16_quantization\"]\n",
    "    },\n",
    "    \"EXP13\": {\n",
    "        \"post_training_optimizations\": None,\n",
    "        \"training_aware_optimizations\": [\"knowledge_distillation\", \"pruning\", \"float16_quantization\"]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ac12f1e",
   "metadata": {},
   "source": [
    "### Energy Efficiency Monitoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3de93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:35.599361Z",
     "start_time": "2023-03-08T23:02:33.205521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Socket for sending data from powerstat process to main process\n",
    "HOST = \"127.0.0.1\"\n",
    "\n",
    "# Define the measurement tool that will be used to gather power consumption data\n",
    "power_consumption_measurement_tool = \"powerstat\"\n",
    "\n",
    "# RAM\n",
    "def get_ram_memory_uss(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    \n",
    "    return str(process.memory_full_info().uss / (1024*1024)) + ' MB'\n",
    "\n",
    "def get_ram_memory_rss(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    \n",
    "    return str(process.memory_full_info().rss / (1024*1024)) + ' MB'\n",
    "\n",
    "def get_ram_memory_vms(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    \n",
    "    return str(process.memory_full_info().vms / (1024*1024)) + ' MB'\n",
    "\n",
    "def get_ram_memory_pss(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    \n",
    "    return str(process.memory_full_info().pss / (1024*1024)) + ' MB'\n",
    "\n",
    "# CPU\n",
    "def get_cpu_usage(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    \n",
    "    return str(process.cpu_percent(interval=0.5) / psutil.cpu_count()) + ' %'\n",
    "\n",
    "def get_cpu_freq():    \n",
    "    return str(psutil.cpu_freq()[0]) + \" MHz\"\n",
    "\n",
    "def perf(PORT):\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.connect((HOST, PORT))\n",
    "    cmd = \"echo pirata.lab | sudo -S -p \\\"\\\" perf stat -e power/energy-cores/,power/energy-pkg/\"\n",
    "    out = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "    s.sendall(out.stdout)\n",
    "    \n",
    "    s.close()\n",
    "    \n",
    "def kill_perf():\n",
    "    cmd = \"echo pirata.lab | sudo -S -p \\\"\\\" pkill perf\"\n",
    "    subprocess.run(cmd, shell=True)\n",
    "\n",
    "def powerstat(PORT):\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.connect((HOST, PORT))\n",
    "    cmd = \"echo pirata.lab | sudo -S -p \\\"\\\" powerstat -R 0.5 120\"\n",
    "    out = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "    s.sendall(out.stdout)\n",
    "    \n",
    "    s.close()\n",
    "    \n",
    "def kill_powerstat():\n",
    "    cmd = \"echo pirata.lab | sudo -S -p \\\"\\\" pkill powerstat\"\n",
    "    subprocess.run(cmd, shell=True)\n",
    "    \n",
    "def get_cpu_power_draw():    \n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "    PORT = np.random.randint(10000, 20000)\n",
    "    s.bind((HOST, PORT))\n",
    "    s.listen()\n",
    "    \n",
    "    if power_consumption_measurement_tool == \"powerstat\":\n",
    "        p = multiprocessing.Process(target=powerstat, args=(PORT,))\n",
    "    elif power_consumption_measurement_tool == \"perf\":\n",
    "        p = multiprocessing.Process(target=perf, args=(PORT,))\n",
    "        \n",
    "    p.start()\n",
    "    conn, _addr = s.accept()\n",
    "    time.sleep(max(1, stats_sampling_rate / 2))\n",
    "    \n",
    "    if power_consumption_measurement_tool == \"powerstat\":\n",
    "        q = multiprocessing.Process(target=kill_powerstat)\n",
    "    elif power_consumption_measurement_tool == \"perf\":\n",
    "        q = multiprocessing.Process(target=kill_perf)\n",
    "        \n",
    "    q.start()\n",
    "    \n",
    "    out = conn.recv(2048).decode()\n",
    "    \n",
    "    power_consumption = re.findall(r'CPU: (.+?) Watts', out)[0].strip() + \" W\"\n",
    "    \n",
    "    s.close()\n",
    "    p.terminate()\n",
    "    q.terminate()\n",
    "    \n",
    "    return power_consumption\n",
    "\n",
    "# IO\n",
    "def get_io_usage(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    \n",
    "    io_counters = process.io_counters()\n",
    "    io_usage_process = io_counters[2] + io_counters[3] # read_bytes + write_bytes\n",
    "    disk_io_counter = psutil.disk_io_counters()\n",
    "    disk_io_total = disk_io_counter[2] + disk_io_counter[3] # read_bytes + write_bytes\n",
    "    io_usage_process = io_usage_process / disk_io_total * 100\n",
    "    io_usage_process = np.round(io_usage_process, 2)\n",
    "    io_usage_process = str(io_usage_process) + \" %\"\n",
    "\n",
    "    return io_usage_process\n",
    "\n",
    "def get_bytes_written(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    \n",
    "    io_counters = process.io_counters()\n",
    "    process_bytes_written = io_counters[3]\n",
    "    total_bytes_written = psutil.disk_io_counters()[3]\n",
    "    process_bytes_written = process_bytes_written / total_bytes_written * 100\n",
    "    process_bytes_written = np.round(process_bytes_written, 2)\n",
    "    process_bytes_written = str(process_bytes_written) + \" %\"\n",
    "\n",
    "    return process_bytes_written\n",
    "\n",
    "def get_bytes_read(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    \n",
    "    io_counters = process.io_counters()\n",
    "    process_bytes_read = io_counters[2]\n",
    "    total_bytes_read = psutil.disk_io_counters()[2]\n",
    "    process_bytes_read = process_bytes_read / total_bytes_read * 100\n",
    "    process_bytes_read = np.round(process_bytes_read, 2)\n",
    "    process_bytes_read = str(process_bytes_read) + \" %\"\n",
    "\n",
    "    return process_bytes_read\n",
    "\n",
    "# GPU\n",
    "get_gpu_memory_system = lambda: os.popen('nvidia-smi --query-gpu=memory.used --format=csv,noheader').read().split('\\n')[0] # SYSTEM WIDE\n",
    "def get_gpu_memory(pid):\n",
    "    output = os.popen('nvidia-smi | awk \\'/' + str(pid) + '/{print $8}\\'').read().split('\\n')[0]\n",
    "    output = \"0 MiB\" if output == \"\" else output.replace(\"MiB\", \"\") + \" MiB\"\n",
    "\n",
    "    return output\n",
    "\n",
    "get_gpu_usage = lambda: os.popen('nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader').read().split('\\n')[0]\n",
    "get_gpu_freq = lambda: os.popen('nvidia-smi --query-gpu=clocks.gr --format=csv,noheader').read().split('\\n')[0]\n",
    "get_gpu_power_draw = lambda: os.popen('nvidia-smi --query-gpu=power.draw --format=csv,noheader').read().split('\\n')[0]\n",
    "\n",
    "test_pid = os.getpid()\n",
    "\n",
    "# Test functions\n",
    "print(\"RAM Memory Usage (USS): \" + get_ram_memory_uss(test_pid))\n",
    "print(\"RAM Memory Usage (RSS): \" + get_ram_memory_rss(test_pid))\n",
    "print(\"RAM Memory Usage (VMS): \" + get_ram_memory_vms(test_pid))\n",
    "print(\"RAM Memory Usage (PSS): \" + get_ram_memory_pss(test_pid))\n",
    "# print(\"RAM Power Draw: \" + get_cpu_power_draw()[1])\n",
    "print(\"CPU Usage: \" + get_cpu_usage(test_pid))\n",
    "print(\"CPU Frequency: \" + get_cpu_freq())\n",
    "# print(\"CPU Cores Power Draw: \" + get_cpu_power_draw()[0])\n",
    "# print(\"CPU Package Power Draw: \" + get_cpu_power_draw()[2])\n",
    "print(\"CPU Power Usage: \" + get_cpu_power_draw())\n",
    "print(\"I/O Usage: \" + get_io_usage(test_pid))\n",
    "print(\"Bytes Written to disk: \" + str(get_bytes_written(test_pid)))\n",
    "print(\"Bytes Read to disk: \" + str(get_bytes_read(test_pid)))\n",
    "print(\"GPU Memory Usage: \" + get_gpu_memory(test_pid))\n",
    "print(\"GPU Usage: \" + get_gpu_usage())\n",
    "print(\"GPU Frequency: \" + get_gpu_freq())\n",
    "print(\"GPU Power Draw: \" + get_gpu_power_draw())\n",
    "\n",
    "def get_stats(pid):\n",
    "    stats = {}\n",
    "    stats['ram_memory_uss'] = get_ram_memory_uss(pid)\n",
    "    stats['ram_memory_rss'] = get_ram_memory_rss(pid)\n",
    "    stats['ram_memory_vms'] = get_ram_memory_vms(pid)\n",
    "    stats['ram_memory_pss'] = get_ram_memory_pss(pid)\n",
    "    # stats[\"ram_power_draw\"] = get_cpu_power_draw()[1]\n",
    "    stats['cpu_usage'] = get_cpu_usage(pid)\n",
    "    stats['cpu_freq'] = get_cpu_freq()\n",
    "    # stats['cpu_cores_power_draw'] = get_cpu_power_draw()[0]\n",
    "    # stats['cpu_package_power_draw'] = get_cpu_power_draw()[2]\n",
    "    stats['cpu_power_draw'] = get_cpu_power_draw()\n",
    "    stats['io_usage'] = get_io_usage(pid)\n",
    "    stats['bytes_written'] = get_bytes_written(pid)\n",
    "    stats['bytes_read'] = get_bytes_read(pid)\n",
    "    stats['gpu_memory'] = get_gpu_memory(pid)\n",
    "    stats['gpu_usage'] = get_gpu_usage()\n",
    "    stats['gpu_freq'] = get_gpu_freq()\n",
    "    stats['gpu_power_draw'] = get_gpu_power_draw()\n",
    "\n",
    "    return stats\n",
    "\n",
    "def sample_stats(test, sampling_rate, pid, directory):\n",
    "    print(f\"test: {test}\")\n",
    "    \n",
    "    stats_list = []\n",
    "    started = False\n",
    "    \n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    while True:\n",
    "        stats = get_stats(pid)\n",
    "        stats_list.append(stats)\n",
    "\n",
    "        # write stats to pickle file\n",
    "        with open(f\"{directory}/crypto_spider_5g_fcnn_optimized_benchmark_{test}_stats.pkl\", 'wb') as f:\n",
    "            pickle.dump(stats_list, f)\n",
    "\n",
    "        if not started:\n",
    "            # write file started.txt to signal that the sampling has started\n",
    "            with open(f\"started_{test}.txt\", 'w') as f:\n",
    "                f.write(\"STARTED\")\n",
    "            \n",
    "            print(\"\\nStats sampling started\")\n",
    "\n",
    "        started = True\n",
    "\n",
    "        # check if file \"stop.txt\" exists\n",
    "        if os.path.isfile(f\"stop_{test}.txt\"):\n",
    "            print(\"Stats sampling stopped\")\n",
    "            os.remove(f\"stop_{test}.txt\")\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(sampling_rate)\n",
    "\n",
    "def get_stats_background(test, sampling_rate, pid, directory):\n",
    "    proc = Process(target=sample_stats, args=(test, sampling_rate, pid, directory))\n",
    "    proc.start()\n",
    "\n",
    "    return proc\n",
    "\n",
    "def strip_units(x):\n",
    "    return float(x.split(' ')[0])\n",
    "\n",
    "def agg_stats(agg_func, stats_list, average_time_spent):\n",
    "    average_stats = copy.deepcopy(stats_list)\n",
    "\n",
    "    # strip units of the stats of every trial in stats_list\n",
    "    for trial in average_stats:\n",
    "        for snapshot in trial:\n",
    "            for stat in snapshot:\n",
    "                stats_value = snapshot[stat]\n",
    "                stats_value_stripped = strip_units(stats_value)\n",
    "                snapshot[stat] = stats_value_stripped\n",
    "\n",
    "    trials_list = []\n",
    "    \n",
    "    # convert to a numpy array\n",
    "    for trial in average_stats:\n",
    "        df = pd.DataFrame(trial)\n",
    "        trial = df.to_numpy()\n",
    "        \n",
    "        trials_list.append(trial)\n",
    "    \n",
    "    trials_list_np = np.array(trials_list)\n",
    "    \n",
    "    print(\"trials_list_np.shape: {}\".format(trials_list_np.shape))\n",
    "    \n",
    "    # fill first axis of trials_list_np with NaNs until all trials have the same length\n",
    "    # trials_list_np = np.array([np.pad(trial, ((0, trials_list_np.shape[0] - trial.shape[0]), (0, 0)), 'constant', constant_values=np.nan) for trial in trials_list_np])\n",
    "    max_length = max([trial.shape[0] for trial in trials_list_np])\n",
    "    trials_list_np_filled = []\n",
    "    \n",
    "    for trial in trials_list_np:\n",
    "        trial_length = trial.shape[0]\n",
    "        \n",
    "        if trial_length < max_length:\n",
    "            print(f\"Trial length ({trial_length}) is smaller than max length ({max_length}). Filling with NaNs...\")\n",
    "        \n",
    "            # fill first axis of trial with NaNs until trial has the same length as the longest trial\n",
    "            trial = np.pad(trial, ((0, max_length - trial_length), (0, 0)), 'constant', constant_values=np.nan)\n",
    "            \n",
    "            print(\"trial.shape: {}\".format(trial.shape))\n",
    "        \n",
    "        trials_list_np_filled.append(trial)\n",
    "    \n",
    "    trials_list_np_filled = np.array(trials_list_np_filled)\n",
    "    \n",
    "    print(\"trials_list_np_filled.shape: {}\".format(trials_list_np_filled.shape))\n",
    "\n",
    "    average_stats_np = agg_func(trials_list_np_filled, axis=0)\n",
    "\n",
    "    print(\"average_stats_np.shape: {}\".format(average_stats_np.shape))\n",
    "\n",
    "    return average_stats_np\n",
    "\n",
    "def get_average_stats(stats_list, average_time_spent):\n",
    "    return agg_stats(agg_func=np.nanmean, stats_list=stats_list, average_time_spent=average_time_spent)\n",
    "\n",
    "def get_std_dev_stats(stats_list, average_time_spent):\n",
    "    return agg_stats(agg_func=np.nanstd, stats_list=stats_list, average_time_spent=average_time_spent)\n",
    "\n",
    "def get_max_stats(stats_list, average_time_spent):\n",
    "    return agg_stats(agg_func=np.nanmax, stats_list=stats_list, average_time_spent=average_time_spent)\n",
    "\n",
    "def save_stats_to_logfile(test, average_stats, std_dev_stats, max_stats):\n",
    "    units = {'ram_memory_uss': 'MB', 'ram_memory_rss': 'MB', 'ram_memory_vms': 'MB', 'ram_memory_pss': 'MB', 'cpu_usage': '%', 'cpu_freq': 'MHz', 'cpu_power_draw': 'W', 'io_usage': '%', 'bytes_written': 'MB', 'bytes_read': 'MB', 'gpu_memory': 'MB', 'gpu_usage': '%', 'gpu_freq': 'MHz', 'gpu_power_draw': 'W'}\n",
    "\n",
    "    for key in average_stats.keys():\n",
    "        logger.info(f'[{test}] {key} (average): {average_stats[key]} {units[key]}')\n",
    "        logger.info(f'[{test}] {key} (std_dev): {std_dev_stats[key]} {units[key]}')\n",
    "        logger.info(f'[{test}] {key} (max): {max_stats[key]} {units[key]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0acccbc2",
   "metadata": {},
   "source": [
    "### General Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c00b71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:35.619069Z",
     "start_time": "2023-03-08T23:02:35.611110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Runtime optimizations\n",
    "def apply_runtime_optimizations():\n",
    "    logger.info(\"Applying runtime optimizations\")\n",
    "\n",
    "    total_cpu_cores = os.popen('nproc').read().strip()\n",
    "    number_sockets = int(os.popen('grep \"^physical id\" /proc/cpuinfo | awk \\'{print $4}\\' | sort -un | tail -1').read().strip()) + 1\n",
    "    number_cpu_cores = int((int(total_cpu_cores) / 2) / number_sockets)\n",
    "\n",
    "    logger.info(\"number of CPU cores per socket: {}\".format(number_cpu_cores))\n",
    "    logger.info(\"number of sockets: {}\".format(number_sockets))\n",
    "\n",
    "    # set intra_op_parallelism = number of physical core per socket\n",
    "    # set inter_op_parallelism = number of sockets\n",
    "\n",
    "    tf.config.set_soft_device_placement(True)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(number_cpu_cores)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(number_sockets)\n",
    "\n",
    "# Graph optimizations\n",
    "def apply_graph_optimizations():\n",
    "    logger.info(\"Applying graph optimizations\")\n",
    "\n",
    "    # Export the graph to a protobuf file\n",
    "    tf.io.write_graph(tf.compat.v1.get_default_graph(), './', 'graph.pbtxt', as_text=True)\n",
    "\n",
    "    # Freeze the graph\n",
    "    os.system('python3 -m tensorflow.python.tools.freeze_graph --input_graph crypto_spider_5g_fcnn_frozen.pb --input_checkpoint crypto_spider_5g_fcnn.h5 --output_graph crypto_spider_5g_fcnn_frozen.pb --output_node_names model_output')\n",
    "\n",
    "    # Optimize the graph for inference\n",
    "    os.system('python3 -m tensorflow.python.tools.optimize_for_inference --input crypto_spider_5g_fcnn_frozen.pb --output crypto_spider_5g_fcnn_frozen.pb --input_names input_1 --output_names model_output --frozen_func true')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d64930ea",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9f57d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:35.758590Z",
     "start_time": "2023-03-08T23:02:35.622433Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, verbose=False, extra_callbacks=[]):\n",
    "    print(\"Training model\")\n",
    "    print(\"Verbose: {}\".format(verbose))\n",
    "    print(\"Extra callbacks: {}\".format(extra_callbacks))\n",
    "    \n",
    "    # es = EarlyStopping(monitor='val_loss', mode='min', patience=es_patience, verbose=verbose, restore_best_weights=es_restore_best_weights)\n",
    "    # history = model.fit(data_transformed, pd.get_dummies(df_train['tag']), validation_split=validation_split, epochs=epochs, batch_size=4096, callbacks=[es] + extra_callbacks, verbose=verbose)\n",
    "    # history = model.fit(data_transformed, pd.get_dummies(df_train['tag']), validation_split=validation_split, epochs=epochs, batch_size=4096, callbacks=extra_callbacks, verbose=verbose)\n",
    "    history = model.fit(data_transformed, pd.get_dummies(df_train['tag']), validation_split=validation_split, epochs=50, batch_size=4096, callbacks=extra_callbacks, verbose=verbose)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f90563b",
   "metadata": {},
   "source": [
    "### Model Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd29636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:35.913771Z",
     "start_time": "2023-03-08T23:02:35.761722Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_integer_quantization(trained_model):\n",
    "    def dataset_generator():\n",
    "        for data in tf.data.Dataset.from_tensor_slices((data_transformed)).batch(1).take(int(len(data_transformed) * 0.25)):\n",
    "            yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(trained_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = dataset_generator\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    logger.info(\"Applied Full-integer Quantization\")\n",
    "\n",
    "    # Save the model to disk\n",
    "    quantized_tflite_model_file = 'model.tflite'\n",
    "    \n",
    "    with open(quantized_tflite_model_file, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    return quantized_tflite_model_file\n",
    "\n",
    "\n",
    "def float16_quantization(trained_model):\n",
    "    def dataset_generator():\n",
    "        for data in tf.data.Dataset.from_tensor_slices((data_transformed)).batch(1).take(int(len(data_transformed) * 0.25)):\n",
    "            yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(trained_model)\n",
    "    converter.representative_dataset = dataset_generator\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.float16]\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    logger.info(\"Applied float16 Activations and int8 weights-Quantization\")\n",
    "\n",
    "    # Save the model to disk\n",
    "    quantized_tflite_model_file = 'model.tflite'\n",
    "    \n",
    "    with open(quantized_tflite_model_file, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    return quantized_tflite_model_file\n",
    "\n",
    "\n",
    "def float16_int8_quantization(trained_model):\n",
    "    def dataset_generator():\n",
    "        for data in tf.data.Dataset.from_tensor_slices((data_transformed)).batch(1).take(int(len(data_transformed) * 0.25)):\n",
    "            yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(trained_model)\n",
    "    converter.representative_dataset = dataset_generator\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    logger.info(\"Applied float16 Activations and int8 weights-Quantization\")\n",
    "\n",
    "    # Save the model to disk\n",
    "    quantized_tflite_model_file = 'model.tflite'\n",
    "    \n",
    "    with open(quantized_tflite_model_file, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    return quantized_tflite_model_file\n",
    "\n",
    "# Fine-tune pretrained model with pruning\n",
    "def pruning(model, convert_to_tflite=True):\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "    # Compute end step to finish pruning after 10% of pre-training epochs\n",
    "    batch_size = 256\n",
    "    # n_epochs = 20\n",
    "    n_epochs = 10\n",
    "    \n",
    "    validation_split = 0.1\n",
    "\n",
    "    num_samples = len(df_train)\n",
    "    end_step = np.ceil(num_samples / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "    pruning_params = {\n",
    "        \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.50, final_sparsity=0.80, begin_step=0, end_step=end_step\n",
    "        )\n",
    "    }\n",
    "\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model_for_pruning.compile(\n",
    "        optimizer=\"adam\", loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    ]\n",
    "    \n",
    "    history = model_for_pruning.fit(data_transformed, pd.get_dummies(df_train['tag']), validation_split=validation_split, epochs=n_epochs, batch_size=batch_size, callbacks=callbacks)\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "    \n",
    "    if convert_to_tflite:\n",
    "        # convert to tflite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY, tf.lite.Optimize.DEFAULT]\n",
    "        pruned_tflite_model = converter.convert()\n",
    "        \n",
    "        # Save the model to disk\n",
    "        pruned_tflite_model_file = 'model.tflite'\n",
    "        \n",
    "        with open(pruned_tflite_model_file, 'wb') as f:\n",
    "            f.write(pruned_tflite_model)\n",
    "\n",
    "        return pruned_tflite_model_file, history\n",
    "        \n",
    "        # return pruned_model, history\n",
    "    else:\n",
    "        return pruned_model, history\n",
    "\n",
    "\n",
    "# Quantization Aware Training\n",
    "def quantization_aware_training(model):\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "    q_aware_model = quantize_model(model)\n",
    "    q_aware_model.compile(\n",
    "        optimizer=\"adam\", loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    batch_size = 256\n",
    "    n_epochs = 10\n",
    "    validation_split = 0.1\n",
    "\n",
    "    history = q_aware_model.fit(data_transformed, pd.get_dummies(df_train['tag']), validation_split=validation_split, epochs=n_epochs, batch_size=batch_size)\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    quantized_model = converter.convert()\n",
    "    \n",
    "    # Save the model to disk\n",
    "    quantized_tflite_model_file = 'model.tflite'\n",
    "    \n",
    "    with open(quantized_tflite_model_file, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    return quantized_tflite_model_file, history\n",
    "\n",
    "\n",
    "# Knowledge Distillation\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n",
    "            # The magnitudes of the gradients produced by the soft targets scale\n",
    "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
    "            distillation_loss = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "                )\n",
    "                * self.temperature ** 2\n",
    "            )\n",
    "\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss, \"distillation_loss\": distillation_loss})\n",
    "\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "def knowledge_distillation(teacher, convert_to_tflite=True):\n",
    "    logger.info(\"Applying Knowledge Distillation\")\n",
    "    student = small_model(input_dim=len(gf), n_output=2)\n",
    "\n",
    "    # Initialize and compile distiller\n",
    "    distiller = Distiller(student=student, teacher=teacher)\n",
    "    distiller.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "        student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(\n",
    "        monitor=\"val_student_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=es_patience,\n",
    "        restore_best_weights=es_restore_best_weights,\n",
    "    )\n",
    "    history = distiller.fit(\n",
    "        data_transformed,\n",
    "        pd.get_dummies(df_train[\"tag\"]),\n",
    "        validation_split=validation_split,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "    \n",
    "    if convert_to_tflite:\n",
    "        # convert to tflite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(distiller.student)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        distilled_tflite_model = converter.convert()\n",
    "        \n",
    "        # Save the model to disk\n",
    "        distilled_tflite_model_file = 'model.tflite'\n",
    "        \n",
    "        with open(distilled_tflite_model_file, 'wb') as f:\n",
    "            f.write(distilled_tflite_model)\n",
    "\n",
    "        return distilled_tflite_model_file, history\n",
    "\n",
    "        # return distiller.student, history\n",
    "    else:\n",
    "        return distiller.student, history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41c1d715",
   "metadata": {},
   "source": [
    "### Benchmarking Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ff5cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:36.081076Z",
     "start_time": "2023-03-08T23:02:35.916574Z"
    }
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def measure_time() -> float:\n",
    "    start = time.perf_counter()\n",
    "    yield lambda: time.perf_counter() - start\n",
    "\n",
    "class StatsCollectionManager():\n",
    "    def __init__(self, test, sampling_rate=0.1, pid=None, directory=None):\n",
    "        self.test = test\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.pid = pid\n",
    "        self.proc = None\n",
    "        \n",
    "        if directory is None:\n",
    "            self.directory = f\"poc_energy_efficiency_crypto/\"\n",
    "        else:\n",
    "            self.directory = f\"poc_energy_efficiency_crypto/{directory}/\"\n",
    "\n",
    "        print(\"Starting stats collection for test: {}\".format(test))\n",
    " \n",
    "    def __enter__(self):\n",
    "        self.proc = get_stats_background(test=self.test, sampling_rate=self.sampling_rate, pid=self.pid, directory=self.directory)\n",
    "\n",
    "        while True:\n",
    "            # check if file stats.txt exists\n",
    "            if os.path.exists(f\"started_{self.test}.txt\"):\n",
    "                print(\"\\nStats collection started\")\n",
    "                os.remove(f\"started_{self.test}.txt\")\n",
    "                break\n",
    " \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        # write file stop.txt to signal to the background process to stop\n",
    "        with open(f\"stop_{self.test}.txt\", \"w\") as f:\n",
    "            print(\"Stopping stats collection\")\n",
    "            f.write(\"STOP\")\n",
    "\n",
    "        while True:\n",
    "            if os.path.isfile(f\"{self.directory}/crypto_spider_5g_fcnn_optimized_benchmark_{self.test}_stats.pkl\"):\n",
    "                print(\"Stats file found\")\n",
    "                print(self.test)\n",
    "\n",
    "                break\n",
    "\n",
    "def perform_inference(model):\n",
    "    model.predict(data_transformed)\n",
    "    \n",
    "def perform_inference_onnx(sess):\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_name = sess.get_outputs()[0].name\n",
    "    sess.run([output_name], {input_name: data_transformed.astype(np.float32)})\n",
    "    \n",
    "def perform_inference_tflite(interpreter, batch_size):\n",
    "    # check input details and convert data to the right type\n",
    "    input_details = interpreter.get_input_details()\n",
    "    print(input_details)\n",
    "    dtype = input_details[0]['dtype']\n",
    "    input_data = np.array(data_transformed, dtype=dtype)\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    interpreter.resize_tensor_input(input_details['index'], (batch_size, input_data.shape[1]))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    # create batches of data_transformed\n",
    "    batch_size = batch_size\n",
    "    \n",
    "    preds = []\n",
    "\n",
    "    for i in range(0, len(data_transformed), batch_size):\n",
    "        batch = data_transformed[i:i+batch_size]\n",
    "        # print(f\"Batch {i//batch_size} has {len(batch)} elements\")\n",
    "        \n",
    "        batch_data = np.array(batch, dtype=dtype)\n",
    "        \n",
    "        if len(batch) == batch_size:        \n",
    "            interpreter.set_tensor(input_details['index'], batch_data)\n",
    "            interpreter.invoke()\n",
    "            output_data = interpreter.get_tensor(output_details['index'])\n",
    "            \n",
    "            preds.append(output_data)\n",
    "    \n",
    "    return np.concatenate(preds)\n",
    "\n",
    "def save_model_to_h5(keras_model, model_filename):\n",
    "    keras_model.save(f\"{model_filename}.h5\", include_optimizer=False)\n",
    "    \n",
    "    return f\"{model_filename}.h5\"\n",
    "\n",
    "def save_model_to_tflite(tflite_model, model_filename):\n",
    "    with open(f\"{model_filename}.tflite\", 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    return f\"{model_filename}.tflite\"\n",
    "\n",
    "def save_model_to_onnx(onnx_model, model_filename):\n",
    "    onnx.save(onnx_model, f\"{model_filename}.onnx\")\n",
    "    \n",
    "    return f\"{model_filename}.onnx\"\n",
    "    \n",
    "def gzip_model(model_path):\n",
    "    with open(model_path, 'rb') as f_in, open(f\"{model_path}.gz\", 'wb') as f_out:\n",
    "        f_out.write(gzip.compress(f_in.read()))\n",
    "        \n",
    "    os.remove(model_path)\n",
    "    \n",
    "    return f\"{model_path}.gz\"\n",
    "\n",
    "def unzip_model(model_path):\n",
    "    with gzip.open(model_path, 'rb') as f_in, open(model_path[:-3], 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "            \n",
    "    return model_path[:-3]\n",
    "\n",
    "def save_model(model, ext):\n",
    "    model_filename = \"model\"\n",
    "    \n",
    "    if ext == \"h5\":\n",
    "        model_path = save_model_to_h5(model, model_filename)\n",
    "    elif ext == \"onnx\":\n",
    "        model_path = save_model_to_onnx(model, model_filename)\n",
    "    elif ext == \"tflite\":\n",
    "        model_path = save_model_to_tflite(model, model_filename)\n",
    "    else:\n",
    "        raise Exception(\"Model format not supported\")\n",
    "    \n",
    "    model_path = gzip_model(model_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "def load_model_from_h5(model_path) -> tf.keras.Model:\n",
    "    keras_model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    return keras_model\n",
    "\n",
    "def load_model_from_tflite(model_path) -> tf.lite.Interpreter:\n",
    "    tflite_model = tf.lite.Interpreter(model_path=model_path)\n",
    "    tflite_model.allocate_tensors()\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "def load_model_from_onnx(model_path) -> onnx.ModelProto:\n",
    "    # onnx_model = onnx.load(model_path)\n",
    "    \n",
    "    # return onnx_model\n",
    "    \n",
    "    # onnx_model = rt.InferenceSession(model_path, providers=[\"CUDAExecutionProvider\"])\n",
    "    onnx_model = rt.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "    \n",
    "    return onnx_model\n",
    "\n",
    "def load_model(ext):\n",
    "    model_path = f\"model.{ext}.gz\"\n",
    "    \n",
    "    model_path = unzip_model(model_path)\n",
    "    \n",
    "    if ext == \"h5\":\n",
    "        model = load_model_from_h5(model_path)\n",
    "    elif ext == \"onnx\":\n",
    "        model = load_model_from_onnx(model_path)\n",
    "    elif ext == \"tflite\":\n",
    "        model = load_model_from_tflite(model_path)\n",
    "    else:\n",
    "        raise Exception(\"Model format not supported\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def export_keras_model_to_tflite(model):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "def export_keras_model_to_onnx(model):\n",
    "    spec = (tf.TensorSpec(shape=(1, 16), dtype=tf.float32, name='input'),)\n",
    "    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, output_path=\"model.onnx\")\n",
    "    \n",
    "    return model_proto\n",
    "\n",
    "def export_tflite_model_to_onnx(model_path):\n",
    "    spec = (tf.TensorSpec(shape=(1, 16), dtype=tf.float32, name='input'),)\n",
    "    model_proto, _ = tf2onnx.convert.from_tflite(model_path, output_path=\"model.onnx\")\n",
    "    \n",
    "    return model_proto\n",
    "\n",
    "def get_gzipped_model_size(ext):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  if os.path.exists(f\"model.{ext}.gz\"):\n",
    "    return os.path.getsize(f\"model.{ext}.gz\")\n",
    "  else:\n",
    "      RuntimeError(f\"Model with extension \\\"{ext}\\\" not found\")\n",
    "\n",
    "def delete_model(ext):\n",
    "    if os.path.exists(f\"model.{ext}.gz\"):\n",
    "        os.remove(f\"model.{ext}.gz\")\n",
    "    else:\n",
    "        RuntimeError(f\"Model with extension \\\"{ext}\\\" not found\")\n",
    "    \n",
    "    if os.path.exists(f\"model.{ext}\"):\n",
    "        os.remove(f\"model.{ext}\")\n",
    "    else:\n",
    "        RuntimeError(f\"Model with extension \\\"{ext}\\\" not found\")\n",
    "\n",
    "def perform_evaluation_onnx(experiment, device=\"CPU\"):\n",
    "    assert device in [\"CPU\", \"GPU\"]\n",
    "\n",
    "    logger.info(f\"Evaluating {experiment} model on test set\")\n",
    "\n",
    "    # Load model\n",
    "    sess = load_model(ext=\"onnx\")\n",
    "    sess.set_providers([\"CPUExecutionProvider\"] if device == \"CPU\" else [\"CUDAExecutionProvider\"])\n",
    "\n",
    "    # Load test set\n",
    "    test_data_transformed = standard.transform(df_test[gf])\n",
    "    \n",
    "    # get input shape\n",
    "    input_shape = sess.get_inputs()[0].shape\n",
    "    logger.info(f\"Input shape: {input_shape}\")\n",
    "    \n",
    "    # get output shape\n",
    "    output_shape = sess.get_outputs()[0].shape\n",
    "    logger.info(f\"Output shape: {output_shape}\")\n",
    "    \n",
    "    # transform data to the expected tensor type\n",
    "    test_data_transformed = test_data_transformed.astype(np.float32)\n",
    "\n",
    "    # Evaluate model\n",
    "    results = sess.run(None, {sess.get_inputs()[0].name: test_data_transformed})\n",
    "    \n",
    "    # get predictions\n",
    "    predictions = results[0]\n",
    "    \n",
    "    \n",
    "    if ml_task == \"binary_classification\":\n",
    "        # convert predictions to labels\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(df_test[\"tag\"], predictions)\n",
    "        f1 = f1_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "        auc = roc_auc_score(df_test[\"tag\"], predictions)\n",
    "        recall = recall_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "        precision = precision_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "        balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], predictions)\n",
    "        matthews = matthews_corrcoef(df_test[\"tag\"], predictions)\n",
    "\n",
    "        logger.info(f\"Accuracy: {accuracy}\")\n",
    "        logger.info(f\"F1 score: {f1}\")\n",
    "        logger.info(f\"AUC: {auc}\")\n",
    "        logger.info(f\"Recall: {recall}\")\n",
    "        logger.info(f\"Precision: {precision}\")\n",
    "        logger.info(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "        logger.info(f\"Matthews correlation coefficient: {matthews}\")\n",
    "        \n",
    "        test_results = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"auc\": auc,\n",
    "            \"recall\": recall,\n",
    "            \"precision\": precision,\n",
    "            \"balanced_accuracy\": balanced_accuracy,\n",
    "            \"matthews\": matthews\n",
    "        }\n",
    "        \n",
    "        # save to datframe\n",
    "        df_evaluation = pd.DataFrame([[experiment, device, accuracy, f1, auc, recall, precision, balanced_accuracy, matthews]], columns=[\"experiment\", \"device\", \"accuracy\", \"f1\", \"auc\", \"recall\", \"precision\", \"balanced_accuracy\", \"matthews\"])\n",
    "    elif ml_task == \"regression\":\n",
    "        mae = mean_absolute_error(df_test[\"tag\"], predictions)\n",
    "        mse = mean_squared_error(df_test[\"tag\"], predictions)\n",
    "        mape = mean_absolute_percentage_error(df_test[\"tag\"], predictions)\n",
    "        smape = 1/len(df_test[\"tag\"]) * np.sum(2 * np.abs(predictions - df_test[\"tag\"]) / (np.abs(predictions) + np.abs(df_test[\"tag\"])))\n",
    "        \n",
    "        logger.info(f\"MAE: {mae}\")\n",
    "        logger.info(f\"MSE: {mse}\")\n",
    "        logger.info(f\"MAPE: {mape}\")\n",
    "        logger.info(f\"SMAPE: {smape}\")\n",
    "        \n",
    "        test_results = {\n",
    "            \"mae\": mae,\n",
    "            \"mse\": mse,\n",
    "            \"mape\": mape,\n",
    "            \"smape\": smape\n",
    "        }\n",
    "            \n",
    "        # save to datframe\n",
    "        df_evaluation = pd.DataFrame([[experiment, device, mae, mse, mape, smape]], columns=[\"experiment\", \"device\", \"mae\", \"mse\", \"mape\", \"smape\"])\n",
    "    \n",
    "    with open(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.pkl\", \"wb\") as f:\n",
    "        pickle.dump(test_results, f)\n",
    "    \n",
    "    return df_evaluation\n",
    "\n",
    "def perform_evaluation_tflite(experiment, device=\"CPU\"):\n",
    "    assert device in [\"CPU\", \"GPU\"]\n",
    "\n",
    "    logger.info(f\"Evaluating {experiment} model on test set\")\n",
    "\n",
    "    # Load model\n",
    "    interpreter = load_model(ext=\"tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Load test set\n",
    "    test_data_transformed = standard.transform(df_test[gf])\n",
    "    \n",
    "    # get input shape\n",
    "    input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "    logger.info(f\"Input shape: {input_shape}\")\n",
    "    \n",
    "    # get output shape\n",
    "    output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "    logger.info(f\"Output shape: {output_shape}\")\n",
    "    \n",
    "    # transform data to the expected tensor type\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    dtype = input_details[\"dtype\"]    \n",
    "    input_data = np.array(test_data_transformed, dtype=dtype)\n",
    "    \n",
    "    # reshape model input\n",
    "    batch_size = 256\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    interpreter.resize_tensor_input(input_details['index'], (batch_size, input_data.shape[1]))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    preds = []\n",
    "\n",
    "    # create batches of test_data_transformed\n",
    "    for i in range(0, len(test_data_transformed), batch_size):\n",
    "        batch = test_data_transformed[i:i+batch_size]\n",
    "        # print(f\"Batch {i//batch_size} has {len(batch)} elements\")\n",
    "        \n",
    "        batch_data = np.array(batch, dtype=dtype)\n",
    "        \n",
    "        if len(batch) == batch_size:        \n",
    "            interpreter.set_tensor(input_details['index'], batch_data)\n",
    "            interpreter.invoke()\n",
    "            output_data = interpreter.get_tensor(output_details['index'])\n",
    "            \n",
    "            preds.append(output_data)\n",
    "    \n",
    "    predictions = np.concatenate(preds)\n",
    "    \n",
    "    # take the labels of the test set\n",
    "    test_labels = df_test[\"tag\"].values[:len(predictions)]\n",
    "\n",
    "    # Evaluate model\n",
    "    # accuracy = accuracy_score(test_labels, predictions)\n",
    "    # balanced_accuracy = balanced_accuracy_score(test_labels, predictions)\n",
    "    # f1 = f1_score(test_labels, predictions, average=\"weighted\")\n",
    "\n",
    "    # logger.info(f\"Accuracy: {accuracy}\")\n",
    "    # logger.info(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "    # logger.info(f\"F1 score: {f1}\")\n",
    "    \n",
    "    # conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "    # logger.info(f\"Confusion matrix: {conf_matrix}\")\n",
    "    \n",
    "    # test_results = {\n",
    "    #     \"accuracy\": accuracy,\n",
    "    #     \"balanced_accuracy\": balanced_accuracy,\n",
    "    #     \"f1\": f1,\n",
    "    # }\n",
    "\n",
    "    # with open(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(test_results, f)\n",
    "    \n",
    "    # # save to datframe\n",
    "    # df_evaluation = pd.DataFrame([[experiment, device, accuracy, balanced_accuracy, f1]], columns=[\"experiment\", \"device\", \"accuracy\", \"balanced_accuracy\", \"f1\"])\n",
    "    \n",
    "    if ml_task == \"binary_classification\":\n",
    "        # convert predictions to labels\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        f1 = f1_score(test_labels, predictions, average=\"weighted\")\n",
    "        auc = roc_auc_score(test_labels, predictions)\n",
    "        recall = recall_score(test_labels, predictions, average=\"weighted\")\n",
    "        precision = precision_score(test_labels, predictions, average=\"weighted\")\n",
    "        balanced_accuracy = balanced_accuracy_score(test_labels, predictions)\n",
    "        matthews = matthews_corrcoef(test_labels, predictions)\n",
    "\n",
    "        logger.info(f\"Accuracy: {accuracy}\")\n",
    "        logger.info(f\"F1 score: {f1}\")\n",
    "        logger.info(f\"AUC: {auc}\")\n",
    "        logger.info(f\"Recall: {recall}\")\n",
    "        logger.info(f\"Precision: {precision}\")\n",
    "        logger.info(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "        logger.info(f\"Matthews correlation coefficient: {matthews}\")\n",
    "        \n",
    "        test_results = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"auc\": auc,\n",
    "            \"recall\": recall,\n",
    "            \"precision\": precision,\n",
    "            \"balanced_accuracy\": balanced_accuracy,\n",
    "            \"matthews\": matthews\n",
    "        }\n",
    "        \n",
    "        # save to datframe\n",
    "        df_evaluation = pd.DataFrame([[experiment, device, accuracy, f1, auc, recall, precision, balanced_accuracy, matthews]], columns=[\"experiment\", \"device\", \"accuracy\", \"f1\", \"auc\", \"recall\", \"precision\", \"balanced_accuracy\", \"matthews\"])\n",
    "    elif ml_task == \"regression\":\n",
    "        mae = mean_absolute_error(test_labels, predictions)\n",
    "        mse = mean_squared_error(test_labels, predictions)\n",
    "        mape = mean_absolute_percentage_error(test_labels, predictions)\n",
    "        smape = 1/len(test_labels) * np.sum(2 * np.abs(predictions - test_labels) / (np.abs(predictions) + np.abs(test_labels)))\n",
    "        \n",
    "        logger.info(f\"MAE: {mae}\")\n",
    "        logger.info(f\"MSE: {mse}\")\n",
    "        logger.info(f\"MAPE: {mape}\")\n",
    "        logger.info(f\"SMAPE: {smape}\")\n",
    "        \n",
    "        test_results = {\n",
    "            \"mae\": mae,\n",
    "            \"mse\": mse,\n",
    "            \"mape\": mape,\n",
    "            \"smape\": smape\n",
    "        }\n",
    "            \n",
    "        # save to datframe\n",
    "        df_evaluation = pd.DataFrame([[experiment, device, mae, mse, mape, smape]], columns=[\"experiment\", \"device\", \"mae\", \"mse\", \"mape\", \"smape\"])\n",
    "    \n",
    "    with open(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.pkl\", \"wb\") as f:\n",
    "        pickle.dump(test_results, f)\n",
    "    \n",
    "    return df_evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:02:51.070478Z",
     "start_time": "2023-03-08T23:02:50.991297Z"
    }
   },
   "outputs": [],
   "source": [
    "def benchmark_training(device, post_training_optimizations=None, training_aware_optimizations=None):\n",
    "    gpu_memory_usage = get_gpu_memory_system()\n",
    "    gpu_memory_usage = gpu_memory_usage.replace('MiB', '')\n",
    "    gpu_memory_usage = float(gpu_memory_usage)\n",
    "\n",
    "    assert gpu_memory_usage == idle_gpu_memory_usage\n",
    "\n",
    "    print(f\"GPU memory usage: {gpu_memory_usage} MiB\")\n",
    "    \n",
    "    pid = os.getpid()\n",
    "\n",
    "    with StatsCollectionManager(test=\"training\", sampling_rate=stats_sampling_rate, pid=pid) as training_scm:\n",
    "        with measure_time() as training_time_measure:\n",
    "            # Training\n",
    "            if training_aware_optimizations is None:\n",
    "                logging.info(\"Training without training-aware optimizations\")\n",
    "\n",
    "                with tf.device(device):\n",
    "                    model = baseline_model(input_dim=len(gf), n_output=2)\n",
    "                    model, history = train_model(model)\n",
    "                    tflite_model = export_keras_model_to_tflite(model)\n",
    "                    model_path = save_model_to_tflite(tflite_model, model_filename=\"model\")\n",
    "                    # model_path = apply_post_training_optimizations(model, post_training_optimizations)\n",
    "                    \n",
    "                    if post_training_optimizations is not None:\n",
    "                        if \"full_integer_quantization\" in post_training_optimizations:\n",
    "                            model_path = full_integer_quantization(model)\n",
    "                        elif \"float16_quantization\" in post_training_optimizations:\n",
    "                            model_path = float16_quantization(model)\n",
    "                        elif \"float16_int8_quantization\" in post_training_optimizations:\n",
    "                            model_path = float16_int8_quantization(model)\n",
    "                        \n",
    "                        # print(\"Exporting TFLite model to ONNX\")\n",
    "                        # onnx_model = export_tflite_model_to_onnx(model_path)\n",
    "            else:\n",
    "                with tf.device(device):\n",
    "                    logging.info(\"Training with training-aware optimizations\")\n",
    "                    model = baseline_model(input_dim=len(gf), n_output=2)\n",
    "                    model, history = train_model(model)\n",
    "                    # model, history = apply_training_aware_optimizations(model, training_aware_optimizations)\n",
    "                                        \n",
    "                    if training_aware_optimizations is not None:\n",
    "                        if \"knowledge_distillation\" in training_aware_optimizations:\n",
    "                            if \"pruning\" in training_aware_optimizations or \"quantization_aware_training\" in training_aware_optimizations:\n",
    "                                model_path, _history = knowledge_distillation(model, convert_to_tflite=False)\n",
    "                            else:\n",
    "                                model_path, _history = knowledge_distillation(model)\n",
    "                        \n",
    "                        if \"pruning\" in training_aware_optimizations:\n",
    "                            if \"quantization_aware_training\" in training_aware_optimizations:\n",
    "                                model_path, _history = pruning(model, convert_to_tflite=False)\n",
    "                            else:\n",
    "                                model_path, _history = pruning(model)\n",
    "                            \n",
    "                        if \"quantization_aware_training\" in training_aware_optimizations:\n",
    "                            model_path, _history = quantization_aware_training(model)\n",
    "                        \n",
    "                        # print(\"Exporting TFLite model to ONNX\")\n",
    "                        # onnx_model = export_tflite_model_to_onnx(model_path)\n",
    "    \n",
    "    gpu_memory_usage = get_gpu_memory_system()\n",
    "    gpu_memory_usage = gpu_memory_usage.replace('MiB', '')\n",
    "    gpu_memory_usage = float(gpu_memory_usage)\n",
    "\n",
    "    print(f\"GPU memory usage: {gpu_memory_usage} MiB\")\n",
    "\n",
    "    assert gpu_memory_usage == idle_gpu_memory_usage\n",
    "\n",
    "    # # Get total parameters count\n",
    "    # if isinstance(model, tf.keras.Model):\n",
    "    #     num_params = model.count_params()\n",
    "    # elif isinstance(model, tf.lite.TFLiteConverter): # NOT SUPPORTED\n",
    "    #     num_params = model._num_parameters\n",
    "    # else:\n",
    "    #     raise Exception(f\"Unknown model type: {type(model)}\")\n",
    "    \n",
    "    # logger.info(f\"Number of total parameters: {num_params}\")\n",
    "\n",
    "    # # Save model to ONNX\n",
    "    # if isinstance(model, tf.keras.Model):\n",
    "    #     # tflite_model = export_keras_model_to_tflite(model)\n",
    "    #     # onnx_model = export_keras_model_to_onnx(model)\n",
    "    #     print(\"Exporting Keras model to ONNX\")\n",
    "    #     onnx_model = export_keras_model_to_onnx(model)\n",
    "    # else:\n",
    "    #     print(\"Exporting TFLite model to ONNX\")\n",
    "    #     onnx_model = export_tflite_model_to_onnx(model_path)\n",
    "    \n",
    "    # save_model(onnx_model, ext=\"onnx\")\n",
    "    \n",
    "    gzip_model(model_path)\n",
    "\n",
    "    # Get training time\n",
    "    training_time = training_time_measure()\n",
    "\n",
    "    # Write training time to file\n",
    "    with open(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_time.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"training_time\": training_time}, f)\n",
    "        \n",
    "\n",
    "def benchmark_inference(device):\n",
    "    gpu_memory_usage = get_gpu_memory_system()\n",
    "    gpu_memory_usage = gpu_memory_usage.replace('MiB', '')\n",
    "    gpu_memory_usage = float(gpu_memory_usage)\n",
    "\n",
    "    assert gpu_memory_usage == idle_gpu_memory_usage\n",
    "\n",
    "    print(f\"GPU memory usage: {gpu_memory_usage} MiB\")\n",
    "    \n",
    "    pid = os.getpid()\n",
    "\n",
    "    # load model\n",
    "    # model = load_gzipped_model_from_h5()\n",
    "    # model = load_model(ext=\"onnx\")\n",
    "    interpreter = load_model(ext=\"tflite\")\n",
    "    \n",
    "    # # resize input\n",
    "    # batch_size = 256\n",
    "    # tensor_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    # interpreter.resize_tensor_input(tensor_index, [batch_size, 16])\n",
    "    # interpreter.allocate_tensors()\n",
    "\n",
    "    # Inference test\n",
    "    with StatsCollectionManager(test=\"inference\", sampling_rate=stats_sampling_rate, pid=pid) as inference_scm:\n",
    "        with measure_time() as inference_time_measure:\n",
    "            with tf.device(device):\n",
    "                # if post_training_optimizations is None and training_aware_optimizations is None:\n",
    "                #     perform_inference(model)\n",
    "                # elif \"quantization\" in post_training_optimizations or \"quantization\" in training_aware_optimizations:\n",
    "                #     perform_inference_quantized(model)\n",
    "                # else:\n",
    "                #     perform_inference(model)\n",
    "                # perform_inference_onnx(model)\n",
    "                perform_inference_tflite(interpreter, batch_size=256)\n",
    "\n",
    "    gpu_memory_usage = get_gpu_memory_system()\n",
    "    gpu_memory_usage = gpu_memory_usage.replace('MiB', '')\n",
    "    gpu_memory_usage = float(gpu_memory_usage)\n",
    "\n",
    "    print(f\"GPU memory usage: {gpu_memory_usage} MiB\")\n",
    "\n",
    "    assert gpu_memory_usage == idle_gpu_memory_usage\n",
    "\n",
    "    # Get inference and load times\n",
    "    inference_time = inference_time_measure()\n",
    "\n",
    "    # Write inference time to file\n",
    "    with open(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_time.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"inference_time\": inference_time}, f)\n",
    "\n",
    "def benchmark_load(device):\n",
    "    gpu_memory_usage = get_gpu_memory_system()\n",
    "    gpu_memory_usage = gpu_memory_usage.replace('MiB', '')\n",
    "    gpu_memory_usage = float(gpu_memory_usage)\n",
    "\n",
    "    assert gpu_memory_usage == idle_gpu_memory_usage\n",
    "\n",
    "    print(f\"GPU memory usage: {gpu_memory_usage} MiB\")\n",
    "    \n",
    "    pid = os.getpid()\n",
    "\n",
    "    # load test\n",
    "    with StatsCollectionManager(test=\"load\", sampling_rate=stats_sampling_rate, pid=pid) as load_scm:\n",
    "        with measure_time() as load_time_measure:\n",
    "            with tf.device(device):\n",
    "                # if post_training_optimizations is None:\n",
    "                #     load_gzipped_model()\n",
    "                # elif \"pruning\" in post_training_optimizations:\n",
    "                #     load_gzipped_pruned_model()\n",
    "                # else:\n",
    "                #     load_gzipped_model()\n",
    "                load_model(ext=\"tflite\")\n",
    "    \n",
    "    gpu_memory_usage = get_gpu_memory_system()\n",
    "    gpu_memory_usage = gpu_memory_usage.replace('MiB', '')\n",
    "    gpu_memory_usage = float(gpu_memory_usage)\n",
    "\n",
    "    print(f\"GPU memory usage: {gpu_memory_usage} MiB\")\n",
    "\n",
    "    assert gpu_memory_usage == idle_gpu_memory_usage\n",
    "\n",
    "    # Get load time\n",
    "    load_time = load_time_measure()\n",
    "\n",
    "    # Write load time to file\n",
    "    with open(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_time.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"load_time\": load_time}, f)\n",
    "\n",
    "def benchmark(experiment, device=\"GPU\"):\n",
    "    assert device in [\"CPU\", \"GPU\"]\n",
    "\n",
    "    logger.info(\"Benchmarking {} on {}\".format(experiment, device))\n",
    "\n",
    "    post_training_optimizations = experiments[experiment][\"post_training_optimizations\"]\n",
    "    training_aware_optimizations = experiments[experiment][\"training_aware_optimizations\"]\n",
    "\n",
    "    training_times_list = []\n",
    "    inference_times_list = []\n",
    "    load_times_list = []\n",
    "    \n",
    "    training_stats_list = []\n",
    "    inference_stats_list = []\n",
    "    load_stats_list = []\n",
    "    \n",
    "    for i in range(0, num_trials):\n",
    "        logger.info(f\"Trial {i}\")\n",
    "\n",
    "        p_training = multiprocessing.Process(target=benchmark_training, args=(device, post_training_optimizations, training_aware_optimizations))\n",
    "        p_training.start()\n",
    "        p_training.join()\n",
    "\n",
    "        p_inference = multiprocessing.Process(target=benchmark_inference, args=(device,))\n",
    "        p_inference.start()\n",
    "        p_inference.join()\n",
    "\n",
    "        p_load = multiprocessing.Process(target=benchmark_load, args=(device,))\n",
    "        p_load.start()\n",
    "        p_load.join()\n",
    "        \n",
    "        # Read training, inference and load times to file\n",
    "        with open(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_time.pkl\", \"rb\") as f:\n",
    "            training_time = pickle.load(f)\n",
    "        \n",
    "        with open(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_time.pkl\", \"rb\") as f:\n",
    "            inference_time = pickle.load(f)\n",
    "        \n",
    "        with open(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_time.pkl\", \"rb\") as f:\n",
    "            load_time = pickle.load(f)\n",
    "\n",
    "        inference_time = inference_time[\"inference_time\"]\n",
    "        training_time = training_time[\"training_time\"]\n",
    "        load_time = load_time[\"load_time\"]\n",
    "\n",
    "        # Delete trial times files\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_time.pkl\")\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_time.pkl\")\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_time.pkl\")\n",
    "        \n",
    "        # Read training, inference and load stats to file\n",
    "        with open('poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_stats.pkl', 'rb') as f:\n",
    "            training_stats = pickle.load(f)\n",
    "            \n",
    "        with open('poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats.pkl', 'rb') as f:\n",
    "            inference_stats = pickle.load(f)\n",
    "\n",
    "        with open('poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_stats.pkl', 'rb') as f:\n",
    "            load_stats = pickle.load(f)\n",
    "    \n",
    "        # Delete trial stats files\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_stats.pkl\")\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats.pkl\")\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_stats.pkl\")\n",
    "\n",
    "        logger.info(\"Trial {} - Training time: {:.2f}s\".format(i, training_time))\n",
    "        logger.info(\"Trial {} - Inference time: {:.2f}s\".format(i, inference_time))\n",
    "        logger.info(\"Trial {} - Load time: {:.2f}s\".format(i, load_time))\n",
    "        logger.info(\"Trial {} - Total time: {:.2f}s\".format(i, training_time + inference_time + load_time))\n",
    "\n",
    "        # Save training, inference and load times\n",
    "        training_times_list.append(training_time)\n",
    "        inference_times_list.append(inference_time)\n",
    "        load_times_list.append(load_time)\n",
    "\n",
    "        # Save training, inference and load stats\n",
    "        training_stats_list.append(training_stats)\n",
    "        inference_stats_list.append(inference_stats)\n",
    "        load_stats_list.append(load_stats)\n",
    "        \n",
    "        if i != num_trials - 1:\n",
    "            # Delete models\n",
    "            delete_model(\"onnx\")\n",
    "            delete_model(\"tflite\")\n",
    "            delete_model(\"h5\")\n",
    "\n",
    "    average_training_time = np.mean(training_times_list)\n",
    "    std_dev_training_time = np.std(training_times_list)\n",
    "    max_training_time = np.max(training_times_list)\n",
    "\n",
    "    average_inference_time = np.mean(inference_times_list)\n",
    "    std_dev_inference_time = np.std(inference_times_list)\n",
    "    max_inference_time = np.max(inference_times_list)\n",
    "\n",
    "    average_load_time = np.mean(load_times_list)\n",
    "    std_dev_load_time = np.std(load_times_list)\n",
    "    max_load_time = np.max(load_times_list)\n",
    "\n",
    "    # Time spent on training\n",
    "    logger.info(\"Average training time: {}\".format(np.round(average_training_time, 2)))\n",
    "    logger.info(\"Standard deviation of training time: {}\".format(np.round(std_dev_training_time, 2)))\n",
    "    logger.info(\"Max. training time: {}\".format(np.round(max_training_time, 2)))\n",
    "\n",
    "    # Time spent on inference\n",
    "    logger.info(\"Average inference time: {}\".format(np.round(average_inference_time, 2)))\n",
    "    logger.info(\"Standard deviation of inference time: {}\".format(np.round(std_dev_inference_time, 2)))\n",
    "    logger.info(\"Max. inference time: {}\".format(np.round(max_inference_time, 2)))\n",
    "\n",
    "    # Time spent on loading\n",
    "    logger.info(\"Average load time: {}\".format(np.round(average_load_time, 2)))\n",
    "    logger.info(\"Standard deviation of load time: {}\".format(np.round(std_dev_load_time, 2)))\n",
    "    logger.info(\"Max. load time: {}\".format(np.round(max_load_time, 2)))\n",
    "\n",
    "    # Get average training, inference and load times\n",
    "    average_training_stats_list = get_average_stats(training_stats_list, average_training_time)\n",
    "    std_dev_training_stats_list = get_std_dev_stats(training_stats_list, std_dev_training_time)\n",
    "    max_training_stats_list = get_max_stats(training_stats_list, max_training_time)\n",
    "\n",
    "    average_inference_stats_list = get_average_stats(inference_stats_list, average_inference_time)\n",
    "    std_dev_inference_stats_list = get_std_dev_stats(inference_stats_list, std_dev_inference_time)\n",
    "    max_inference_stats_list = get_max_stats(inference_stats_list, max_inference_time)\n",
    "\n",
    "    average_load_stats_list = get_average_stats(load_stats_list, average_load_time)\n",
    "    std_dev_load_stats_list = get_std_dev_stats(load_stats_list, std_dev_load_time)\n",
    "    max_load_stats_list = get_max_stats(load_stats_list, max_load_time)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = get_gzipped_model_size(ext=\"tflite\")\n",
    "    logger.info(f\"Model size (gzip): {model_size}\")\n",
    "\n",
    "    # Save training, inference and load metrics to dataframe\n",
    "    stats = training_stats_list[0][0].keys()\n",
    "    \n",
    "    average_traning_stats_names = [f\"average_training_{stat}\" for stat in stats]\n",
    "    std_dev_training_stats_names = [f\"std_dev_training_{stat}\" for stat in stats]\n",
    "    max_training_stats_names = [f\"max_training_{stat}\" for stat in stats]\n",
    "    \n",
    "    average_inference_stats_names = [f\"average_inference_{stat}\" for stat in stats]\n",
    "    std_dev_inference_stats_names = [f\"std_dev_inference_{stat}\" for stat in stats]\n",
    "    max_inference_stats_names = [f\"max_inference_{stat}\" for stat in stats]\n",
    "    \n",
    "    average_load_stats_names = [f\"average_load_{stat}\" for stat in stats]\n",
    "    std_dev_load_stats_names = [f\"std_dev_load_{stat}\" for stat in stats]\n",
    "    max_load_stats_names = [f\"max_load_{stat}\" for stat in stats]\n",
    "\n",
    "    df_times_columns = [\"experiment\", \"device\", \"average_training_time\", \"std_dev_training_time\", \"max_training_time\", \"average_inference_time\", \"std_dev_inference_time\", \"max_inference_time\", \"average_load_time\", \"std_dev_load_time\", \"max_load_time\", \"model_size\"]\n",
    "    df_times = pd.DataFrame(columns=df_times_columns)\n",
    "    times_row = [experiment, device, average_training_time, std_dev_training_time, max_training_time, average_inference_time, std_dev_inference_time, max_inference_time, average_load_time, std_dev_load_time, max_load_time, model_size]\n",
    "    df_times.loc[0] = times_row\n",
    "\n",
    "    df_training_stats_columns = [\"experiment\", \"device\", \"snapshot\", *average_traning_stats_names, *std_dev_training_stats_names, *max_training_stats_names]\n",
    "    df_training_stats = pd.DataFrame(columns=df_training_stats_columns)\n",
    "    \n",
    "    for index, _snapshot in enumerate(average_training_stats_list):\n",
    "        row = np.array([experiment, device, index])\n",
    "        row = np.append(row, average_training_stats_list[index])\n",
    "        row = np.append(row, std_dev_training_stats_list[index])\n",
    "        row = np.append(row, max_training_stats_list[index])\n",
    "    \n",
    "        df_training_stats.loc[index] = row\n",
    "\n",
    "    df_inference_stats_columns = [\"experiment\", \"device\", \"snapshot\", *average_inference_stats_names, *std_dev_inference_stats_names, *max_inference_stats_names]\n",
    "    df_inference_stats = pd.DataFrame(columns=df_inference_stats_columns)\n",
    "    \n",
    "    for index, _snapshot in enumerate(average_inference_stats_list):\n",
    "        row = np.array([experiment, device, index])\n",
    "        row = np.append(row, average_inference_stats_list[index])\n",
    "        row = np.append(row, std_dev_inference_stats_list[index])\n",
    "        row = np.append(row, max_inference_stats_list[index])\n",
    "\n",
    "        df_inference_stats.loc[index] = row\n",
    "\n",
    "    df_load_stats_columns = [\"experiment\", \"device\", \"snapshot\", *average_load_stats_names, *std_dev_load_stats_names, *max_load_stats_names]\n",
    "    df_load_stats = pd.DataFrame(columns=df_load_stats_columns)\n",
    "    \n",
    "    for index, _snapshot in enumerate(average_load_stats_list):\n",
    "        row = np.array([experiment, device, index])\n",
    "        row = np.append(row, average_load_stats_list[index])\n",
    "        row = np.append(row, std_dev_load_stats_list[index])\n",
    "        row = np.append(row, max_load_stats_list[index])\n",
    "\n",
    "        df_load_stats.loc[index] = row\n",
    "\n",
    "    # # Save average and std_dev of training, inference and load stats to log file\n",
    "    # save_stats_to_logfile(test=\"training\", average_stats=average_training_stats_list, std_dev_stats=std_dev_training_stats_list)\n",
    "    # save_stats_to_logfile(test=\"inference\", average_stats=average_inference_stats_list, std_dev_stats=std_dev_inference_stats_list)\n",
    "    # save_stats_to_logfile(test=\"load\", average_stats=average_load_stats_list, std_dev_stats=std_dev_load_stats_list)\n",
    "\n",
    "    return df_times, df_training_stats, df_inference_stats, df_load_stats\n",
    "\n",
    "def run_benchmark(experiments, devices):\n",
    "    for experiment in experiments:\n",
    "        for device in devices:\n",
    "            print(f\"Running experiment {experiment} on device {device}\")\n",
    "            df_times, df_training_stats, df_inference_stats, df_load_stats = benchmark(experiment=experiment, device=device)\n",
    "            df_evaluation = perform_evaluation_tflite(experiment=experiment, device=device)\n",
    "\n",
    "            # load times dataframe from file and append new df\n",
    "            if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_times.csv\"):\n",
    "                df_times.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_times.csv\", mode=\"a\", header=False, index=False)\n",
    "            else:\n",
    "                df_times.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_times.csv\", index=False)\n",
    "\n",
    "            # load training, inference and load stats dataframe from file and append new df\n",
    "            if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_stats.csv\"):\n",
    "                df_training_stats.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_stats.csv\", mode=\"a\", header=False, index=False)\n",
    "            else:\n",
    "                df_training_stats.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_stats.csv\", index=False)\n",
    "            \n",
    "            if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats.csv\"):\n",
    "                df_inference_stats.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats.csv\", mode=\"a\", header=False, index=False)\n",
    "            else:\n",
    "                df_inference_stats.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats.csv\", index=False)\n",
    "            \n",
    "            if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_stats.csv\"):\n",
    "                df_load_stats.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_stats.csv\", mode=\"a\", header=False, index=False)\n",
    "            else:\n",
    "                df_load_stats.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_stats.csv\", index=False)\n",
    "            \n",
    "            # load evaluation dataframe from file and append new df\n",
    "            if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.csv\"):\n",
    "                df_evaluation.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.csv\", mode=\"a\", header=False, index=False)\n",
    "            else:\n",
    "                df_evaluation.to_csv(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.csv\", index=False)\n",
    "                \n",
    "            \n",
    "def clean_files():\n",
    "    if os.path.exists(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_times.csv\"):\n",
    "        os.remove(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_times.csv\")\n",
    "\n",
    "    if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_stats.csv\"):\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_stats.csv\")\n",
    "\n",
    "    if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats.csv\"):\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats.csv\")\n",
    "\n",
    "    if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_stats.csv\"):\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_stats.csv\")\n",
    "\n",
    "    if os.path.exists(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.csv\"):\n",
    "        os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.csv\")\n",
    "\n",
    "def load_files():\n",
    "    df_times = pd.read_csv(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_times.csv\")\n",
    "    df_training_stats = pd.read_csv(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_training_stats.csv\")\n",
    "    df_inference_stats = pd.read_csv(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats.csv\")\n",
    "    df_load_stats = pd.read_csv(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_load_stats.csv\")\n",
    "    df_evaluation = pd.read_csv(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation.csv\")\n",
    "    \n",
    "    return df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation\n",
    "\n",
    "def aggregate_stats_by_time(df):\n",
    "    columns_avg_std = [col for col in df.columns if \"max\" not in col]\n",
    "    \n",
    "    df_avg_std = df[columns_avg_std].groupby([\"experiment\", \"device\"]).mean().reset_index()\n",
    "    df_avg_std = df_avg_std.drop(columns=[\"snapshot\"])\n",
    "    \n",
    "    # columns containing \"max\" in the name must be aggregated by taking the maximum value instead of the mean\n",
    "    columns_max = [col for col in df.columns if \"max\" in col]\n",
    "    \n",
    "    # add experiment and device columns to columns_max\n",
    "    columns_max.append(\"experiment\")\n",
    "    columns_max.append(\"device\")\n",
    "    \n",
    "    df_max = df[columns_max]\n",
    "    df_max = df_max.groupby([\"experiment\", \"device\"]).max().reset_index()\n",
    "    \n",
    "    df_final = pd.concat([df_avg_std, df_max[columns_max[:-2]]], axis=1)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def sort_by_experiment(df):    \n",
    "    # sort dataframes by experiment. experiment is the string \"EXP\" followed by a number, so we can sort by the number\n",
    "    df['sort'] = df['experiment'].str.extract('(\\d+)', expand=False).astype(int)\n",
    "    df.sort_values('sort',inplace=True, ascending=True)\n",
    "    df = df.drop('sort', axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_energy_consumption(df_experiment, experiment, df_times):    \n",
    "    average_exp_duration = df_times[f\"average_{experiment}_time\"]\n",
    "    df_experiment[\"total_average_cpu_energy_consumption\"] = df_experiment[f\"average_{experiment}_cpu_power_draw\"] * average_exp_duration\n",
    "    df_experiment[\"total_average_gpu_energy_consumption\"] = df_experiment[f\"average_{experiment}_gpu_power_draw\"] * average_exp_duration\n",
    "    \n",
    "    # calculate percentage of energy consumption reduction with respect to the baseline (EXP0)\n",
    "    baseline = df_experiment[df_experiment[\"experiment\"] == \"EXP0\"]\n",
    "    baseline_cpu_energy_consumption = baseline[\"total_average_cpu_energy_consumption\"].values[0]\n",
    "    baseline_gpu_energy_consumption = baseline[\"total_average_gpu_energy_consumption\"].values[0]\n",
    "    \n",
    "    percentage_cpu_energy_consumption_reduction = (df_experiment[\"total_average_cpu_energy_consumption\"] - baseline_cpu_energy_consumption) / baseline_cpu_energy_consumption * 100\n",
    "    percentage_gpu_energy_consumption_reduction = (df_experiment[\"total_average_gpu_energy_consumption\"] - baseline_gpu_energy_consumption) / baseline_gpu_energy_consumption * 100\n",
    "    \n",
    "    # round to 5 decimal places\n",
    "    percentage_cpu_energy_consumption_reduction = percentage_cpu_energy_consumption_reduction.round(5)\n",
    "    percentage_gpu_energy_consumption_reduction = percentage_gpu_energy_consumption_reduction.round(5)\n",
    "    \n",
    "    # if number is positive, put a \"+\" in front of it\n",
    "    # percentage_cpu_energy_consumption_reduction = percentage_cpu_energy_consumption_reduction.apply(lambda x: f\"+{x}\" if x > 0 else x)\n",
    "    # percentage_gpu_energy_consumption_reduction = percentage_gpu_energy_consumption_reduction.apply(lambda x: f\"+{x}\" if x > 0 else x)\n",
    "        \n",
    "    df_experiment[\"percentage_cpu_energy_consumption_reduction\"] = percentage_cpu_energy_consumption_reduction\n",
    "    df_experiment[\"percentage_gpu_energy_consumption_reduction\"] = percentage_gpu_energy_consumption_reduction\n",
    "    \n",
    "    # in baseline put a \"N/A\" in the percentage column\n",
    "    df_experiment.loc[df_experiment[\"experiment\"] == \"EXP0\", \"percentage_cpu_energy_consumption_reduction\"] = \"N/A\"\n",
    "    df_experiment.loc[df_experiment[\"experiment\"] == \"EXP0\", \"percentage_gpu_energy_consumption_reduction\"] = \"N/A\"\n",
    "        \n",
    "    return df_experiment\n",
    "\n",
    "def sort_columns(df_training_stats, df_inference_stats, df_load_stats):\n",
    "    print(\"Sorting columns...\")\n",
    "    \n",
    "    stats = [\"ram_memory_uss\", \"ram_memory_rss\", \"ram_memory_vms\", \"ram_memory_pss\", \"cpu_usage\", \"cpu_freq\", \"cpu_power_draw\", \"io_usage\", \"bytes_written\", \"bytes_read\", \"gpu_memory\", \"gpu_usage\", \"gpu_freq\", \"gpu_power_draw\"]\n",
    "    tests = [\"training\", \"inference\", \"load\"]\n",
    "    \n",
    "    for test in tests:\n",
    "        test_columns = [\"experiment\", \"device\"]\n",
    "        \n",
    "        for i in range(len(stats)):\n",
    "            stat_test_columns = [f\"average_{test}_{stats[i]}\", f\"std_dev_{test}_{stats[i]}\", f\"max_{test}_{stats[i]}\"]\n",
    "            test_columns.extend(stat_test_columns)\n",
    "        \n",
    "        test_columns.extend([\"total_average_cpu_energy_consumption\", \"percentage_cpu_energy_consumption_reduction\", \"total_average_gpu_energy_consumption\", \"percentage_gpu_energy_consumption_reduction\"])\n",
    "        \n",
    "        if test == \"training\":\n",
    "            df_training_stats = df_training_stats[test_columns]\n",
    "        elif test == \"inference\":\n",
    "            df_inference_stats = df_inference_stats[test_columns]\n",
    "        elif test == \"load\":\n",
    "            df_load_stats = df_load_stats[test_columns]\n",
    "    \n",
    "    return df_training_stats, df_inference_stats, df_load_stats\n",
    "\n",
    "def move_model_size(df_times, df_load_stats):\n",
    "    print(\"Moving 'model size' column from times to load stats dataframe...\")\n",
    "    \n",
    "    df_load_stats[\"model_size\"] = df_times[\"model_size\"]\n",
    "    df_times = df_times.drop(\"model_size\", axis=1)\n",
    "    \n",
    "    return df_times, df_load_stats\n",
    "\n",
    "def pretty_format_column_names(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation):\n",
    "    print(\"Pretty formating column names...\")\n",
    "    \n",
    "    stats = [\"ram_memory_uss\", \"ram_memory_rss\", \"ram_memory_vms\", \"ram_memory_pss\", \"cpu_usage\", \"cpu_freq\", \"cpu_power_draw\", \"io_usage\", \"bytes_written\", \"bytes_read\", \"gpu_memory\", \"gpu_usage\", \"gpu_freq\", \"gpu_power_draw\"]\n",
    "\n",
    "    stats_names = {\n",
    "        \"ram_memory_uss\": \"RAM Memory USS (B)\",\n",
    "        \"ram_memory_rss\": \"RAM Memory RSS (B)\",\n",
    "        \"ram_memory_vms\": \"RAM Memory VMS (B)\",\n",
    "        \"ram_memory_pss\": \"RAM Memory PSS (B)\",\n",
    "        \"cpu_usage\": \"CPU Usage (%)\",\n",
    "        \"cpu_freq\": \"CPU Frequency (MHz)\",\n",
    "        \"cpu_power_draw\": \"CPU Power Draw (W)\",\n",
    "        \"io_usage\": \"I/O Usage (%)\",\n",
    "        \"bytes_written\": \"Bytes Written (B)\",\n",
    "        \"bytes_read\": \"Bytes Read (B)\",\n",
    "        \"gpu_memory\": \"GPU Memory (MB)\",\n",
    "        \"gpu_usage\": \"GPU Usage (%)\",\n",
    "        \"gpu_freq\": \"GPU Frequency (MHz)\",\n",
    "        \"gpu_power_draw\": \"GPU Power Draw (W)\"\n",
    "    }\n",
    "    \n",
    "    agg_types = [\"Avg.\", \"Std. Dev.\", \"Max.\"]\n",
    "    \n",
    "    stats_column_names = [\"Experiment\", \"Device\"]\n",
    "    \n",
    "    for i in range(len(stats)):\n",
    "        for j in range(len(agg_types)):\n",
    "            stats_column_names.append(f\"{agg_types[j]} {stats_names[stats[i]]}\")\n",
    "    \n",
    "    stats_column_names.extend([\"Total Avg. CPU Energy Consumption (J)\", \"Percentage of Total Avg. CPU Energy Consumption Reduction (%)\", \"Total Avg. GPU Energy Consumption (J)\", \"Percentage of Total Avg. GPU Energy Consumption Reduction (%)\"])\n",
    "            \n",
    "    df_training_stats.columns = stats_column_names\n",
    "    df_inference_stats.columns = stats_column_names\n",
    "    df_load_stats.columns = stats_column_names + [\"Model Size (B)\"]\n",
    "    \n",
    "    binary_classification_evaluation_column_names = [\"Experiment\", \"Device\", \"Accuracy\", \"F1 Score\", \"AUC\", \"Recall\", \"Precision\", \"Balanced Accuracy\", \"Matthews Correlation Coefficient\"]\n",
    "    regression_evaluation_column_names = [\"Experiment\", \"Device\", \"Mean Squared Error\", \"Mean Absolute Error\", \"Mean Absolute Percentage Error\", \"Symmetric Mean Absolute Percentage Error\"]\n",
    "    \n",
    "    evaluation_column_names = binary_classification_evaluation_column_names if ml_task == \"binary_classification\" else regression_evaluation_column_names\n",
    "    \n",
    "    df_evaluation.columns = evaluation_column_names\n",
    "    \n",
    "    times_names = {\n",
    "        \"training_time\": \"Training Time (s)\",\n",
    "        \"inference_time\": \"Inference Time (s)\",\n",
    "        \"load_time\": \"Load Time (s)\",\n",
    "    }\n",
    "    \n",
    "    times_column_names = [\"Experiment\", \"Device\"]\n",
    "    \n",
    "    for time_column in times_names:\n",
    "        for agg_type in agg_types:\n",
    "            times_column_names.append(f\"{agg_type} {times_names[time_column]}\")\n",
    "    \n",
    "    df_times.columns = times_column_names\n",
    "            \n",
    "    return df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation\n",
    "\n",
    "def round_results(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation, decimal_places=5):\n",
    "    print(\"Rounding results...\")\n",
    "    \n",
    "    df_times = df_times.round(decimal_places)\n",
    "    df_training_stats = df_training_stats.round(decimal_places)\n",
    "    df_inference_stats = df_inference_stats.round(decimal_places)\n",
    "    df_load_stats = df_load_stats.round(decimal_places)\n",
    "    df_evaluation = df_evaluation.round(decimal_places)\n",
    "    \n",
    "    return df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation\n",
    "\n",
    "def remove_unnecesary_stats(df_training_stats, df_inference_stats, df_load_stats):\n",
    "    print(\"Removing unnecesary stats...\")\n",
    "    \n",
    "    # if device is CPU, remove GPU columns\n",
    "    df_training_stats = df_training_stats.drop(df_training_stats[df_training_stats[\"Device\"] == \"CPU\"].filter(regex=\"GPU\").columns, axis=1)\n",
    "    df_inference_stats = df_inference_stats.drop(df_inference_stats[df_inference_stats[\"Device\"] == \"CPU\"].filter(regex=\"GPU\").columns, axis=1)\n",
    "    df_load_stats = df_load_stats.drop(df_load_stats[df_load_stats[\"Device\"] == \"CPU\"].filter(regex=\"GPU\").columns, axis=1)\n",
    "    \n",
    "    # remove RSS columns\n",
    "    df_training_stats = df_training_stats.drop(df_training_stats.filter(regex=\"RSS\").columns, axis=1)\n",
    "    df_inference_stats = df_inference_stats.drop(df_inference_stats.filter(regex=\"RSS\").columns, axis=1)\n",
    "    df_load_stats = df_load_stats.drop(df_load_stats.filter(regex=\"RSS\").columns, axis=1)\n",
    "    \n",
    "    # remove VMS columns\n",
    "    df_training_stats = df_training_stats.drop(df_training_stats.filter(regex=\"VMS\").columns, axis=1)\n",
    "    df_inference_stats = df_inference_stats.drop(df_inference_stats.filter(regex=\"VMS\").columns, axis=1)\n",
    "    df_load_stats = df_load_stats.drop(df_load_stats.filter(regex=\"VMS\").columns, axis=1)\n",
    "    \n",
    "    # remove PSS columns\n",
    "    df_training_stats = df_training_stats.drop(df_training_stats.filter(regex=\"PSS\").columns, axis=1)\n",
    "    df_inference_stats = df_inference_stats.drop(df_inference_stats.filter(regex=\"PSS\").columns, axis=1)\n",
    "    df_load_stats = df_load_stats.drop(df_load_stats.filter(regex=\"PSS\").columns, axis=1)\n",
    "    \n",
    "    # remove bytes written and bytes read columns\n",
    "    df_training_stats = df_training_stats.drop(df_training_stats.filter(regex=\"Bytes\").columns, axis=1)\n",
    "    df_inference_stats = df_inference_stats.drop(df_inference_stats.filter(regex=\"Bytes\").columns, axis=1)\n",
    "    df_load_stats = df_load_stats.drop(df_load_stats.filter(regex=\"Bytes\").columns, axis=1)\n",
    "    \n",
    "    # remove IO usage column\n",
    "    df_training_stats = df_training_stats.drop(df_training_stats.filter(regex=\"I/O\").columns, axis=1)\n",
    "    df_inference_stats = df_inference_stats.drop(df_inference_stats.filter(regex=\"I/O\").columns, axis=1)\n",
    "    df_load_stats = df_load_stats.drop(df_load_stats.filter(regex=\"I/O\").columns, axis=1)\n",
    "    \n",
    "    return df_training_stats, df_inference_stats, df_load_stats\n",
    "\n",
    "def reorder_columns(df_training_stats, df_inference_stats, df_load_stats):\n",
    "    # reorder columns\n",
    "    order = [\"Experiment\", \"Device\", \"Total Avg. CPU Energy Consumption (J)\", \"Percentage of Total Avg. CPU Energy Consumption Reduction (%)\", \"Total Avg. GPU Energy Consumption (J)\", \"Percentage of Total Avg. GPU Energy Consumption Reduction (%)\", \"Avg. CPU Power Draw (W)\", \"Std. Dev. CPU Power Draw (W)\", \"Max. CPU Power Draw (W)\", \"Avg. GPU Power Draw (W)\", \"Std. Dev. GPU Power Draw (W)\", \"Max. GPU Power Draw (W)\", \"Avg. CPU Usage (%)\", \"Std. Dev. CPU Usage (%)\", \"Max. CPU Usage (%)\", \"Avg. GPU Usage (%)\", \"Std. Dev. GPU Usage (%)\", \"Max. GPU Usage (%)\", \"Avg. CPU Frequency (MHz)\", \"Std. Dev. CPU Frequency (MHz)\", \"Max. CPU Frequency (MHz)\", \"Avg. GPU Frequency (MHz)\", \"Std. Dev. GPU Frequency (MHz)\", \"Max. GPU Frequency (MHz)\", \"Avg. RAM Memory USS (B)\", \"Std. Dev. RAM Memory USS (B)\", \"Max. RAM Memory USS (B)\", \"Avg. RAM Memory PSS (B)\", \"Std. Dev. RAM Memory PSS (B)\", \"Max. RAM Memory PSS (B)\", \"Avg. RAM Memory RSS (B)\", \"Std. Dev. RAM Memory RSS (B)\", \"Max. RAM Memory RSS (B)\", \"Avg. RAM Memory VMS (B)\", \"Std. Dev. RAM Memory VMS (B)\", \"Max. RAM Memory VMS (B)\", \"Avg. I/O Usage (%)\", \"Std. Dev. I/O Usage (%)\", \"Max. I/O Usage (%)\", \"Avg. Bytes Written (B)\", \"Std. Dev. Bytes Written (B)\", \"Max. Bytes Written (B)\", \"Avg. Bytes Read (B)\", \"Std. Dev. Bytes Read (B)\", \"Max. Bytes Read (B)\"]\n",
    "    \n",
    "    df_training_stats = df_training_stats[order]\n",
    "    df_inference_stats = df_inference_stats[order]\n",
    "    df_load_stats = df_load_stats[order + [\"Model Size (B)\"]]\n",
    "    \n",
    "    return df_training_stats, df_inference_stats, df_load_stats\n",
    "\n",
    "def group_with_evaluation(df_stats, df_evaluation):\n",
    "    # add evaluation metrics to inference stats\n",
    "    df_stats[\"Accuracy\"] = df_evaluation[\"Accuracy\"]\n",
    "    df_stats[\"Balanced Accuracy\"] = df_evaluation[\"Balanced Accuracy\"]\n",
    "    df_stats[\"F1 Score\"] = df_evaluation[\"F1 Score\"]\n",
    "    \n",
    "    return df_stats\n",
    "  \n",
    "def get_results(aggregate_by_time=True):\n",
    "    df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation = load_files()\n",
    "        \n",
    "    if aggregate_by_time:\n",
    "        print(\"Aggregating stats by time...\")\n",
    "        \n",
    "        df_training_stats = aggregate_stats_by_time(df_training_stats)\n",
    "        df_inference_stats = aggregate_stats_by_time(df_inference_stats)\n",
    "        df_load_stats = aggregate_stats_by_time(df_load_stats)\n",
    "    \n",
    "    print(\"Sorting dataframes by experiment...\")\n",
    "    df_times = sort_by_experiment(df_times)\n",
    "    df_training_stats = sort_by_experiment(df_training_stats)\n",
    "    df_inference_stats = sort_by_experiment(df_inference_stats)\n",
    "    df_load_stats = sort_by_experiment(df_load_stats)\n",
    "    df_evaluation = sort_by_experiment(df_evaluation)\n",
    "        \n",
    "    # calculate energy consumption in Joules\n",
    "    print(\"Calculating energy consumption...\")\n",
    "    df_training_stats = calculate_energy_consumption(df_training_stats, \"training\", df_times)\n",
    "    df_inference_stats = calculate_energy_consumption(df_inference_stats, \"inference\", df_times)\n",
    "    df_load_stats = calculate_energy_consumption(df_load_stats, \"load\", df_times)\n",
    "    \n",
    "    # put columns of the same statistic together\n",
    "    df_training_stats, df_inference_stats, df_load_stats = sort_columns(df_training_stats, df_inference_stats, df_load_stats)\n",
    "    \n",
    "    # move column \"model_size\" of df_times to df_load_stats\n",
    "    df_times, df_load_stats = move_model_size(df_times, df_load_stats)\n",
    "\n",
    "    # pretty print column names\n",
    "    df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation = pretty_format_column_names(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation)\n",
    "    \n",
    "    # reorder columns\n",
    "    # df_training_stats, df_inference_stats, df_load_stats = reorder_columns(df_training_stats, df_inference_stats, df_load_stats)\n",
    "    \n",
    "    # round results\n",
    "    df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation = round_results(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation, decimal_places=3)\n",
    "    \n",
    "    # remove unnecesary stats\n",
    "    df_training_stats, df_inference_stats, df_load_stats = remove_unnecesary_stats(df_training_stats, df_inference_stats, df_load_stats)\n",
    "    \n",
    "    # group inference and evaluation\n",
    "    # df_inference_stats = group_inference_and_evaluation(df_inference_stats, df_evaluation)\n",
    "    \n",
    "    # remove Device column\n",
    "    # df_times = df_times.drop(columns=[\"Device\"])\n",
    "    # df_training_stats = df_training_stats.drop(columns=[\"Device\"])\n",
    "    # df_inference_stats = df_inference_stats.drop(columns=[\"Device\"])\n",
    "    # df_load_stats = df_load_stats.drop(columns=[\"Device\"])\n",
    "    # df_evaluation = df_evaluation.drop(columns=[\"Device\"])\n",
    "    \n",
    "    # remove \"EXP\" from experiment names\n",
    "    # df_times[\"Experiment\"] = df_times[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    # df_training_stats[\"Experiment\"] = df_training_stats[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    # df_inference_stats[\"Experiment\"] = df_inference_stats[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    # df_load_stats[\"Experiment\"] = df_load_stats[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    # df_evaluation[\"Experiment\"] = df_evaluation[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    \n",
    "    return df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation\n",
    "\n",
    "def print_results(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation):\n",
    "    print(\"Results of the benchmark:\")\n",
    "    \n",
    "    print(\"Training/Inference/Load times:\")\n",
    "    display(df_times)\n",
    "    \n",
    "    print(\"Training stats:\")\n",
    "    display(df_training_stats)\n",
    "    \n",
    "    print(\"Inference stats:\")\n",
    "    display(df_inference_stats)\n",
    "    \n",
    "    print(\"Load stats:\")\n",
    "    display(df_load_stats)\n",
    "    \n",
    "    print(\"Evaluation:\")\n",
    "    display(df_evaluation)\n",
    "\n",
    "def export_results(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation):\n",
    "    print(\"Exporting results...\")\n",
    "    \n",
    "    Path(f\"results/{batch_size}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    df_times.to_csv(f\"results/{batch_size}/times_{batch_size}.csv\", index=False)\n",
    "    df_training_stats.to_csv(f\"results/{batch_size}/training_stats_{batch_size}.csv\")\n",
    "    df_inference_stats.to_csv(f\"results/{batch_size}/inference_stats_{batch_size}.csv\")\n",
    "    df_load_stats.to_csv(f\"results/{batch_size}/load_stats_{batch_size}.csv\")\n",
    "    df_evaluation.to_csv(f\"results/{batch_size}/evaluation_{batch_size}.csv\")\n",
    "    \n",
    "    print(\"Results exported to results/ folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[gf].columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_files()\n",
    "run_benchmark(experiments=experiments, devices=devices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation = get_results(aggregate_by_time=True)\n",
    "print_results(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation = get_results(aggregate_by_time=True)\n",
    "export_results(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_results(batch_size):\n",
    "    df_times = pd.read_csv(f\"results/{batch_size}/times_{batch_size}.csv\")\n",
    "    df_training_stats = pd.read_csv(f\"results/{batch_size}/training_stats_{batch_size}.csv\")\n",
    "    df_inference_stats = pd.read_csv(f\"results/{batch_size}/inference_stats_{batch_size}.csv\")\n",
    "    df_load_stats = pd.read_csv(f\"results/{batch_size}/load_stats_{batch_size}.csv\")\n",
    "    df_evaluation = pd.read_csv(f\"results/{batch_size}/evaluation_{batch_size}.csv\")\n",
    "    \n",
    "    return df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation\n",
    "\n",
    "def reorder_stats_columns(df_training_stats, df_inference_stats, df_load_stats):\n",
    "    # reorder columns\n",
    "    order = [\"Experiment\", \"Device\", \"Total Avg. CPU Energy Consumption (J)\", \"Percentage of Total Avg. CPU Energy Consumption Reduction (%)\", \"Avg. CPU Power Draw (W)\", \"Std. Dev. CPU Power Draw (W)\", \"Max. CPU Power Draw (W)\", \"Avg. CPU Usage (%)\", \"Std. Dev. CPU Usage (%)\", \"Max. CPU Usage (%)\", \"Avg. CPU Frequency (MHz)\", \"Std. Dev. CPU Frequency (MHz)\", \"Max. CPU Frequency (MHz)\", \"Avg. RAM Memory USS (B)\", \"Std. Dev. RAM Memory USS (B)\", \"Max. RAM Memory USS (B)\"]\n",
    "    \n",
    "    df_training_stats = df_training_stats[order]\n",
    "    df_inference_stats = df_inference_stats[order]\n",
    "    df_load_stats = df_load_stats[order + [\"Model Size (B)\"]]\n",
    "    \n",
    "    return df_training_stats, df_inference_stats, df_load_stats\n",
    "\n",
    "def reorder_times_columns(df_times):\n",
    "    # reorder columns\n",
    "    order = [\"Experiment\", \"Device\", \"Avg. Inference Time (s)\", \"Std. Dev. Inference Time (s)\", \"Max. Inference Time (s)\", \"Avg. Training Time (s)\", \"Std. Dev. Training Time (s)\", \"Max. Training Time (s)\", \"Avg. Load Time (s)\", \"Std. Dev. Load Time (s)\", \"Max. Load Time (s)\"]\n",
    "    \n",
    "    df_times = df_times[order]\n",
    "    \n",
    "    return df_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 256, 1024]\n",
    "\n",
    "_df_times, _df_training_stats, _df_inference_stats, _df_load_stats, df_evaluation_32 = load_processed_results(32)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation = load_processed_results(batch_size)\n",
    "    \n",
    "    # replace last two column names\n",
    "    new_column_names = [\"Total Avg. CPU Energy Consumption (J)\", \"Percentage of Total Avg. CPU Energy Consumption Reduction (%)\"]\n",
    "\n",
    "    df_training_stats.columns = df_training_stats.columns[:-2].tolist() + new_column_names\n",
    "    df_inference_stats.columns = df_inference_stats.columns[:-2].tolist() + new_column_names\n",
    "    df_load_stats.columns = df_load_stats.columns[:-3].tolist() + new_column_names + [\"Model Size (B)\"]\n",
    "    \n",
    "    # reorder columns\n",
    "    df_times = reorder_times_columns(df_times)\n",
    "    df_training_stats, df_inference_stats, df_load_stats = reorder_stats_columns(df_training_stats, df_inference_stats, df_load_stats)\n",
    "\n",
    "    # round results\n",
    "    df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation = round_results(df_times, df_training_stats, df_inference_stats, df_load_stats, df_evaluation, decimal_places=3)\n",
    "\n",
    "    # remove unnecesary stats\n",
    "    df_training_stats, df_inference_stats, df_load_stats = remove_unnecesary_stats(df_training_stats, df_inference_stats, df_load_stats)\n",
    "\n",
    "    # group inference and evaluation\n",
    "    df_times = group_with_evaluation(df_times, df_evaluation_32)\n",
    "    df_training_stats = group_with_evaluation(df_training_stats, df_evaluation_32)\n",
    "    df_inference_stats = group_with_evaluation(df_inference_stats, df_evaluation_32)\n",
    "    df_load_stats = group_with_evaluation(df_load_stats, df_evaluation_32)\n",
    "\n",
    "    # remove Device column\n",
    "    df_times = df_times.drop(columns=[\"Device\"])\n",
    "    df_training_stats = df_training_stats.drop(columns=[\"Device\"])\n",
    "    df_inference_stats = df_inference_stats.drop(columns=[\"Device\"])\n",
    "    df_load_stats = df_load_stats.drop(columns=[\"Device\"])\n",
    "    df_evaluation = df_evaluation.drop(columns=[\"Device\"])\n",
    "\n",
    "    # remove \"EXP\" from experiment names\n",
    "    df_times[\"Experiment\"] = df_times[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    df_training_stats[\"Experiment\"] = df_training_stats[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    df_inference_stats[\"Experiment\"] = df_inference_stats[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    df_load_stats[\"Experiment\"] = df_load_stats[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "    df_evaluation[\"Experiment\"] = df_evaluation[\"Experiment\"].str.replace(\"EXP\", \"\")\n",
    "\n",
    "    print(\"Exporting results...\")\n",
    "    \n",
    "    revision = \"rev6\"\n",
    "    Path(f\"results/{revision}/{batch_size}\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # remove unnamed columns\n",
    "    df_times = df_times.loc[:, ~df_times.columns.str.contains('^Unnamed')]\n",
    "    df_training_stats = df_training_stats.loc[:, ~df_training_stats.columns.str.contains('^Unnamed')]\n",
    "    df_inference_stats = df_inference_stats.loc[:, ~df_inference_stats.columns.str.contains('^Unnamed')]\n",
    "    df_load_stats = df_load_stats.loc[:, ~df_load_stats.columns.str.contains('^Unnamed')]\n",
    "    df_evaluation = df_evaluation.loc[:, ~df_evaluation.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    # replace \"Experiment\" by \"Opt. Strategy Id.\"\n",
    "    df_times = df_times.rename(columns={\"Experiment\": \"Opt. Strategy Id.\"})\n",
    "    df_training_stats = df_training_stats.rename(columns={\"Experiment\": \"Opt. Strategy Id.\"})\n",
    "    df_inference_stats = df_inference_stats.rename(columns={\"Experiment\": \"Opt. Strategy Id.\"})\n",
    "    df_load_stats = df_load_stats.rename(columns={\"Experiment\": \"Opt. Strategy Id.\"})\n",
    "    df_evaluation = df_evaluation.rename(columns={\"Experiment\": \"Opt. Strategy Id.\"})\n",
    "    \n",
    "    df_times.to_csv(f\"results/{revision}/{batch_size}/times_{batch_size}_final.csv\", index=False)\n",
    "    df_training_stats.to_csv(f\"results/{revision}/{batch_size}/training_stats_{batch_size}_final.csv\", index=False)\n",
    "    df_inference_stats.to_csv(f\"results/{revision}/{batch_size}/inference_stats_{batch_size}_final.csv\", index=False)\n",
    "    df_load_stats.to_csv(f\"results/{revision}/{batch_size}/load_stats_{batch_size}_final.csv\", index=False)\n",
    "    df_evaluation.to_csv(f\"results/{revision}/{batch_size}/evaluation_{batch_size}_final.csv\", index=False)\n",
    "    \n",
    "    print(\"Results exported to results/ folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 256, 1024]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    df_times = pd.read_csv(f\"results/{revision}/{batch_size}/times_{batch_size}_final.csv\")\n",
    "    df_training_stats = pd.read_csv(f\"results/{revision}/{batch_size}/training_stats_{batch_size}_final.csv\")\n",
    "    df_inference_stats = pd.read_csv(f\"results/{revision}/{batch_size}/inference_stats_{batch_size}_final.csv\")\n",
    "    df_load_stats = pd.read_csv(f\"results/{revision}/{batch_size}/load_stats_{batch_size}_final.csv\")\n",
    "    df_evaluation = pd.read_csv(f\"results/{revision}/{batch_size}/evaluation_{batch_size}_final.csv\")\n",
    "    \n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    display(df_times)\n",
    "    display(df_training_stats)\n",
    "    display(df_inference_stats)\n",
    "    display(df_load_stats)\n",
    "    display(df_evaluation)\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 256, 1024]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    df_times = pd.read_csv(f\"results_final/rev6/{batch_size}/times_{batch_size}_final.csv\")\n",
    "    df_training_stats = pd.read_csv(f\"results_final/rev6/{batch_size}/training_stats_{batch_size}_final.csv\")\n",
    "    df_inference_stats = pd.read_csv(f\"results_final/rev6/{batch_size}/inference_stats_{batch_size}_final.csv\")\n",
    "    df_load_stats = pd.read_csv(f\"results_final/rev6/{batch_size}/load_stats_{batch_size}_final.csv\")\n",
    "    df_evaluation = pd.read_csv(f\"results_final/rev6/{batch_size}/evaluation_{batch_size}_final.csv\")\n",
    "    \n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    display(df_times)\n",
    "    display(df_training_stats)\n",
    "    display(df_inference_stats)\n",
    "    display(df_load_stats)\n",
    "    display(df_evaluation)\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change font family to 'Liberation Serif'\n",
    "import matplotlib as mpl\n",
    "mpl.rc('font',family='Liberation Serif')\n",
    "\n",
    "def plot_stats(experiment, device, test):\n",
    "    df_test = pd.read_csv(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_{test}_stats.csv\")\n",
    "    \n",
    "    display(df_test)\n",
    "    \n",
    "    stats_list = df_test.columns[3:]\n",
    "    # stats_names = [stat.replace(\"_\", \" \").capitalize() for stat in stats_list]\n",
    "    stats_names = {stat: \" \".join(stat.split(\"_\")).capitalize() for stat in stats_list}\n",
    "        \n",
    "    for stat in stats_list:\n",
    "        print(f\"Plotting {stat} for {experiment} on {device} for {test}\")\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "        sns.set(style=\"darkgrid\")\n",
    "        sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "        sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "\n",
    "        stat_data = df_test[(df_test[\"experiment\"] == experiment) & (df_test[\"device\"] == device)][stat]\n",
    "        baseline_stat_data = df_test[(df_test[\"experiment\"] == \"EXP0\") & (df_test[\"device\"] == device)][stat]\n",
    "        \n",
    "        if experiments[experiment][\"post_training_optimizations\"] is not None:\n",
    "            post_training_optimizations = \" \".join(experiments[experiment][\"post_training_optimizations\"]).replace(\"_\", \" \").capitalize()\n",
    "        else:\n",
    "            post_training_optimizations = \"\"\n",
    "        if experiments[experiment][\"training_aware_optimizations\"] is not None:\n",
    "            training_aware_optimizations = \" \".join(experiments[experiment][\"training_aware_optimizations\"]).replace(\"_\", \" \").capitalize()\n",
    "        else:\n",
    "            training_aware_optimizations = \"\"\n",
    "        \n",
    "        experiment_name = f\"{post_training_optimizations} {training_aware_optimizations}\"\n",
    "        print(experiment_name)\n",
    "        \n",
    "        time = np.arange(0, len(stat_data))\n",
    "        baseline_time = np.arange(0, len(baseline_stat_data))\n",
    "        \n",
    "        max_time = max(len(time), len(baseline_time))\n",
    "        \n",
    "        ax = sns.lineplot(x=time, y=stat_data, label=experiment_name)\n",
    "        ax = sns.lineplot(x=baseline_time, y=baseline_stat_data, label=\"Baseline\")\n",
    "\n",
    "        ax.set(xlabel=\"Time (s)\", ylabel=stat)\n",
    "        ax.set(ylabel=stats_names[stat])\n",
    "        # ax.set_xticks(np.arange(0, max_time + 1, 1))\n",
    "        # ax.set_xticklabels(np.arange(0, max_time + 1, 1))\n",
    "        ax.set_xlim(0, max(time))\n",
    "        ax.set_ylim(0, max(df_test[stat]) * 1.1)\n",
    "        # ax.set_title(f\"{stats_names[stat]}. {experiment_name} ({device})\", fontsize=17)\n",
    "        ax.set_title(f\"{stats_names[stat]}. {experiment} ({device})\", fontsize=17)\n",
    "        ax.legend(prop={'family':'Liberation Serif'}, loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        Path(f\"poc_energy_efficiency_crypto/plots\").mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(f\"poc_energy_efficiency_crypto/plots/crypto_spider_5g_fcnn_optimized_benchmark_{test}_stats_{experiment}_{device}_{stat}.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "tests = [\"training\", \"inference\", \"load\"]\n",
    "\n",
    "for experiment in experiments:\n",
    "    if experiment == \"EXP0\":\n",
    "        continue\n",
    "    \n",
    "    for device in devices:\n",
    "        for test in tests:\n",
    "            # plot_stats(experiment, device, test)\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 256, 1024]\n",
    "\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "# plot df_inference_stats for all batch sizes\n",
    "def plot_inference_stats(device, df_inference_stats, batch_size):\n",
    "    display(df_inference_stats)\n",
    "    \n",
    "    stat = \"Percentage of Total Avg. CPU Energy Consumption Reduction (%)\"\n",
    "    stat_title = \"Energy Consumption Reduction (%)\"\n",
    "    \n",
    "    print(f\"Plotting {stat} ({device}) for batch size {batch_size}\")\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "    # sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "    # sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "    sns.set_style(\"darkgrid\", {\"axes.grid\": True, \"font.family\": \"Liberation Serif\"})\n",
    "    \n",
    "    # change color of the background of the grid\n",
    "    sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5, \"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "    exps = np.arange(1, len(df_inference_stats[stat]))\n",
    "    \n",
    "    # exps should be the x-axis and the bars should be the y-axis, that is, the bars should be arranged vertically\n",
    "    ax = sns.barplot(x=exps, y=df_inference_stats[stat].values[1:], label=stat_title, palette=\"Set2\", linewidth=0.6, edgecolor=\".35\", saturation=0.93, ci=None)\n",
    "    \n",
    "    # put the labels on the bars\n",
    "    values = df_inference_stats[stat].values[1:]\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        offset = 3 if v > 0 else -6\n",
    "        ax.text(i, v + offset, str(round(v, 2)), color=(0.18, 0.18, 0.18), fontweight='bold', ha=\"center\", fontfamily=\"Liberation Serif\", fontsize=12)\n",
    "       \n",
    "    # set y-axis upper limit to 100\n",
    "    ax.set_ylim(-115, 100)\n",
    "    \n",
    "    ax.set_yticks(np.arange(-125, 125, 25))\n",
    "\n",
    "    # ax.set(xlabel=\"Time (s)\", ylabel=stat)\n",
    "    # ax.set(ylabel=stat.replace(\"_\", \" \").capitalize())\n",
    "    # ax.set_xlim(0, max(time))\n",
    "    # ax.set_ylim(0, max(df_inference_stats[stat]) * 1.1)\n",
    "    ax.set_title(f\"{stat_title}. Batch size {batch_size}\", fontsize=15, fontfamily=\"Liberation Serif\")\n",
    "    # ax.legend(prop={'family':'Liberation Serif'}, loc=\"lower right\")\n",
    "    # change axis labels font family\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontfamily=\"Liberation Serif\")\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontfamily=\"Liberation Serif\")\n",
    "    ax.set_xlabel(\"Optimization Strategies\", fontfamily=\"Liberation Serif\")\n",
    "    ax.set_ylabel(stat_title, fontfamily=\"Liberation Serif\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    Path(f\"poc_energy_efficiency_crypto/plots\").mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(f\"poc_energy_efficiency_crypto/plots/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats_batch_size_{batch_size}_{stat}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "print(\"Plotting inference stats for all batch sizes\")\n",
    "\n",
    "for device in devices:\n",
    "    for batch_size in batch_sizes:\n",
    "        df_inference_stats = pd.read_csv(f\"results_final/rev6/{batch_size}/inference_stats_{batch_size}_final.csv\")\n",
    "        \n",
    "        plot_inference_stats(device, df_inference_stats, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_sizes = [32, 256, 1024]\n",
    "batch_sizes = [256]\n",
    "\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "# plot df_inference_stats for all batch sizes\n",
    "def plot_inference_stats(device, df_inference_stats, batch_size):\n",
    "    display(df_inference_stats)\n",
    "    \n",
    "    stat_title = \"Energy Consumption Reduction (%) vs. Accuracy\"\n",
    "    stat1 = \"Percentage of Total Avg. CPU Energy Consumption Reduction (%)\"\n",
    "    stat2 = \"Balanced Accuracy\"\n",
    "    \n",
    "    print(f\"Plotting {stat_title} ({device}) for batch size {batch_size}\")\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "    # sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "    # sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "    sns.set_style(\"darkgrid\", {\"axes.grid\": True, \"font.family\": \"Liberation Serif\"})\n",
    "    \n",
    "    # change color of the background of the grid\n",
    "    sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5, \"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "    exps = np.arange(1, len(df_inference_stats))\n",
    "    \n",
    "    # exps should be the x-axis and the bars should be the y-axis, that is, the bars should be arranged vertically\n",
    "    ax = sns.barplot(x=exps, y=df_inference_stats[stat1].values[1:], label=stat1, linewidth=0.6, edgecolor=\".35\", saturation=0.93, ci=None, color=\"#3f73c5\")\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    # put a line in the same spot as the points of the scatter that has the same width as the bars\n",
    "    ax2 = sns.lineplot(x=exps - 1, y=df_inference_stats[stat2].values[1:] * 100, linewidth=2, color=\"#f4c94f\", zorder=-1)\n",
    "    \n",
    "    ax2 = sns.scatterplot(x=exps - 1, y=df_inference_stats[stat2].values[1:] * 100, label=stat2, palette=\"Set2\", marker=\"o\", s=40, color=\"#fb8500\", edgecolors=\"#fb8500\", linewidth=0.0)\n",
    "    \n",
    "    # for i, v in enumerate(df_inference_stats[stat2].values[1:] * 100):\n",
    "    #     ax2.plot([i - 0.3, i + 0.3], [v, v], color=\"red\", linewidth=2)\n",
    "    \n",
    "    # put the labels on the bars\n",
    "    values1 = df_inference_stats[stat1].values[1:]\n",
    "    \n",
    "    for i, v in enumerate(values1):\n",
    "        offset = 3 if v > 0 else -6\n",
    "        ax.text(i, v + offset, str(round(v, 2)), color=(0.18, 0.18, 0.18), fontweight='bold', ha=\"center\", fontfamily=\"Liberation Serif\", fontsize=12)\n",
    "    \n",
    "    values2 = df_inference_stats[stat2].values[1:] * 100\n",
    "    \n",
    "    # for i, v in enumerate(values2):\n",
    "    #     offset = 0.01 if v > 0 else -0.01\n",
    "    #     ax2.text(i, v + offset, str(round(v, 2)), color=(0.18, 0.18, 0.18), fontweight='bold', ha=\"center\", fontfamily=\"Liberation Serif\", fontsize=12)\n",
    "       \n",
    "    # set y-axis upper limit to 100\n",
    "    ax.set_ylim(-115, 115)\n",
    "    ax.set_yticks(np.arange(-125, 125, 25))\n",
    "    \n",
    "    ax2.set_ylim(-115, 115)\n",
    "    ax2.set_yticks(np.arange(-125, 125, 25))\n",
    "\n",
    "    # ax.set(xlabel=\"Time (s)\", ylabel=stat)\n",
    "    # ax.set(ylabel=stat.replace(\"_\", \" \").capitalize())\n",
    "    # ax.set_xlim(0, max(time))\n",
    "    # ax.set_ylim(0, max(df_inference_stats[stat]) * 1.1)\n",
    "    ax.set_title(f\"{stat_title}. Batch size {batch_size}\", fontsize=15, fontfamily=\"Liberation Serif\")\n",
    "    # put legend out of the plot\n",
    "    fig.legend(prop={'family':'Liberation Serif'}, loc='lower center', bbox_to_anchor=(0.5, -0.1), fancybox=False, ncol=1, facecolor=\"none\")\n",
    "    # remove legend color\n",
    "    ax2.legend_.remove()\n",
    "    # change axis labels font family\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontfamily=\"Liberation Serif\")\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontfamily=\"Liberation Serif\")\n",
    "    ax.set_xlabel(\"Optimization Strategies\", fontfamily=\"Liberation Serif\")\n",
    "    ax.set_ylabel(stat1, fontfamily=\"Liberation Serif\")\n",
    "    ax2.set_ylabel(stat2, fontfamily=\"Liberation Serif\")\n",
    "    \n",
    "    # hide ax2 y ticks from 0 to -125\n",
    "    for tick in ax2.get_yticklabels()[0:5]:\n",
    "        tick.set_visible(False)\n",
    " \n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    Path(f\"poc_energy_efficiency_crypto/plots\").mkdir(parents=True, exist_ok=True)\n",
    "    # plt.savefig(f\"poc_energy_efficiency_crypto/plots/crypto_spider_5g_fcnn_optimized_benchmark_inference_stats_batch_size_{batch_size}_{stat}.png\", dpi=300)\n",
    "    # plt.close()\n",
    "    \n",
    "    pause\n",
    "    \n",
    "print(\"Plotting inference stats for all batch sizes\")\n",
    "\n",
    "for device in devices:\n",
    "    for batch_size in batch_sizes:\n",
    "        df_inference_stats = pd.read_csv(f\"results_final/rev6/{batch_size}/inference_stats_{batch_size}_final.csv\")\n",
    "        \n",
    "        plot_inference_stats(device, df_inference_stats, batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cc40d75",
   "metadata": {},
   "source": [
    "## Automatic Hyperparameter Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed262465",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54e2f67f",
   "metadata": {},
   "source": [
    "#### NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbe904",
   "metadata": {},
   "outputs": [],
   "source": [
    " # find the student model structure by searching for the model\n",
    "num_layers_grid = [1, 2, 3, 4]\n",
    "num_neurons_grid = [4, 8, 16, 32, 64]\n",
    "\n",
    "# make all possible combinations of the model structure (num_layers, num_neuron_for_each_layer)\n",
    "combinations = []\n",
    "\n",
    "for num_layers in num_layers_grid:\n",
    "    product = list(itertools.product(num_neurons_grid, repeat=num_layers))\n",
    "    combinations.append(product)\n",
    "\n",
    "# build the models\n",
    "for num_layers in range(0, len(combinations)):\n",
    "    print(f\"num_layers: {num_layers + 1}\")\n",
    "    \n",
    "    for index, num_neurons in enumerate(combinations[num_layers]):\n",
    "        print(f\"index: {index}\")\n",
    "        neurons = combinations[num_layers][index]\n",
    "        \n",
    "        student = Sequential()\n",
    "        \n",
    "        for layer, neurons_layer in enumerate(neurons):\n",
    "            print(f\"- layer: {layer}, neurons_layer: {neurons_layer}\")\n",
    "        \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nas(device=\"CPU\"):\n",
    "    teacher = baseline_model(input_dim=len(gf), n_output=2)\n",
    "    teacher, history = train_model(teacher)\n",
    "\n",
    "    num_trials = 5\n",
    "    \n",
    "    logger.info(\"Starting test_nas\")\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    \n",
    "    model_energy_consumption = {}\n",
    "    \n",
    "    Path(f\"poc_energy_efficiency_crypto/NAS\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        with StatsCollectionManager(directory=\"NAS\", test=\"NAS\", sampling_rate=stats_sampling_rate, pid=pid) as nas_scm:\n",
    "            with measure_time() as nas_time_measure:\n",
    "                logger.info(\"Applying Knowledge Distillation (with NAS)\")\n",
    "        \n",
    "                # find the student model structure by searching for the model\n",
    "                num_layers_grid = [1, 2, 3]\n",
    "                num_neurons_grid = [4, 8, 16, 32]\n",
    "                \n",
    "                # num_layers_grid = [1,]\n",
    "                # num_neurons_grid = [4,]\n",
    "\n",
    "                # make all possible combinations of the model structure (num_layers, num_neuron_for_each_layer)\n",
    "                combinations = []\n",
    "\n",
    "                for num_layers in num_layers_grid:\n",
    "                    product = list(itertools.product(num_neurons_grid, repeat=num_layers))\n",
    "                    combinations.append(product)\n",
    "\n",
    "                # build the models\n",
    "                for num_layers in range(0, len(combinations)):\n",
    "                    logger.info(f\"num_layers: {num_layers + 1}\")\n",
    "                    \n",
    "                    for index, num_neurons in enumerate(combinations[num_layers]):\n",
    "                        neurons = combinations[num_layers][index]\n",
    "                        logger.info(f\"neurons: {num_neurons}\")\n",
    "                        \n",
    "                        student = Sequential()\n",
    "                        student.add(tf.keras.Input(shape=(len(gf),)))\n",
    "                        \n",
    "                        for layer, neurons_layer in enumerate(neurons):\n",
    "                            logger.info(f\"layer: {layer}\")\n",
    "                            logger.info(f\"neurons_layer: {neurons_layer}\")\n",
    "                            \n",
    "                            student.add(Dense(neurons_layer, activation='relu'))\n",
    "                        \n",
    "                        student.add(Dense(2, activation='softmax'))\n",
    "                        student.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                        student.summary()\n",
    "\n",
    "                        # Initialize and compile distiller\n",
    "                        distiller = Distiller(student=student, teacher=teacher)\n",
    "                        distiller.compile(\n",
    "                            optimizer=keras.optimizers.Adam(),\n",
    "                            metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "                            student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                            distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "                            alpha=0.1,\n",
    "                            temperature=3,\n",
    "                        )\n",
    "\n",
    "                        es = EarlyStopping(\n",
    "                            monitor=\"val_student_loss\",\n",
    "                            mode=\"min\",\n",
    "                            patience=es_patience,\n",
    "                            restore_best_weights=es_restore_best_weights,\n",
    "                        )\n",
    "                        _history = distiller.fit(\n",
    "                            data_transformed,\n",
    "                            pd.get_dummies(df_train[\"tag\"]),\n",
    "                            validation_split=validation_split,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[es],\n",
    "                            verbose=0,\n",
    "                        )\n",
    "                                                \n",
    "                        # save the model\n",
    "                        comb_id = f\"{num_layers + 1}_{index}\"\n",
    "                        distiller.student.save(f\"poc_energy_efficiency_crypto/NAS/models/model_{comb_id}.h5\")\n",
    "                        \n",
    "                        for i in range(0, num_trials):\n",
    "                            logger.info(f\"Trial Evaluation: {i}\")\n",
    "                        \n",
    "                            # evaluate the model\n",
    "                            data_transformed_test = standard.transform(df_test[gf])\n",
    "                            \n",
    "                            with StatsCollectionManager(directory=\"NAS\", test=f\"evaluation_nas_{comb_id}_{i}\", sampling_rate=stats_sampling_rate, pid=pid) as evaluation_nas_scm:\n",
    "                                with measure_time() as evaluation_nas_time_measure:\n",
    "                                    predictions = distiller.student.predict(data_transformed_test)\n",
    "                            \n",
    "                            evaluation_nas_time = evaluation_nas_time_measure()\n",
    "                            \n",
    "                            # Write evaluation nas time to file\n",
    "                            with open(f\"poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_nas_time_{comb_id}_{i}.pkl\", \"wb\") as f:\n",
    "                                pickle.dump({\"evaluation_nas_time\": evaluation_nas_time}, f)\n",
    "                                \n",
    "                        if ml_task == \"binary_classification\":\n",
    "                            predictions = np.argmax(predictions, axis=1)\n",
    "                            \n",
    "                            accuracy = accuracy_score(df_test[\"tag\"], predictions)\n",
    "                            f1 = f1_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "                            auc = roc_auc_score(df_test[\"tag\"], predictions)\n",
    "                            recall = recall_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "                            precision = precision_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "                            balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], predictions)\n",
    "                            matthews = matthews_corrcoef(df_test[\"tag\"], predictions)\n",
    "\n",
    "                            logger.info(f\"Accuracy: {accuracy}\")\n",
    "                            logger.info(f\"F1 score: {f1}\")\n",
    "                            logger.info(f\"AUC: {auc}\")\n",
    "                            logger.info(f\"Recall: {recall}\")\n",
    "                            logger.info(f\"Precision: {precision}\")\n",
    "                            logger.info(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "                            logger.info(f\"Matthews correlation coefficient: {matthews}\")\n",
    "                            \n",
    "                            test_results = {\n",
    "                                \"accuracy\": accuracy,\n",
    "                                \"f1\": f1,\n",
    "                                \"auc\": auc,\n",
    "                                \"recall\": recall,\n",
    "                                \"precision\": precision,\n",
    "                                \"balanced_accuracy\": balanced_accuracy,\n",
    "                                \"matthews\": matthews\n",
    "                            }\n",
    "                        elif ml_task == \"regression\":\n",
    "                            mae = mean_absolute_error(df_test[\"tag\"], predictions)\n",
    "                            mse = mean_squared_error(df_test[\"tag\"], predictions)\n",
    "                            mape = mean_absolute_percentage_error(df_test[\"tag\"], predictions)\n",
    "                            smape = 1/len(df_test[\"tag\"]) * np.sum(2 * np.abs(predictions - df_test[\"tag\"]) / (np.abs(predictions) + np.abs(df_test[\"tag\"])))\n",
    "                            \n",
    "                            logger.info(f\"MAE: {mae}\")\n",
    "                            logger.info(f\"MSE: {mse}\")\n",
    "                            logger.info(f\"MAPE: {mape}\")\n",
    "                            logger.info(f\"SMAPE: {smape}\")\n",
    "                            \n",
    "                            test_results = {\n",
    "                                \"mae\": mae,\n",
    "                                \"mse\": mse,\n",
    "                                \"mape\": mape,\n",
    "                                \"smape\": smape\n",
    "                            }\n",
    "                            \n",
    "                        with open(f\"poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_nas_test_results_{comb_id}.pkl\", \"wb\") as f:\n",
    "                            pickle.dump(test_results, f)\n",
    "\n",
    "                        # accuracy = accuracy_score(df_test['tag'], np.argmax(preds, axis=1))\n",
    "                        # balanced_accuracy = balanced_accuracy_score(df_test['tag'], np.argmax(preds, axis=1))\n",
    "                        # f1 = f1_score(df_test['tag'], np.argmax(preds, axis=1))\n",
    "\n",
    "                        # print(f\"Accuracy: {accuracy}\")\n",
    "                        # print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "                        # print(f\"F1 score: {f1}\")\n",
    "                        \n",
    "                        # # save the results\n",
    "                        # test_results = [accuracy, balanced_accuracy, f1]\n",
    "                        \n",
    "                        # with open(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_nas_test_results_{comb_id}.pkl\", \"wb\") as f:\n",
    "                        #     pickle.dump({\"test_results\": test_results}, f)\n",
    "                                    \n",
    "                        evaluation_nas_times_list = []\n",
    "                        evaluation_nas_stats_list = []\n",
    "                        \n",
    "                        for i in range(0, num_trials):\n",
    "                            logger.info(f\"Trial Evaluation: {i}. Comb ID: {comb_id}\")\n",
    "                                \n",
    "                            # Read nas time from file\n",
    "                            with open(f\"poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_nas_time_{comb_id}_{i}.pkl\", \"rb\") as f:\n",
    "                                evaluation_nas_time = pickle.load(f)\n",
    "\n",
    "                            evaluation_nas_time = evaluation_nas_time[\"evaluation_nas_time\"]\n",
    "                            evaluation_nas_time = float(evaluation_nas_time)\n",
    "\n",
    "                            # # Delete nas time file\n",
    "                            # os.remove(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_nas_time_{comb_id}_{i}.pkl\")\n",
    "                                    \n",
    "                            # Read nas stats from file\n",
    "                            with open(f'poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_nas_{comb_id}_{i}_stats.pkl', 'rb') as f:\n",
    "                                evaluation_nas_stats = pickle.load(f)\n",
    "                                \n",
    "                            # # Delete nas stats file\n",
    "                            # os.remove(f\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_nas_stats_{comb_id}_{i}.pkl\")\n",
    "                            \n",
    "                            logger.info(\"Trial {}. Comb ID {}. Evaluation NAS time: {}\".format(i, comb_id, evaluation_nas_time))\n",
    "\n",
    "                            # Save NAS times\n",
    "                            evaluation_nas_times_list.append(evaluation_nas_time)\n",
    "\n",
    "                            # Save NAS stats\n",
    "                            evaluation_nas_stats_list.append(evaluation_nas_stats)\n",
    "\n",
    "                        average_evaluation_nas_time = np.mean(evaluation_nas_times_list)\n",
    "                        std_dev_evaluation_nas_time = np.std(evaluation_nas_times_list)\n",
    "                        max_evaluation_nas_time = np.max(evaluation_nas_times_list)\n",
    "\n",
    "                        # Time spent on NAS\n",
    "                        logger.info(f\"Comb ID: {comb_id}. \" + \"Average NAS time: {}\".format(np.round(average_evaluation_nas_time, 2)))\n",
    "                        logger.info(f\"Comb ID: {comb_id}. \" + \"Standard deviation of NAS time: {}\".format(np.round(std_dev_evaluation_nas_time, 2)))\n",
    "                        logger.info(f\"Comb ID: {comb_id}. \" + \"Max. NAS time: {}\".format(np.round(max_evaluation_nas_time, 2)))\n",
    "\n",
    "                        # Get average NAS metrics\n",
    "                        average_evaluation_nas_stats_list = get_average_stats(evaluation_nas_stats_list, average_evaluation_nas_time)\n",
    "                        std_dev_evaluation_nas_stats_list = get_std_dev_stats(evaluation_nas_stats_list, std_dev_evaluation_nas_time)\n",
    "                        max_evaluation_nas_stats_list = get_max_stats(evaluation_nas_stats_list, max_evaluation_nas_time)\n",
    "                        \n",
    "                        # Save NAS metrics to dataframe\n",
    "                        stats = evaluation_nas_stats_list[0][0].keys()\n",
    "\n",
    "                        averag_evaluation_nas_stats_names = [f\"average_evaluation_nas_{stat}\" for stat in stats]\n",
    "                        std_dev_evaluation_nas_stats_names = [f\"std_dev_evaluation_nas_{stat}\" for stat in stats]\n",
    "                        max_evaluation_nas_stats_names = [f\"max_evaluation_nas_{stat}\" for stat in stats]\n",
    "\n",
    "                        df_evaluation_nas_times_columns = [\"experiment\", \"device\", \"average_evaluation_nas_time\", \"std_dev_evaluation_nas_time\", \"max_evaluation_nas_time\"]\n",
    "                        df_evaluation_nas_times = pd.DataFrame(columns=df_evaluation_nas_times_columns)\n",
    "                        \n",
    "                        experiment = f\"evaluation_nas_{comb_id}\"\n",
    "                        evaluation_nas_times_row = [experiment, device, average_evaluation_nas_time, std_dev_evaluation_nas_time, max_evaluation_nas_time]\n",
    "                        df_evaluation_nas_times.loc[0] = evaluation_nas_times_row\n",
    "\n",
    "                        df_evaluation_nas_stats_columns = [\"experiment\", \"device\", \"snapshot\", *averag_evaluation_nas_stats_names, *std_dev_evaluation_nas_stats_names, *max_evaluation_nas_stats_names]\n",
    "                        df_evaluation_nas_stats = pd.DataFrame(columns=df_evaluation_nas_stats_columns)\n",
    "                        \n",
    "                        for index, _snapshot in enumerate(average_evaluation_nas_stats_list):\n",
    "                            row = np.array([experiment, device, index])\n",
    "                            row = np.append(row, average_evaluation_nas_stats_list[index])\n",
    "                            row = np.append(row, std_dev_evaluation_nas_stats_list[index])\n",
    "                            row = np.append(row, max_evaluation_nas_stats_list[index])\n",
    "\n",
    "                            df_evaluation_nas_stats.loc[index] = row\n",
    "                            \n",
    "                        assert df_evaluation_nas_times.shape[0] == 1\n",
    "                        assert df_evaluation_nas_stats.shape[0] == len(average_evaluation_nas_stats_list)\n",
    "                        assert df_evaluation_nas_stats.shape[1] == len(df_evaluation_nas_stats_columns)\n",
    "                        \n",
    "                        display(df_evaluation_nas_times)\n",
    "                        display(df_evaluation_nas_stats)\n",
    "\n",
    "                        # calculate the average of the df_evaluation_nas_stats average_evaluation_nas_cpu_power_draw and average_evaluation_nas_gpu_power_draw columns\n",
    "                        global_average_average_evaluation_nas_cpu_power_draw = df_evaluation_nas_stats[f\"average_evaluation_nas_cpu_power_draw\"].astype(float).mean()\n",
    "                        global_average_average_evaluation_nas_gpu_power_draw = df_evaluation_nas_stats[f\"average_evaluation_nas_gpu_power_draw\"].astype(float).mean()\n",
    "                        \n",
    "                        # check that both columns have a single scalar value\n",
    "                        assert np.isscalar(global_average_average_evaluation_nas_cpu_power_draw)\n",
    "                        assert np.isscalar(global_average_average_evaluation_nas_gpu_power_draw)\n",
    "                        \n",
    "                        average_evaluation_nas_exp_duration = df_evaluation_nas_times[f\"average_evaluation_nas_time\"].values[0]\n",
    "                        \n",
    "                        print(\"Average evaluation NAS experiment duration: {}\".format(average_evaluation_nas_exp_duration))\n",
    "                        print(\"Average evaluation NAS CPU power draw: {}\".format(global_average_average_evaluation_nas_cpu_power_draw))\n",
    "                        print(\"Average evaluation NAS GPU power draw: {}\".format(global_average_average_evaluation_nas_gpu_power_draw))\n",
    "                        \n",
    "                        total_average_cpu_energy_consumption = global_average_average_evaluation_nas_cpu_power_draw * average_evaluation_nas_exp_duration\n",
    "                        total_average_gpu_energy_consumption = global_average_average_evaluation_nas_gpu_power_draw * average_evaluation_nas_exp_duration\n",
    "                        \n",
    "                        model_energy_consumption[comb_id] = {\n",
    "                            \"cpu\": total_average_cpu_energy_consumption,\n",
    "                            \"gpu\": total_average_gpu_energy_consumption\n",
    "                        }\n",
    "                        \n",
    "                        print(\"Model energy consumption:\\n{}\".format(model_energy_consumption))\n",
    "                    \n",
    "                # normalize energy consumption between 0 and 1\n",
    "                normalized_model_energy_consumption = deepcopy(model_energy_consumption)\n",
    "                \n",
    "                max_cpu_energy_consumption = max([model_energy_consumption[comb_id][\"cpu\"] for comb_id in model_energy_consumption])\n",
    "                max_gpu_energy_consumption = max([model_energy_consumption[comb_id][\"gpu\"] for comb_id in model_energy_consumption])\n",
    "                \n",
    "                for comb_id in model_energy_consumption:\n",
    "                    normalized_model_energy_consumption[comb_id][\"cpu\"] = model_energy_consumption[comb_id][\"cpu\"] / max_cpu_energy_consumption\n",
    "                    normalized_model_energy_consumption[comb_id][\"gpu\"] = model_energy_consumption[comb_id][\"gpu\"] / max_gpu_energy_consumption\n",
    "                \n",
    "                # multiply normalized energy consumption by 0.5\n",
    "                weighted_normalized_model_energy_consumption = deepcopy(normalized_model_energy_consumption)\n",
    "                \n",
    "                for comb_id in model_energy_consumption:\n",
    "                    weighted_normalized_model_energy_consumption[comb_id][\"cpu\"] = normalized_model_energy_consumption[comb_id][\"cpu\"] * 0.5 # TODO: parameterize the weight\n",
    "                    weighted_normalized_model_energy_consumption[comb_id][\"gpu\"] = normalized_model_energy_consumption[comb_id][\"gpu\"] * 0.5\n",
    "                                    \n",
    "                # Read all evaluation nas test results files\n",
    "                evaluation_nas_test_results_files = glob.glob(\"poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_nas_test_results_*.pkl\")\n",
    "                measured_performance = {}\n",
    "                \n",
    "                for evaluation_nas_test_results_file in evaluation_nas_test_results_files:\n",
    "                    comb_id = evaluation_nas_test_results_file.split(\"_\")[-2] + \"_\" + evaluation_nas_test_results_file.split(\"_\")[-1].split(\".\")[0]\n",
    "                    \n",
    "                    with open(evaluation_nas_test_results_file, 'rb') as f:\n",
    "                        evaluation_nas_test_results = pickle.load(f)\n",
    "\n",
    "                        # # Delete evaluation nas test results file\n",
    "                        # os.remove(evaluation_nas_test_results)\n",
    "                    \n",
    "                    # Get measured performance\n",
    "                    measured_performance[comb_id] = evaluation_nas_test_results[\"balanced_accuracy\"] # TODO: parameterize the metric\n",
    "                                \n",
    "                # multiply measured performance by 0.5\n",
    "                weighted_measured_performance = deepcopy(measured_performance)\n",
    "                \n",
    "                for comb_id in measured_performance:\n",
    "                    weighted_measured_performance[comb_id] = measured_performance[comb_id] * 0.5 # TODO: parameterize the weight\n",
    "                \n",
    "                # calculate weighted average of energy consumption and measured performance\n",
    "                weighted_average_energy_consumption_and_measured_performance = {}\n",
    "                \n",
    "                for comb_id in model_energy_consumption:\n",
    "                    weighted_average_energy_consumption_and_measured_performance[comb_id] = weighted_normalized_model_energy_consumption[comb_id][\"cpu\"] + weighted_measured_performance[comb_id] # TODO: parameterize the platform that is being optimized (cpu or gpu)\n",
    "            \n",
    "                # get best comb_id\n",
    "                best_comb_id = max(weighted_average_energy_consumption_and_measured_performance, key=weighted_average_energy_consumption_and_measured_performance.get)\n",
    "                logger.info(f\"Best comb_id: {best_comb_id}\")\n",
    "                logger.info(f\"Best comb_id energy consumption: {model_energy_consumption[best_comb_id]}\")\n",
    "                logger.info(f\"Best comb_id measured performance: {measured_performance[best_comb_id]}\")\n",
    "                logger.info(f\"Best comb_id normalized energy consumption cpu: {normalized_model_energy_consumption[best_comb_id]['cpu']}\")\n",
    "                logger.info(f\"Best comb_id normalized energy consumption gpu: {normalized_model_energy_consumption[best_comb_id]['gpu']}\")\n",
    "                logger.info(f\"Best comb_id weighted normalized energy consumption cpu: {weighted_normalized_model_energy_consumption[best_comb_id]['cpu']}\")\n",
    "                logger.info(f\"Best comb_id weighted normalized energy consumption gpu: {weighted_normalized_model_energy_consumption[best_comb_id]['gpu']}\")\n",
    "                logger.info(f\"Best comb_id weighted measured performance: {weighted_measured_performance[best_comb_id]}\")\n",
    "                logger.info(f\"Best comb_id weighted average energy consumption and measured performance: {weighted_average_energy_consumption_and_measured_performance[best_comb_id]}\")\n",
    "                \n",
    "                # save best comb_id\n",
    "                with open(f\"poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_best_comb_id.pkl\", \"wb\") as f:\n",
    "                    pickle.dump({\"best_comb_id\": best_comb_id}, f)\n",
    "                \n",
    "        # Get nas time\n",
    "        nas_time = nas_time_measure()\n",
    "\n",
    "        # Write nas time to file\n",
    "        with open(f\"poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_nas_time.pkl\", \"wb\") as f:\n",
    "            pickle.dump({\"nas_time\": nas_time}, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ceb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the child process\n",
    "p_nas = multiprocessing.Process(target=test_nas, args=(\"cpu\",))\n",
    "\n",
    "p_nas.start()\n",
    "p_nas.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53404d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_times_list = []\n",
    "nas_stats_list = []\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "for i in range(0, num_trials):\n",
    "    logger.info(f\"Trial {i}\")\n",
    "    \n",
    "    p_nas = multiprocessing.Process(target=test_nas, args=(device,))\n",
    "    \n",
    "    p_nas.start()\n",
    "    p_nas.join()\n",
    "            \n",
    "    # Read nas time from file\n",
    "    with open(f\"poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_nas_time.pkl\", \"rb\") as f:\n",
    "        nas_time = pickle.load(f)\n",
    "\n",
    "    nas_time = nas_time[\"nas_time\"]\n",
    "\n",
    "    # # Delete nas time file\n",
    "    # os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_nas_time.pkl\")\n",
    "            \n",
    "    # Read nas stats from file\n",
    "    with open('poc_energy_efficiency_crypto/NAS/crypto_spider_5g_fcnn_optimized_benchmark_nas_stats.pkl', 'rb') as f:\n",
    "        nas_stats = pickle.load(f)\n",
    "        \n",
    "    # # Delete nas stats file\n",
    "    # os.remove(\"poc_energy_efficiency_crypto/crypto_spider_5g_fcnn_optimized_benchmark_nas_stats.pkl\")\n",
    "\n",
    "    logger.info(\"Trial {} - NAS time: {:.2f}s\".format(i, nas_time))\n",
    "\n",
    "    # Save NAS times\n",
    "    nas_times_list.append(nas_time)\n",
    "\n",
    "    # Save NAS stats\n",
    "    nas_stats_list.append(nas_stats)\n",
    "\n",
    "average_nas_time = np.mean(nas_times_list)\n",
    "std_dev_nas_time = np.std(nas_times_list)\n",
    "max_nas_time = np.max(nas_times_list)\n",
    "\n",
    "# Time spent on NAS\n",
    "logger.info(\"Average NAS time: {}\".format(np.round(average_nas_time, 2)))\n",
    "logger.info(\"Standard deviation of NAS time: {}\".format(np.round(std_dev_nas_time, 2)))\n",
    "logger.info(\"Max. NAS time: {}\".format(np.round(max_nas_time, 2)))\n",
    "\n",
    "# Get average NAS metrics\n",
    "average_nas_stats_list = get_average_stats(nas_stats_list, average_nas_time)\n",
    "std_dev_nas_stats_list = get_std_dev_stats(nas_stats_list, std_dev_nas_time)\n",
    "max_nas_stats_list = get_max_stats(nas_stats_list, max_nas_time)\n",
    "\n",
    "# Save NAS metrics to dataframe\n",
    "stats = nas_stats_list[0][0].keys()\n",
    "\n",
    "averag_nas_stats_names = [f\"average_nas_{stat}\" for stat in stats]\n",
    "std_dev_nas_stats_names = [f\"std_dev_nas_{stat}\" for stat in stats]\n",
    "max_nas_stats_names = [f\"max_nas_{stat}\" for stat in stats]\n",
    "\n",
    "df_nas_times_columns = [\"experiment\", \"device\", \"average_nas_time\", \"std_dev_nas_time\", \"max_nas_time\"]\n",
    "df_nas_times = pd.DataFrame(columns=df_nas_times_columns)\n",
    "\n",
    "experiment = \"nas\"\n",
    "\n",
    "nas_times_row = [experiment, device, average_nas_time, std_dev_nas_time, max_nas_time]\n",
    "df_nas_times.loc[0] = nas_times_row\n",
    "\n",
    "df_nas_stats_columns = [\"experiment\", \"device\", \"snapshot\", *averag_nas_stats_names, *std_dev_nas_stats_names, *max_nas_stats_names]\n",
    "df_nas_stats = pd.DataFrame(columns=df_nas_stats_columns)\n",
    "\n",
    "for index, _snapshot in enumerate(average_nas_stats_list):\n",
    "    row = np.array([experiment, device, index])\n",
    "    row = np.append(row, average_nas_stats_list[index])\n",
    "    row = np.append(row, std_dev_nas_stats_list[index])\n",
    "    row = np.append(row, max_nas_stats_list[index])\n",
    "\n",
    "    df_nas_stats.loc[index] = row\n",
    "\n",
    "display(df_nas_times)\n",
    "display(df_nas_stats)\n",
    "\n",
    "average_nas_time = df_nas_times[\"average_nas_time\"].values[0]\n",
    "global_average_nas_cpu_power_draw = df_nas_stats[\"average_nas_cpu_power_draw\"].astype(float).mean()\n",
    "\n",
    "print(f\"Average NAS time: {average_nas_time}\")\n",
    "print(f\"Global average NAS CPU power draw: {global_average_nas_cpu_power_draw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nas(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# balance the number of samples in each class\n",
    "def balance_classes(data, labels):\n",
    "    # count number of samples in each class\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    counts_dict = dict(zip(unique, counts))\n",
    "    print(\"Class counts:\", counts_dict)\n",
    "    \n",
    "    # find the class with the most samples\n",
    "    max_class = max(counts_dict, key=counts_dict.get)\n",
    "    max_class_count = counts_dict[max_class]\n",
    "    print(\"Class with the most samples:\", max_class)\n",
    "    \n",
    "    # find the class with the least samples\n",
    "    min_class = min(counts_dict, key=counts_dict.get)\n",
    "    min_class_count = counts_dict[min_class]\n",
    "    print(\"Class with the least samples:\", min_class)\n",
    "    \n",
    "    # find the difference between the number of samples in the two classes\n",
    "    diff = max_class_count - min_class_count\n",
    "    print(\"Difference between the number of samples in the two classes:\", diff)\n",
    "    \n",
    "    # find the indices of the samples in the class with the least samples\n",
    "    indices = np.where(labels == min_class)[0]\n",
    "    # randomly select the same number of samples from the class with the most samples\n",
    "    selected_indices = np.random.choice(np.where(labels == max_class)[0], diff, replace=False)\n",
    "    # combine the indices of the two classes\n",
    "    combined_indices = np.concatenate((indices, selected_indices))\n",
    "    # shuffle the indices\n",
    "    shuffled_indices = np.random.permutation(combined_indices)\n",
    "    # return the balanced data and labels\n",
    "    \n",
    "    return data[shuffled_indices], labels[shuffled_indices]\n",
    "\n",
    "balanced_data, balanced_labels = balance_classes(data_transformed, df_train['tag'].values)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
    "# rf.fit(data_transformed, df_train['tag'].values)\n",
    "rf.fit(balanced_data, balanced_labels)\n",
    "\n",
    "test_data_transformed = standard.transform(df_test[gf])\n",
    "predictions = rf.predict(test_data_transformed)\n",
    "\n",
    "accuracy = accuracy_score(df_test[\"tag\"], predictions)\n",
    "balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], predictions)\n",
    "f1 = f1_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650ad6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T22:41:02.310112Z",
     "start_time": "2023-02-03T22:40:56.845962Z"
    }
   },
   "outputs": [],
   "source": [
    "teacher = baseline_model(input_dim=len(gf), n_output=2)\n",
    "teacher, history = train_model(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a322bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = small_model(input_dim=len(gf), n_output=2)\n",
    "\n",
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_student_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=es_patience,\n",
    "    restore_best_weights=es_restore_best_weights,\n",
    ")\n",
    "history = distiller.fit(\n",
    "    data_transformed,\n",
    "    pd.get_dummies(df_train[\"tag\"]),\n",
    "    validation_split=validation_split,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[es],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "test_data_transformed = standard.transform(df_test[gf])\n",
    "predictions = distiller.student.predict(test_data_transformed)\n",
    "balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], np.argmax(predictions, axis=1))\n",
    "print(f\"Balanced accuracy: {balanced_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274aadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=3,\n",
    ")\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_student_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=es_patience,\n",
    "    restore_best_weights=es_restore_best_weights,\n",
    ")\n",
    "history = distiller.fit(\n",
    "    data_transformed,\n",
    "    pd.get_dummies(df_train[\"tag\"]),\n",
    "    validation_split=validation_split,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[es],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0dae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_nas_exp_duration = df_nas_times[f\"average_{experiment}_time\"]\n",
    "df_nas_stats[\"total_average_cpu_energy_consumption\"] = df_nas_stats[f\"average_nas_cpu_power_draw\"] * average_nas_exp_duration\n",
    "df_nas_stats[\"total_average_gpu_energy_consumption\"] = df_nas_stats[f\"average_nas_gpu_power_draw\"] * average_nas_exp_duration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "243a4164",
   "metadata": {},
   "source": [
    "#### KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faccd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = baseline_model(input_dim=len(gf), n_output=2)\n",
    "teacher, history = train_model(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "class DistillerEstimator(BaseEstimator):\n",
    "    def __init__(self, alpha=0.1, temperature=3):\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        student = small_model(input_dim=len(gf), n_output=2)\n",
    "        distiller = Distiller(student=student, teacher=teacher)\n",
    "        distiller.compile(\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "            student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "            distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "            alpha=self.alpha,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        es = EarlyStopping(\n",
    "            monitor=\"val_student_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=es_patience,\n",
    "            restore_best_weights=es_restore_best_weights,\n",
    "        )\n",
    "        history = distiller.fit(\n",
    "            X,\n",
    "            y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es],\n",
    "            verbose=False,\n",
    "        )\n",
    "        self.distiller_ = distiller\n",
    "        return self\n",
    "    \n",
    "    # def score(self, X, y):\n",
    "    #     test_data_transformed = standard.transform(df_test[gf])\n",
    "    #     predictions = self.distiller_.student.predict(test_data_transformed)\n",
    "    #     balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], np.argmax(predictions, axis=1))\n",
    "    #     return balanced_accuracy\n",
    "    def score(self, X, y):\n",
    "        predictions = self.distiller_.student.predict(X)\n",
    "        balanced_accuracy = balanced_accuracy_score(np.argmax(y, axis=1), np.argmax(predictions, axis=1))\n",
    "        \n",
    "        return balanced_accuracy\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.distiller_.student.predict(X)\n",
    "        print(f\"predictions shape: {predictions.shape}\")\n",
    "        y = np.array(y)\n",
    "        accuracy = accuracy_score(np.argmax(y, axis=1), np.argmax(predictions, axis=1))\n",
    "        balanced_accuracy = balanced_accuracy_score(np.argmax(y, axis=1), np.argmax(predictions, axis=1))\n",
    "        f1 = f1_score(np.argmax(y, axis=1), np.argmax(predictions, axis=1), average=\"weighted\")\n",
    "        \n",
    "        accuracy = round(accuracy, 3)\n",
    "        balanced_accuracy = round(balanced_accuracy, 3)\n",
    "        f1 = round(f1, 3)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "        print(f\"F1 score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'temperature': [0.5, 1, 1.5, 2, 2.5, 3]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=DistillerEstimator(), param_grid=param_grid, cv=3, verbose=2)\n",
    "grid_search.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f71f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "de = DistillerEstimator(alpha=0.1, temperature=2)\n",
    "de.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "de.evaluate(test_data_transformed, pd.get_dummies(df_test[\"tag\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dec564b6",
   "metadata": {},
   "source": [
    "#### PT Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline_model(input_dim=len(gf), n_output=2)\n",
    "baseline, history = train_model(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator():\n",
    "    dataset_size = 0.1\n",
    "    print(\"Generating dataset for float16 activations and int8 weights quantization\")\n",
    "    print(\"Original Dataset size: \", int(len(data_transformed)))\n",
    "    print(\"Dataset size: \", int(len(data_transformed) * dataset_size))\n",
    "    \n",
    "    for data in tf.data.Dataset.from_tensor_slices((data_transformed)).batch(1).take(int(len(data_transformed) * dataset_size)):\n",
    "        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(baseline)\n",
    "converter.representative_dataset = dataset_generator\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.float16]\n",
    "quantized_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float16_quantization(trained_model, dataset_size: float = 0.25):\n",
    "    def dataset_generator(): # Does not need a representative dataset\n",
    "        print(\"Generating dataset for float16 weights quantization\")\n",
    "        print(\"Original Dataset size: \", int(len(data_transformed)))\n",
    "        print(\"Dataset size: \", int(len(data_transformed) * dataset_size))\n",
    "        \n",
    "        for data in tf.data.Dataset.from_tensor_slices((data_transformed)).batch(1).take(int(len(data_transformed) * dataset_size)):\n",
    "            yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(trained_model)\n",
    "    # converter.representative_dataset = dataset_generator\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.float16]\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    logger.info(\"Applied float16 weights quantization\")\n",
    "\n",
    "    # Save the model to disk\n",
    "    quantized_tflite_model_file = 'model.tflite'\n",
    "    \n",
    "    with open(quantized_tflite_model_file, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    return quantized_tflite_model_file\n",
    "\n",
    "dataset_sizes = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for dataset_size in dataset_sizes:\n",
    "    tflite_model = float16_quantization(baseline, dataset_size)\n",
    "\n",
    "    # evaluate\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    test_data_transformed = standard.transform(df_test[gf])\n",
    "    predictions = []\n",
    "\n",
    "    # cast data to float32\n",
    "    test_data_transformed = test_data_transformed.astype(np.float32)\n",
    "\n",
    "    for i in range(len(test_data_transformed)):\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_data_transformed[i].reshape(1, -1))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append(np.argmax(output_data))\n",
    "    \n",
    "    accuracy = accuracy_score(df_test[\"tag\"], predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], predictions)\n",
    "    f1 = f1_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "    \n",
    "    accuracy = round(accuracy, 4)\n",
    "    balanced_accuracy = round(balanced_accuracy, 4)\n",
    "    f1 = round(f1, 4)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int8_quantization(trained_model, dataset_size: float = 0.25):\n",
    "    def dataset_generator():\n",
    "        print(\"Generating dataset for int8 weights quantization\")\n",
    "        print(\"Original Dataset size: \", int(len(data_transformed)))\n",
    "        print(\"Dataset size: \", int(len(data_transformed) * dataset_size))\n",
    "        \n",
    "        for data in tf.data.Dataset.from_tensor_slices((data_transformed)).batch(1).take(int(len(data_transformed) * dataset_size)):\n",
    "            yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(trained_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = dataset_generator\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    logger.info(\"Applied int8 weights quantization\")\n",
    "\n",
    "    # Save the model to disk\n",
    "    quantized_tflite_model_file = 'model.tflite'\n",
    "    \n",
    "    with open(quantized_tflite_model_file, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    return quantized_tflite_model_file\n",
    "\n",
    "dataset_sizes = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for dataset_size in dataset_sizes:\n",
    "    tflite_model = int8_quantization(baseline, dataset_size)\n",
    "\n",
    "    # evaluate\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    test_data_transformed = standard.transform(df_test[gf])\n",
    "    predictions = []\n",
    "\n",
    "    # cast data to float32\n",
    "    test_data_transformed = test_data_transformed.astype(np.int8)\n",
    "\n",
    "    for i in range(len(test_data_transformed)):\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_data_transformed[i].reshape(1, -1))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append(np.argmax(output_data))\n",
    "        \n",
    "    accuracy = accuracy_score(df_test[\"tag\"], predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], predictions)\n",
    "    f1 = f1_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "    \n",
    "    accuracy = round(accuracy, 4)\n",
    "    balanced_accuracy = round(balanced_accuracy, 4)\n",
    "    f1 = round(f1, 4)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float16_activations_int8_weights_quantization(trained_model, dataset_size: float = 0.25):\n",
    "    def dataset_generator():\n",
    "        print(\"Generating dataset for float16 activations and int8 weights quantization\")\n",
    "        print(\"Original Dataset size: \", int(len(data_transformed)))\n",
    "        print(\"Dataset size: \", int(len(data_transformed) * dataset_size))\n",
    "        \n",
    "        for data in tf.data.Dataset.from_tensor_slices((data_transformed)).batch(1).take(int(len(data_transformed) * dataset_size)):\n",
    "            yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(trained_model)\n",
    "    converter.representative_dataset = dataset_generator\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    logger.info(\"Applied float16 activations and int8 weights quantization\")\n",
    "\n",
    "    # Save the model to disk\n",
    "    quantized_tflite_model_file = 'model.tflite'\n",
    "    \n",
    "    with open(quantized_tflite_model_file, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    return quantized_tflite_model_file\n",
    "\n",
    "dataset_sizes = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for dataset_size in dataset_sizes:\n",
    "    tflite_model = float16_activations_int8_weights_quantization(baseline, dataset_size)\n",
    "\n",
    "    # evaluate\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    test_data_transformed = standard.transform(df_test[gf])\n",
    "    predictions = []\n",
    "\n",
    "    # cast data to float32\n",
    "    test_data_transformed = test_data_transformed.astype(np.float32)\n",
    "\n",
    "    for i in range(len(test_data_transformed)):\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_data_transformed[i].reshape(1, -1))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append(np.argmax(output_data))\n",
    "        \n",
    "    accuracy = accuracy_score(df_test[\"tag\"], predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], predictions)\n",
    "    f1 = f1_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "    \n",
    "    accuracy = round(accuracy, 4)\n",
    "    balanced_accuracy = round(balanced_accuracy, 4)\n",
    "    f1 = round(f1, 4)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_range_quantization(trained_model, dataset_size: float = 0.25): # Not implemented in the framework\n",
    "    def dataset_generator(): # Does not need a representative dataset\n",
    "        print(\"Generating dataset for dynamic range quantization\")\n",
    "        print(\"Original Dataset size: \", int(len(data_transformed)))\n",
    "        print(\"Dataset size: \", int(len(data_transformed) * dataset_size))\n",
    "        \n",
    "        for data in tf.data.Dataset.from_tensor_slices((data_transformed)).batch(1).take(int(len(data_transformed) * dataset_size)):\n",
    "            yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(trained_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    logger.info(\"Applied dynamic range quantization\")\n",
    "\n",
    "    # Save the model to disk\n",
    "    quantized_tflite_model_file = 'model.tflite'\n",
    "    \n",
    "    with open(quantized_tflite_model_file, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    return quantized_tflite_model_file\n",
    "\n",
    "dataset_sizes = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for dataset_size in dataset_sizes:\n",
    "    tflite_model = dynamic_range_quantization(baseline, dataset_size)\n",
    "\n",
    "    # evaluate\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    test_data_transformed = standard.transform(df_test[gf])\n",
    "    predictions = []\n",
    "\n",
    "    # cast data to float32\n",
    "    test_data_transformed = test_data_transformed.astype(np.float32)\n",
    "\n",
    "    for i in range(len(test_data_transformed)):\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_data_transformed[i].reshape(1, -1))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append(np.argmax(output_data))\n",
    "        \n",
    "    accuracy = accuracy_score(df_test[\"tag\"], predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(df_test[\"tag\"], predictions)\n",
    "    f1 = f1_score(df_test[\"tag\"], predictions, average=\"weighted\")\n",
    "    \n",
    "    accuracy = round(accuracy, 4)\n",
    "    balanced_accuracy = round(balanced_accuracy, 4)\n",
    "    f1 = round(f1, 4)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b3d1b35",
   "metadata": {},
   "source": [
    "#### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5586344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:03:11.979705Z",
     "start_time": "2023-03-08T23:03:06.529140Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline = baseline_model(input_dim=len(gf), n_output=2)\n",
    "baseline, history = train_model(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f9a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T23:03:12.468550Z",
     "start_time": "2023-03-08T23:03:12.451683Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PolynomialDecayPruningEstimator(BaseEstimator):\n",
    "    def __init__(self, batch_size=256, epochs=10, initial_sparsity=0.50, final_sparsity=0.80):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.initial_sparsity = initial_sparsity\n",
    "        self.final_sparsity = final_sparsity\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fine-tune pretrained model with pruning aware training\n",
    "        prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "        num_samples = len(X)\n",
    "        end_step = np.ceil(num_samples / self.batch_size).astype(np.int32) * self.epochs\n",
    "\n",
    "        pruning_params = {\n",
    "            \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(\n",
    "                initial_sparsity=self.initial_sparsity, final_sparsity=self.final_sparsity, begin_step=0, end_step=end_step\n",
    "            )\n",
    "        }\n",
    "\n",
    "        model_for_pruning = prune_low_magnitude(baseline, **pruning_params)\n",
    "\n",
    "        model_for_pruning.compile(\n",
    "            optimizer=\"adam\", loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        ]\n",
    "\n",
    "        self.history_ = model_for_pruning.fit(X, y, validation_split=0.1, epochs=self.epochs, batch_size=self.batch_size, callbacks=callbacks, verbose=0)\n",
    "\n",
    "        self.pruned_model_ = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "        # convert to tflite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(self.pruned_model_)\n",
    "        converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY, tf.lite.Optimize.DEFAULT]\n",
    "        self.pruned_tflite_model_ = converter.convert()\n",
    "\n",
    "        # Save the model to disk\n",
    "        self.pruned_tflite_model_file_ = 'model.tflite'\n",
    "\n",
    "        with open(self.pruned_tflite_model_file_, 'wb') as f:\n",
    "            f.write(self.pruned_tflite_model_)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Load model\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.pruned_tflite_model_file_)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # Load test set\n",
    "        # test_data_transformed = standard.transform(df_test[gf])\n",
    "        \n",
    "        # get input shape\n",
    "        input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "        logger.info(f\"Input shape: {input_shape}\")\n",
    "        \n",
    "        # get output shape\n",
    "        output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "        logger.info(f\"Output shape: {output_shape}\")\n",
    "        \n",
    "        # transform data to the expected tensor type\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        dtype = input_details[\"dtype\"]    \n",
    "        input_data = np.array(X, dtype=dtype)\n",
    "        \n",
    "        # reshape model input\n",
    "        batch_size = 256\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        interpreter.resize_tensor_input(input_details['index'], (batch_size, input_data.shape[1]))\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "        \n",
    "        preds = []\n",
    "\n",
    "        # create batches of test_data_transformed\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i:i+batch_size]\n",
    "            # print(f\"Batch {i//batch_size} has {len(batch)} elements\")\n",
    "            \n",
    "            batch_data = np.array(batch, dtype=dtype)\n",
    "            \n",
    "            if len(batch) == batch_size:        \n",
    "                interpreter.set_tensor(input_details['index'], batch_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details['index'])\n",
    "                \n",
    "                preds.append(output_data)\n",
    "        \n",
    "        predictions = np.concatenate(preds)\n",
    "        \n",
    "        # take the labels of the test set\n",
    "        test_labels = y.values[:len(predictions)]\n",
    "        \n",
    "        balanced_accuracy = balanced_accuracy_score(np.argmax(y, axis=1), np.argmax(predictions, axis=1))\n",
    "        \n",
    "        return balanced_accuracy\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        # Load model\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.pruned_tflite_model_file_)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # get input shape\n",
    "        input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "        logger.info(f\"Input shape: {input_shape}\")\n",
    "\n",
    "        # get output shape\n",
    "        output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "        logger.info(f\"Output shape: {output_shape}\")\n",
    "\n",
    "        # transform data to the expected tensor type\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        dtype = input_details[\"dtype\"]\n",
    "        input_data = np.array(X, dtype=dtype)\n",
    "\n",
    "        # reshape model input\n",
    "        batch_size = 256\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        interpreter.resize_tensor_input(\n",
    "            input_details[\"index\"], (batch_size, input_data.shape[1])\n",
    "        )\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # create batches of test_data_transformed\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i : i + batch_size]\n",
    "            # print(f\"Batch {i//batch_size} has {len(batch)} elements\")\n",
    "\n",
    "            batch_data = np.array(batch, dtype=dtype)\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                interpreter.set_tensor(input_details[\"index\"], batch_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[\"index\"])\n",
    "\n",
    "                preds.append(output_data)\n",
    "                \n",
    "        predictions = np.concatenate(preds)\n",
    "\n",
    "        # take the labels of the test set\n",
    "        test_labels = y.values[: len(predictions)]\n",
    "\n",
    "        balanced_accuracy = balanced_accuracy_score(\n",
    "            np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1)\n",
    "        )\n",
    "\n",
    "        accuracy = accuracy_score(np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1))\n",
    "        balanced_accuracy = balanced_accuracy_score(np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1))\n",
    "        f1 = f1_score(np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1), average=\"weighted\")\n",
    "        \n",
    "        accuracy = round(accuracy, 3)\n",
    "        balanced_accuracy = round(balanced_accuracy, 3)\n",
    "        f1 = round(f1, 3)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "        print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a959ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    param_grid = {'batch_size': [256, 512, 1024, 2048], 'epochs': [10, 20, 30], 'initial_sparsity': [0.50, 0.60, 0.70, 0.80], 'final_sparsity': [0.60, 0.70, 0.80, 0.90]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=PolynomialDecayPruningEstimator(), param_grid=param_grid, cv=3, verbose=2)\n",
    "    grid_search.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecbfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    pe = PolynomialDecayPruningEstimator(batch_size=1024, epochs=10, initial_sparsity=0.70, final_sparsity=0.90)\n",
    "    pe.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "    pe.evaluate(test_data_transformed, pd.get_dummies(df_test[\"tag\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ae4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ConstantSparsityPruningEstimator(BaseEstimator):\n",
    "    def __init__(self, batch_size=256, epochs=10, target_sparsity=0.80):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.target_sparsity = target_sparsity\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fine-tune pretrained model with pruning aware training\n",
    "        prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "        num_samples = len(X)\n",
    "        end_step = np.ceil(num_samples / self.batch_size).astype(np.int32) * self.epochs\n",
    "\n",
    "        pruning_params = {\n",
    "            \"pruning_schedule\": tfmot.sparsity.keras.ConstantSparsity(\n",
    "                target_sparsity=self.target_sparsity, begin_step=0, end_step=end_step,\n",
    "            )\n",
    "        }\n",
    "\n",
    "        model_for_pruning = prune_low_magnitude(baseline, **pruning_params)\n",
    "\n",
    "        model_for_pruning.compile(\n",
    "            optimizer=\"adam\", loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        ]\n",
    "\n",
    "        self.history_ = model_for_pruning.fit(X, y, validation_split=0.1, epochs=self.epochs, batch_size=self.batch_size, callbacks=callbacks, verbose=0)\n",
    "\n",
    "        self.pruned_model_ = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "        # convert to tflite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(self.pruned_model_)\n",
    "        converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY, tf.lite.Optimize.DEFAULT]\n",
    "        self.pruned_tflite_model_ = converter.convert()\n",
    "\n",
    "        # Save the model to disk\n",
    "        self.pruned_tflite_model_file_ = 'model.tflite'\n",
    "\n",
    "        with open(self.pruned_tflite_model_file_, 'wb') as f:\n",
    "            f.write(self.pruned_tflite_model_)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Load model\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.pruned_tflite_model_file_)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # Load test set\n",
    "        # test_data_transformed = standard.transform(df_test[gf])\n",
    "        \n",
    "        # get input shape\n",
    "        input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "        logger.info(f\"Input shape: {input_shape}\")\n",
    "        \n",
    "        # get output shape\n",
    "        output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "        logger.info(f\"Output shape: {output_shape}\")\n",
    "        \n",
    "        # transform data to the expected tensor type\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        dtype = input_details[\"dtype\"]    \n",
    "        input_data = np.array(X, dtype=dtype)\n",
    "        \n",
    "        # reshape model input\n",
    "        batch_size = 256\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        interpreter.resize_tensor_input(input_details['index'], (batch_size, input_data.shape[1]))\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "        \n",
    "        preds = []\n",
    "\n",
    "        # create batches of test_data_transformed\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i:i+batch_size]\n",
    "            # print(f\"Batch {i//batch_size} has {len(batch)} elements\")\n",
    "            \n",
    "            batch_data = np.array(batch, dtype=dtype)\n",
    "            \n",
    "            if len(batch) == batch_size:        \n",
    "                interpreter.set_tensor(input_details['index'], batch_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details['index'])\n",
    "                \n",
    "                preds.append(output_data)\n",
    "        \n",
    "        predictions = np.concatenate(preds)\n",
    "        \n",
    "        # take the labels of the test set\n",
    "        test_labels = y.values[:len(predictions)]\n",
    "        \n",
    "        balanced_accuracy = balanced_accuracy_score(np.argmax(y, axis=1), np.argmax(predictions, axis=1))\n",
    "        \n",
    "        return balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26019eb0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-08T23:03:19.532Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    param_grid = {'batch_size': [256, 512, 1024, 2048], 'epochs': [10, 20, 30], 'target_sparsity': [0.5, 0.60, 0.70, 0.80, 0.90]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=ConstantSparsityPruningEstimator(), param_grid=param_grid, cv=3, verbose=2)\n",
    "    grid_search.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71f5ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T12:33:43.892493Z",
     "start_time": "2023-03-09T12:33:43.886126Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9493ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PolynomialDecayPruningEstimator:\n",
    "    def __init__(self, model=None, batch_size=256, epochs=10, initial_sparsity=0.50, final_sparsity=0.80):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.initial_sparsity = initial_sparsity\n",
    "        self.final_sparsity = final_sparsity\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fine-tune pretrained model with pruning aware training\n",
    "        prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "        num_samples = len(X)\n",
    "        end_step = np.ceil(num_samples / self.batch_size).astype(np.int32) * self.epochs\n",
    "\n",
    "        pruning_params = {\n",
    "            \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(\n",
    "                initial_sparsity=self.initial_sparsity, final_sparsity=self.final_sparsity, begin_step=0, end_step=end_step\n",
    "            )\n",
    "        }\n",
    "\n",
    "        model_for_pruning = prune_low_magnitude(self.model, **pruning_params)\n",
    "\n",
    "        model_for_pruning.compile(\n",
    "            optimizer=\"adam\", loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        ]\n",
    "\n",
    "        self.history_ = model_for_pruning.fit(X, y, validation_split=0.1, epochs=self.epochs, batch_size=self.batch_size, callbacks=callbacks, verbose=0)\n",
    "\n",
    "        self.pruned_model_ = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "        # convert to tflite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(self.pruned_model_)\n",
    "        converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY, tf.lite.Optimize.DEFAULT]\n",
    "        self.pruned_tflite_model_ = converter.convert()\n",
    "\n",
    "        # Save the model to disk\n",
    "        self.pruned_tflite_model_file_ = 'model.tflite'\n",
    "\n",
    "        with open(self.pruned_tflite_model_file_, 'wb') as f:\n",
    "            f.write(self.pruned_tflite_model_)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Load model\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.pruned_tflite_model_file_)\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        # get input shape\n",
    "        input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "        logger.info(f\"Input shape: {input_shape}\")\n",
    "        \n",
    "        # get output shape\n",
    "        output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "        logger.info(f\"Output shape: {output_shape}\")\n",
    "        \n",
    "        # transform data to the expected tensor type\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        dtype = input_details[\"dtype\"]    \n",
    "        input_data = np.array(X, dtype=dtype)\n",
    "        \n",
    "        # reshape model input\n",
    "        batch_size = 256\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        interpreter.resize_tensor_input(input_details['index'], (batch_size, input_data.shape[1]))\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "        \n",
    "        preds = []\n",
    "\n",
    "        # create batches of test_data_transformed\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i:i+batch_size]\n",
    "            \n",
    "            batch_data = np.array(batch, dtype=dtype)\n",
    "            \n",
    "            if len(batch) == batch_size:        \n",
    "                interpreter.set_tensor(input_details['index'], batch_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details['index'])\n",
    "                \n",
    "                preds.append(output_data)\n",
    "        \n",
    "        predictions = np.concatenate(preds)\n",
    "        \n",
    "        # take the labels of the test set\n",
    "        test_labels = y.values[:len(predictions)]\n",
    "        \n",
    "        balanced_accuracy = balanced_accuracy_score(np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1))\n",
    "        \n",
    "        return balanced_accuracy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Load model\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.pruned_tflite_model_file_)\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        # get input shape\n",
    "        input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "        logger.info(f\"Input shape: {input_shape}\")\n",
    "        \n",
    "        # get output shape\n",
    "        output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "        logger.info(f\"Output shape: {output_shape}\")\n",
    "        \n",
    "        # transform data to the expected tensor type\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        dtype = input_details[\"dtype\"]    \n",
    "        input_data = np.array(X, dtype=dtype)\n",
    "        \n",
    "        # reshape model input\n",
    "        batch_size = 256\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        interpreter.resize_tensor_input(input_details['index'], (batch_size, input_data.shape[1]))\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "        \n",
    "        preds = []\n",
    "\n",
    "        # create batches of test_data_transformed\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i:i+batch_size]\n",
    "            \n",
    "            batch_data = np.array(batch, dtype=dtype)\n",
    "            \n",
    "            if len(batch) == batch_size:        \n",
    "                interpreter.set_tensor(input_details['index'], batch_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details['index'])\n",
    "                \n",
    "                preds.append(output_data)\n",
    "        \n",
    "        predictions = np.concatenate(preds)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pruning(device=\"CPU\"):\n",
    "    baseline = baseline_model(input_dim=len(gf), n_output=2)\n",
    "    baseline, _history = train_model(baseline)\n",
    "\n",
    "    num_trials = 5\n",
    "    \n",
    "    logger.info(\"Starting test_pruning\")\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    \n",
    "    model_energy_consumption = {}\n",
    "    \n",
    "    Path(f\"poc_energy_efficiency_crypto/Pruning\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        with StatsCollectionManager(directory=\"Pruning\", test=\"pruning\", sampling_rate=stats_sampling_rate, pid=pid) as pruning_scm:\n",
    "            with measure_time() as pruning_time_measure:\n",
    "                logger.info(\"Applying Pruning\")\n",
    "        \n",
    "                # find the student model structure by searching for the model\n",
    "                param_grid = {'batch_size': [256, 512, 1024, 2048], 'epochs': [10, 20, 30], 'initial_sparsity': [0.50, 0.60, 0.70, 0.80], 'final_sparsity': [0.60, 0.70, 0.80, 0.90]}\n",
    "                # param_grid = {'batch_size': [256], 'epochs': [10], 'initial_sparsity': [0.50], 'final_sparsity': [0.60]}\n",
    "                batch_size_grid = param_grid['batch_size']\n",
    "                epochs_grid = param_grid['epochs']\n",
    "                initial_sparsity_grid = param_grid['initial_sparsity']\n",
    "                final_sparsity_grid = param_grid['final_sparsity']\n",
    "\n",
    "                # make all possible combinations of the parameters\n",
    "                combinations = list(itertools.product(batch_size_grid, epochs_grid, initial_sparsity_grid, final_sparsity_grid))\n",
    "                    \n",
    "                for comb_id, comb in enumerate(combinations):\n",
    "                    batch_size = comb[0]\n",
    "                    epochs = comb[1]\n",
    "                    initial_sparsity = comb[2]\n",
    "                    final_sparsity = comb[3]\n",
    "                    \n",
    "                    logger.info(f\"Parameters: batch_size={batch_size}, epochs={epochs}, initial_sparsity={initial_sparsity}, final_sparsity={final_sparsity}\")\n",
    "                    \n",
    "                    estimator = PolynomialDecayPruningEstimator(model=baseline, batch_size=batch_size, epochs=epochs, initial_sparsity=initial_sparsity, final_sparsity=final_sparsity)\n",
    "                    estimator.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "                        \n",
    "                    for i in range(0, num_trials):\n",
    "                        logger.info(f\"Trial Evaluation: {i}\")\n",
    "                    \n",
    "                        # evaluate the model\n",
    "                        data_transformed_test = standard.transform(df_test[gf])\n",
    "                        \n",
    "                        with StatsCollectionManager(directory=\"Pruning\", test=f\"evaluation_pruning_{comb_id}_{i}\", sampling_rate=stats_sampling_rate, pid=pid) as evaluation_pruning_scm:\n",
    "                            with measure_time() as evaluation_pruning_time_measure:\n",
    "                                predictions = estimator.predict(data_transformed_test)\n",
    "                        \n",
    "                        evaluation_pruning_time = evaluation_pruning_time_measure()\n",
    "                        \n",
    "                        # Write evaluation pruning time to file\n",
    "                        with open(f\"poc_energy_efficiency_crypto/Pruning/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_pruning_time_{comb_id}_{i}.pkl\", \"wb\") as f:\n",
    "                            pickle.dump({\"evaluation_pruning_time\": evaluation_pruning_time}, f)\n",
    "                            \n",
    "                    if ml_task == \"binary_classification\":\n",
    "                        predictions = np.argmax(predictions, axis=1)\n",
    "                        \n",
    "                        test_labels = df_test[\"tag\"].values[:len(predictions)]\n",
    "                        \n",
    "                        accuracy = accuracy_score(test_labels, predictions)\n",
    "                        f1 = f1_score(test_labels, predictions, average=\"weighted\")\n",
    "                        auc = roc_auc_score(test_labels, predictions)\n",
    "                        recall = recall_score(test_labels, predictions, average=\"weighted\")\n",
    "                        precision = precision_score(test_labels, predictions, average=\"weighted\")\n",
    "                        balanced_accuracy = balanced_accuracy_score(test_labels, predictions)\n",
    "                        matthews = matthews_corrcoef(test_labels, predictions)\n",
    "\n",
    "                        logger.info(f\"Accuracy: {accuracy}\")\n",
    "                        logger.info(f\"F1 score: {f1}\")\n",
    "                        logger.info(f\"AUC: {auc}\")\n",
    "                        logger.info(f\"Recall: {recall}\")\n",
    "                        logger.info(f\"Precision: {precision}\")\n",
    "                        logger.info(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "                        logger.info(f\"Matthews correlation coefficient: {matthews}\")\n",
    "                        \n",
    "                        test_results = {\n",
    "                            \"accuracy\": accuracy,\n",
    "                            \"f1\": f1,\n",
    "                            \"auc\": auc,\n",
    "                            \"recall\": recall,\n",
    "                            \"precision\": precision,\n",
    "                            \"balanced_accuracy\": balanced_accuracy,\n",
    "                            \"matthews\": matthews\n",
    "                        }\n",
    "                    elif ml_task == \"regression\":\n",
    "                        mae = mean_absolute_error(test_labels, predictions)\n",
    "                        mse = mean_squared_error(test_labels, predictions)\n",
    "                        mape = mean_absolute_percentage_error(test_labels, predictions)\n",
    "                        smape = 1/len(test_labels) * np.sum(2 * np.abs(predictions - test_labels) / (np.abs(predictions) + np.abs(test_labels)))\n",
    "                        \n",
    "                        logger.info(f\"MAE: {mae}\")\n",
    "                        logger.info(f\"MSE: {mse}\")\n",
    "                        logger.info(f\"MAPE: {mape}\")\n",
    "                        logger.info(f\"SMAPE: {smape}\")\n",
    "                        \n",
    "                        test_results = {\n",
    "                            \"mae\": mae,\n",
    "                            \"mse\": mse,\n",
    "                            \"mape\": mape,\n",
    "                            \"smape\": smape\n",
    "                        }\n",
    "                        \n",
    "                    with open(f\"poc_energy_efficiency_crypto/Pruning/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_pruning_test_results_{comb_id}.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(test_results, f)\n",
    "                                \n",
    "                    evaluation_pruning_times_list = []\n",
    "                    evaluation_pruning_stats_list = []\n",
    "                    \n",
    "                    for i in range(0, num_trials):\n",
    "                        logger.info(f\"Trial Evaluation: {i}. Comb ID: {comb_id}\")\n",
    "                            \n",
    "                        # Read pruning time from file\n",
    "                        with open(f\"poc_energy_efficiency_crypto/Pruning/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_pruning_time_{comb_id}_{i}.pkl\", \"rb\") as f:\n",
    "                            evaluation_pruning_time = pickle.load(f)\n",
    "\n",
    "                        evaluation_pruning_time = evaluation_pruning_time[\"evaluation_pruning_time\"]\n",
    "                        evaluation_pruning_time = float(evaluation_pruning_time)\n",
    "                                \n",
    "                        # Read pruning stats from file\n",
    "                        with open(f'poc_energy_efficiency_crypto/Pruning/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_pruning_{comb_id}_{i}_stats.pkl', 'rb') as f:\n",
    "                            evaluation_pruning_stats = pickle.load(f)\n",
    "                        \n",
    "                        logger.info(\"Trial {}. Comb ID {}. Evaluation Pruning time: {}\".format(i, comb_id, evaluation_pruning_time))\n",
    "\n",
    "                        # Save Pruning times\n",
    "                        evaluation_pruning_times_list.append(evaluation_pruning_time)\n",
    "\n",
    "                        # Save Pruning stats\n",
    "                        evaluation_pruning_stats_list.append(evaluation_pruning_stats)\n",
    "\n",
    "                    average_evaluation_pruning_time = np.mean(evaluation_pruning_times_list)\n",
    "                    std_dev_evaluation_pruning_time = np.std(evaluation_pruning_times_list)\n",
    "                    max_evaluation_pruning_time = np.max(evaluation_pruning_times_list)\n",
    "\n",
    "                    # Time spent on Pruning\n",
    "                    logger.info(f\"Comb ID: {comb_id}. \" + \"Average Pruning evaluation time: {}\".format(np.round(average_evaluation_pruning_time, 2)))\n",
    "                    logger.info(f\"Comb ID: {comb_id}. \" + \"Standard deviation of Pruning evaluation time: {}\".format(np.round(std_dev_evaluation_pruning_time, 2)))\n",
    "                    logger.info(f\"Comb ID: {comb_id}. \" + \"Max. Pruning evaluation time: {}\".format(np.round(max_evaluation_pruning_time, 2)))\n",
    "\n",
    "                    # Get average Pruning metrics\n",
    "                    average_evaluation_pruning_stats_list = get_average_stats(evaluation_pruning_stats_list, average_evaluation_pruning_time)\n",
    "                    std_dev_evaluation_pruning_stats_list = get_std_dev_stats(evaluation_pruning_stats_list, std_dev_evaluation_pruning_time)\n",
    "                    max_evaluation_pruning_stats_list = get_max_stats(evaluation_pruning_stats_list, max_evaluation_pruning_time)\n",
    "                    \n",
    "                    # Save Pruning metrics to dataframe\n",
    "                    stats = evaluation_pruning_stats_list[0][0].keys()\n",
    "\n",
    "                    averag_evaluation_pruning_stats_names = [f\"average_evaluation_pruning_{stat}\" for stat in stats]\n",
    "                    std_dev_evaluation_pruning_stats_names = [f\"std_dev_evaluation_pruning_{stat}\" for stat in stats]\n",
    "                    max_evaluation_pruning_stats_names = [f\"max_evaluation_pruning_{stat}\" for stat in stats]\n",
    "\n",
    "                    df_evaluation_pruning_times_columns = [\"experiment\", \"device\", \"average_evaluation_pruning_time\", \"std_dev_evaluation_pruning_time\", \"max_evaluation_pruning_time\"]\n",
    "                    df_evaluation_pruning_times = pd.DataFrame(columns=df_evaluation_pruning_times_columns)\n",
    "                    \n",
    "                    experiment = f\"evaluation_pruning_{comb_id}\"\n",
    "                    evaluation_pruning_times_row = [experiment, device, average_evaluation_pruning_time, std_dev_evaluation_pruning_time, max_evaluation_pruning_time]\n",
    "                    df_evaluation_pruning_times.loc[0] = evaluation_pruning_times_row\n",
    "\n",
    "                    df_evaluation_pruning_stats_columns = [\"experiment\", \"device\", \"snapshot\", *averag_evaluation_pruning_stats_names, *std_dev_evaluation_pruning_stats_names, *max_evaluation_pruning_stats_names]\n",
    "                    df_evaluation_pruning_stats = pd.DataFrame(columns=df_evaluation_pruning_stats_columns)\n",
    "                    \n",
    "                    for index, _snapshot in enumerate(average_evaluation_pruning_stats_list):\n",
    "                        row = np.array([experiment, device, index])\n",
    "                        row = np.append(row, average_evaluation_pruning_stats_list[index])\n",
    "                        row = np.append(row, std_dev_evaluation_pruning_stats_list[index])\n",
    "                        row = np.append(row, max_evaluation_pruning_stats_list[index])\n",
    "\n",
    "                        df_evaluation_pruning_stats.loc[index] = row\n",
    "                        \n",
    "                    assert df_evaluation_pruning_times.shape[0] == 1\n",
    "                    assert df_evaluation_pruning_stats.shape[0] == len(average_evaluation_pruning_stats_list)\n",
    "                    assert df_evaluation_pruning_stats.shape[1] == len(df_evaluation_pruning_stats_columns)\n",
    "                    \n",
    "                    display(df_evaluation_pruning_times)\n",
    "                    display(df_evaluation_pruning_stats)\n",
    "\n",
    "                    # calculate the average of the df_evaluation_pruning_stats average_evaluation_pruning_cpu_power_draw and average_evaluation_pruning_gpu_power_draw columns\n",
    "                    global_average_average_evaluation_pruning_cpu_power_draw = df_evaluation_pruning_stats[f\"average_evaluation_pruning_cpu_power_draw\"].astype(float).mean()\n",
    "                    global_average_average_evaluation_pruning_gpu_power_draw = df_evaluation_pruning_stats[f\"average_evaluation_pruning_gpu_power_draw\"].astype(float).mean()\n",
    "                    \n",
    "                    # check that both columns have a single scalar value\n",
    "                    assert np.isscalar(global_average_average_evaluation_pruning_cpu_power_draw)\n",
    "                    assert np.isscalar(global_average_average_evaluation_pruning_gpu_power_draw)\n",
    "                    \n",
    "                    average_evaluation_pruning_exp_duration = df_evaluation_pruning_times[f\"average_evaluation_pruning_time\"].values[0]\n",
    "                    \n",
    "                    logger.info(\"Average evaluation Pruning experiment duration: {}\".format(average_evaluation_pruning_exp_duration))\n",
    "                    logger.info(\"Average evaluation Pruning CPU power draw: {}\".format(global_average_average_evaluation_pruning_cpu_power_draw))\n",
    "                    logger.info(\"Average evaluation Pruning GPU power draw: {}\".format(global_average_average_evaluation_pruning_gpu_power_draw))\n",
    "                    \n",
    "                    total_average_cpu_energy_consumption = global_average_average_evaluation_pruning_cpu_power_draw * average_evaluation_pruning_exp_duration\n",
    "                    total_average_gpu_energy_consumption = global_average_average_evaluation_pruning_gpu_power_draw * average_evaluation_pruning_exp_duration\n",
    "                    \n",
    "                    model_energy_consumption[comb_id] = {\n",
    "                        \"cpu\": total_average_cpu_energy_consumption,\n",
    "                        \"gpu\": total_average_gpu_energy_consumption\n",
    "                    }\n",
    "                    \n",
    "                    logger.info(\"Model energy consumption:\\n{}\".format(model_energy_consumption))\n",
    "                    \n",
    "                # normalize energy consumption between 0 and 1\n",
    "                normalized_model_energy_consumption = deepcopy(model_energy_consumption)\n",
    "                \n",
    "                max_cpu_energy_consumption = max([model_energy_consumption[comb_id][\"cpu\"] for comb_id in model_energy_consumption])\n",
    "                max_gpu_energy_consumption = max([model_energy_consumption[comb_id][\"gpu\"] for comb_id in model_energy_consumption])\n",
    "                \n",
    "                for comb_id in model_energy_consumption:\n",
    "                    normalized_model_energy_consumption[comb_id][\"cpu\"] = model_energy_consumption[comb_id][\"cpu\"] / max_cpu_energy_consumption\n",
    "                    normalized_model_energy_consumption[comb_id][\"gpu\"] = model_energy_consumption[comb_id][\"gpu\"] / max_gpu_energy_consumption\n",
    "                \n",
    "                # multiply normalized energy consumption by 0.5\n",
    "                weighted_normalized_model_energy_consumption = deepcopy(normalized_model_energy_consumption)\n",
    "                \n",
    "                for comb_id in model_energy_consumption:\n",
    "                    weighted_normalized_model_energy_consumption[comb_id][\"cpu\"] = normalized_model_energy_consumption[comb_id][\"cpu\"] * 0.5 # TODO: parameterize the weight\n",
    "                    weighted_normalized_model_energy_consumption[comb_id][\"gpu\"] = normalized_model_energy_consumption[comb_id][\"gpu\"] * 0.5\n",
    "                                    \n",
    "                # Read all evaluation pruning test results files\n",
    "                evaluation_pruning_test_results_files = glob.glob(\"poc_energy_efficiency_crypto/Pruning/crypto_spider_5g_fcnn_optimized_benchmark_evaluation_pruning_test_results_*.pkl\")\n",
    "                measured_performance = {}\n",
    "                \n",
    "                for evaluation_pruning_test_results_file in evaluation_pruning_test_results_files:\n",
    "                    # comb_id = evaluation_pruning_test_results_file.split(\"_\")[-2] + \"_\" + evaluation_pruning_test_results_file.split(\"_\")[-1].split(\".\")[0]\n",
    "                    comb_id = int(evaluation_pruning_test_results_file.split(\"_\")[-1].split(\".\")[0])\n",
    "                    \n",
    "                    with open(evaluation_pruning_test_results_file, 'rb') as f:\n",
    "                        evaluation_pruning_test_results = pickle.load(f)\n",
    "                    \n",
    "                    # Get measured performance\n",
    "                    measured_performance[comb_id] = evaluation_pruning_test_results[\"balanced_accuracy\"] # TODO: parameterize the metric\n",
    "                                \n",
    "                # multiply measured performance by 0.5\n",
    "                weighted_measured_performance = deepcopy(measured_performance)\n",
    "                \n",
    "                for comb_id in measured_performance:\n",
    "                    weighted_measured_performance[comb_id] = measured_performance[comb_id] * 0.5 # TODO: parameterize the weight\n",
    "                \n",
    "                # calculate weighted average of energy consumption and measured performance\n",
    "                weighted_average_energy_consumption_and_measured_performance = {}\n",
    "                                \n",
    "                for comb_id in model_energy_consumption:\n",
    "                    weighted_average_energy_consumption_and_measured_performance[comb_id] = weighted_normalized_model_energy_consumption[comb_id][\"cpu\"] + weighted_measured_performance[comb_id] # TODO: parameterize the platform that is being optimized (cpu or gpu)\n",
    "            \n",
    "                # get best comb_id\n",
    "                best_comb_id = max(weighted_average_energy_consumption_and_measured_performance, key=weighted_average_energy_consumption_and_measured_performance.get)\n",
    "                logger.info(f\"Best comb_id: {best_comb_id}\")\n",
    "                logger.info(f\"Best comb_id energy consumption: {model_energy_consumption[best_comb_id]}\")\n",
    "                logger.info(f\"Best comb_id measured performance: {measured_performance[best_comb_id]}\")\n",
    "                logger.info(f\"Best comb_id normalized energy consumption cpu: {normalized_model_energy_consumption[best_comb_id]['cpu']}\")\n",
    "                logger.info(f\"Best comb_id normalized energy consumption gpu: {normalized_model_energy_consumption[best_comb_id]['gpu']}\")\n",
    "                logger.info(f\"Best comb_id weighted normalized energy consumption cpu: {weighted_normalized_model_energy_consumption[best_comb_id]['cpu']}\")\n",
    "                logger.info(f\"Best comb_id weighted normalized energy consumption gpu: {weighted_normalized_model_energy_consumption[best_comb_id]['gpu']}\")\n",
    "                logger.info(f\"Best comb_id weighted measured performance: {weighted_measured_performance[best_comb_id]}\")\n",
    "                logger.info(f\"Best comb_id weighted average energy consumption and measured performance: {weighted_average_energy_consumption_and_measured_performance[best_comb_id]}\")\n",
    "                \n",
    "                # save best comb_id\n",
    "                with open(f\"poc_energy_efficiency_crypto/Pruning/crypto_spider_5g_fcnn_optimized_benchmark_best_comb_id.pkl\", \"wb\") as f:\n",
    "                    pickle.dump({\"best_comb_id\": best_comb_id}, f)\n",
    "                \n",
    "        # Get pruning time\n",
    "        pruning_time = pruning_time_measure()\n",
    "\n",
    "        # Write pruning time to file\n",
    "        with open(f\"poc_energy_efficiency_crypto/Pruning/crypto_spider_5g_fcnn_optimized_benchmark_pruning_time.pkl\", \"wb\") as f:\n",
    "            pickle.dump({\"pruning_time\": pruning_time}, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the child process\n",
    "p_nas = multiprocessing.Process(target=test_pruning, args=(\"cpu\",))\n",
    "\n",
    "p_nas.start()\n",
    "p_nas.join()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "221825c9",
   "metadata": {},
   "source": [
    "#### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def __init__(self, quantization_config = \"LastValueQuantizer\", num_bits=8, symmetric=True, narrow_range=False, per_axis=False):\n",
    "        self.quantization_config = quantization_config\n",
    "        self.num_bits = num_bits\n",
    "        self.symmetric = symmetric\n",
    "        self.narrow_range = narrow_range\n",
    "        self.per_axis = per_axis\n",
    "        \n",
    "        if self.quantization_config == \"LastValueQuantizer\":\n",
    "            self.quantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer(num_bits=self.num_bits, symmetric=self.symmetric, narrow_range=self.narrow_range, per_axis=self.per_axis)\n",
    "        elif self.quantization_config == \"MovingAverageQuantizer\":\n",
    "            self.quantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=self.num_bits, symmetric=self.symmetric, narrow_range=self.narrow_range, per_axis=self.per_axis)\n",
    "        elif self.quantization_config == \"AllValuesQuantizer\":\n",
    "            self.quantizer = tfmot.quantization.keras.quantizers.AllValuesQuantizer(num_bits=self.num_bits, symmetric=self.symmetric, narrow_range=self.narrow_range, per_axis=self.per_axis)\n",
    "\n",
    "    # Configure how to quantize weights.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return [\n",
    "            (\n",
    "                layer.kernel,\n",
    "                self.quantizer,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Configure how to quantize activations.\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return [\n",
    "            (\n",
    "                layer.kernel,\n",
    "                self.quantizer,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "        # , in the same order\n",
    "        layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "        # , in the same order.\n",
    "        layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_config(self):\n",
    "        # serialize the quantizer\n",
    "        return {\n",
    "            \"quantization_config\": self.quantization_config,\n",
    "            \"num_bits\": self.num_bits,\n",
    "            \"symmetric\": self.symmetric,\n",
    "            \"narrow_range\": self.narrow_range,\n",
    "            \"per_axis\": self.per_axis,\n",
    "        }\n",
    "\n",
    "\n",
    "class QuantizationEstimator(BaseEstimator):\n",
    "    def __init__(\n",
    "        self, batch_size=256, epochs=10, quantization_config=\"LastValueQuantizer\"\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.quantization_config = quantization_config\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fine-tune pretrained model with quantization aware training\n",
    "\n",
    "        # Define the quantization configuration\n",
    "        quantize_config = QuantizeConfig(self.quantization_config)\n",
    "        \n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # annotate baseline model\n",
    "        for layer in baseline.layers:\n",
    "            annotated_layer = tfmot.quantization.keras.quantize_annotate_layer(layer, quantize_config)\n",
    "            model.add(annotated_layer)\n",
    "        \n",
    "        model = tfmot.quantization.keras.quantize_annotate_model(model)\n",
    "\n",
    "        # Apply quantization to the model\n",
    "        with tfmot.quantization.keras.quantize_scope({'DefaultDenseQuantizeConfig': QuantizeConfig}):\n",
    "            q_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "            \n",
    "        q_aware_model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        validation_split = 0.1\n",
    "\n",
    "        history = q_aware_model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "        quantized_model = converter.convert()\n",
    "\n",
    "        # Save the model to disk\n",
    "        self.quantized_tflite_model_file = \"model.tflite\"\n",
    "\n",
    "        with open(self.quantized_tflite_model_file, \"wb\") as f:\n",
    "            f.write(quantized_model)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Load model\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.quantized_tflite_model_file)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # get input shape\n",
    "        input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "        logger.info(f\"Input shape: {input_shape}\")\n",
    "\n",
    "        # get output shape\n",
    "        output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "        logger.info(f\"Output shape: {output_shape}\")\n",
    "\n",
    "        # transform data to the expected tensor type\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        dtype = input_details[\"dtype\"]\n",
    "        input_data = np.array(test_data_transformed, dtype=dtype)\n",
    "\n",
    "        # reshape model input\n",
    "        batch_size = 256\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        interpreter.resize_tensor_input(\n",
    "            input_details[\"index\"], (batch_size, input_data.shape[1])\n",
    "        )\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # create batches of test_data_transformed\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i : i + batch_size]\n",
    "            # print(f\"Batch {i//batch_size} has {len(batch)} elements\")\n",
    "\n",
    "            batch_data = np.array(batch, dtype=dtype)\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                interpreter.set_tensor(input_details[\"index\"], batch_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[\"index\"])\n",
    "\n",
    "                preds.append(output_data)\n",
    "        predictions = np.concatenate(preds)\n",
    "\n",
    "        # take the labels of the test set\n",
    "        test_labels = y.values[: len(predictions)]\n",
    "\n",
    "        balanced_accuracy = balanced_accuracy_score(\n",
    "            test_labels, np.argmax(predictions, axis=1)\n",
    "        )\n",
    "\n",
    "        return balanced_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15afbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    param_grid = {'batch_size': [256, 512, 1024, 2048], 'epochs': [10, 20, 30], 'quantization_config': [\"LastValueQuantizer\", \"MovingAverageQuantizer\", \"AllValuesQuantizer\",]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=QuantizationEstimator(), param_grid=param_grid, cv=3, verbose=2)\n",
    "    grid_search.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea23483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def __init__(self, quantization_config = \"LastValueQuantizer\", num_bits=8, symmetric=True, narrow_range=False, per_axis=False):\n",
    "        self.quantization_config = quantization_config\n",
    "        self.num_bits = num_bits\n",
    "        self.symmetric = symmetric\n",
    "        self.narrow_range = narrow_range\n",
    "        self.per_axis = per_axis\n",
    "        \n",
    "        if self.quantization_config == \"LastValueQuantizer\":\n",
    "            self.quantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer(num_bits=self.num_bits, symmetric=self.symmetric, narrow_range=self.narrow_range, per_axis=self.per_axis)\n",
    "        elif self.quantization_config == \"MovingAverageQuantizer\":\n",
    "            self.quantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=self.num_bits, symmetric=self.symmetric, narrow_range=self.narrow_range, per_axis=self.per_axis)\n",
    "        elif self.quantization_config == \"AllValuesQuantizer\":\n",
    "            self.quantizer = tfmot.quantization.keras.quantizers.AllValuesQuantizer(num_bits=self.num_bits, symmetric=self.symmetric, narrow_range=self.narrow_range, per_axis=self.per_axis)\n",
    "\n",
    "    # Configure how to quantize weights.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return [\n",
    "            (\n",
    "                layer.kernel,\n",
    "                self.quantizer,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Configure how to quantize activations.\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return [\n",
    "            (\n",
    "                layer.kernel,\n",
    "                self.quantizer,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "        # , in the same order\n",
    "        layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "        # , in the same order.\n",
    "        layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_config(self):\n",
    "        # serialize the quantizer\n",
    "        return {\n",
    "            \"quantization_config\": self.quantization_config,\n",
    "            \"num_bits\": self.num_bits,\n",
    "            \"symmetric\": self.symmetric,\n",
    "            \"narrow_range\": self.narrow_range,\n",
    "            \"per_axis\": self.per_axis,\n",
    "        }\n",
    "\n",
    "\n",
    "class QuantizationEstimator(BaseEstimator):\n",
    "    def __init__(\n",
    "        self, batch_size=256, epochs=10,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fine-tune pretrained model with quantization aware training\n",
    "\n",
    "        q_aware_model = tfmot.quantization.keras.quantize_model(baseline)\n",
    "        q_aware_model.compile(\n",
    "            optimizer=\"adam\", loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        validation_split = 0.1\n",
    "\n",
    "        history = q_aware_model.fit(X, y, validation_split=validation_split, epochs=self.epochs, batch_size=self.batch_size)\n",
    "\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "        quantized_model = converter.convert()\n",
    "\n",
    "        # Save the model to disk\n",
    "        self.quantized_tflite_model_file = \"model.tflite\"\n",
    "\n",
    "        with open(self.quantized_tflite_model_file, \"wb\") as f:\n",
    "            f.write(quantized_model)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Load model\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.quantized_tflite_model_file)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # get input shape\n",
    "        input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "        logger.info(f\"Input shape: {input_shape}\")\n",
    "\n",
    "        # get output shape\n",
    "        output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "        logger.info(f\"Output shape: {output_shape}\")\n",
    "\n",
    "        # transform data to the expected tensor type\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        dtype = input_details[\"dtype\"]\n",
    "        input_data = np.array(X, dtype=dtype)\n",
    "\n",
    "        # reshape model input\n",
    "        batch_size = 256\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        interpreter.resize_tensor_input(\n",
    "            input_details[\"index\"], (batch_size, input_data.shape[1])\n",
    "        )\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # create batches of test_data_transformed\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i : i + batch_size]\n",
    "            # print(f\"Batch {i//batch_size} has {len(batch)} elements\")\n",
    "\n",
    "            batch_data = np.array(batch, dtype=dtype)\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                interpreter.set_tensor(input_details[\"index\"], batch_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[\"index\"])\n",
    "\n",
    "                preds.append(output_data)\n",
    "                \n",
    "        predictions = np.concatenate(preds)\n",
    "\n",
    "        # take the labels of the test set\n",
    "        test_labels = y.values[: len(predictions)]\n",
    "\n",
    "        balanced_accuracy = balanced_accuracy_score(\n",
    "            np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1)\n",
    "        )\n",
    "\n",
    "        return balanced_accuracy\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        # Load model\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.quantized_tflite_model_file)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # get input shape\n",
    "        input_shape = interpreter.get_input_details()[0][\"shape\"]\n",
    "        logger.info(f\"Input shape: {input_shape}\")\n",
    "\n",
    "        # get output shape\n",
    "        output_shape = interpreter.get_output_details()[0][\"shape\"]\n",
    "        logger.info(f\"Output shape: {output_shape}\")\n",
    "\n",
    "        # transform data to the expected tensor type\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        dtype = input_details[\"dtype\"]\n",
    "        input_data = np.array(X, dtype=dtype)\n",
    "\n",
    "        # reshape model input\n",
    "        batch_size = 256\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        interpreter.resize_tensor_input(\n",
    "            input_details[\"index\"], (batch_size, input_data.shape[1])\n",
    "        )\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # create batches of test_data_transformed\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i : i + batch_size]\n",
    "            # print(f\"Batch {i//batch_size} has {len(batch)} elements\")\n",
    "\n",
    "            batch_data = np.array(batch, dtype=dtype)\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                interpreter.set_tensor(input_details[\"index\"], batch_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[\"index\"])\n",
    "\n",
    "                preds.append(output_data)\n",
    "                \n",
    "        predictions = np.concatenate(preds)\n",
    "\n",
    "        # take the labels of the test set\n",
    "        test_labels = y.values[: len(predictions)]\n",
    "\n",
    "        balanced_accuracy = balanced_accuracy_score(\n",
    "            np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1)\n",
    "        )\n",
    "\n",
    "        accuracy = accuracy_score(np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1))\n",
    "        balanced_accuracy = balanced_accuracy_score(np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1))\n",
    "        f1 = f1_score(np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1), average=\"weighted\")\n",
    "        \n",
    "        accuracy = round(accuracy, 3)\n",
    "        balanced_accuracy = round(balanced_accuracy, 3)\n",
    "        f1 = round(f1, 3)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "        print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a99642",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    param_grid = {'batch_size': [256, 512, 1024, 2048], 'epochs': [10, 20, 30]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=QuantizationEstimator(), param_grid=param_grid, cv=3, verbose=2)\n",
    "    grid_search.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7147bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe = QuantizationEstimator(batch_size=256, epochs=10)\n",
    "qe.fit(data_transformed, pd.get_dummies(df_train[\"tag\"]))\n",
    "qe.evaluate(test_data_transformed, pd.get_dummies(df_test[\"tag\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc_energy_efficiency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
